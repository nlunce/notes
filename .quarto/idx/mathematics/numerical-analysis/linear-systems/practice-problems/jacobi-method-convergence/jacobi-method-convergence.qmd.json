{"title":"Jacobi Method - Convergence Proof","markdown":{"yaml":{"title":"Jacobi Method - Convergence Proof","format":{"html":{"self-contained":true,"page-layout":"full","toc":true,"toc-depth":3,"toc-location":"right","number-sections":false,"html-math-method":"katex","embed-resources":true,"code-fold":true,"code-summary":"Show Code","code-overflow":"wrap","code-copy":"hover","code-tools":{"source":false,"toggle":true,"caption":"See code"}}},"engine":"jupyter","preview":{"port":3000,"browser":true,"watch-inputs":true,"navigate":true}},"headingText":"**Diagonal Dominance Convergence Theorem**","containsRefs":false,"markdown":"\n\n\n_Theorem 2.10 (p. 107)_\n\nIf the $n \\times n$ matrix $A$ is strictly diagonally dominant, then:\n\n- $A$ is a nonsingular matrix (invertible matrix).\n- For every vector $b$ and every starting guess, the [**Jacobi Method**](../notes/w07/ax-b-iterative-methods/jacobi-method.html) applied to $A \\mathbf{x} = \\mathbf{b}$ converges to the (unique) solution.\n\n## **Spectral Radius Convergence Theorem**\n\n_Theorem A.7 (p. 588)_\n\nIf the $n \\times n$ matrix $A$ has spectral radius $\\rho(A) < 1$, and $\\mathbf{b}$ is arbitrary, then, for any vector $\\mathbf{x}_0$, the iteration $\\mathbf{x}_{k+1} = A \\mathbf{x}_k + \\mathbf{b}$ converges. In fact, there exists a unique $\\mathbf{x}$, such that $\\lim_{k \\to \\infty} \\mathbf{x}_k = \\mathbf{x}$, and $\\mathbf{x} = A \\mathbf{x} + \\mathbf{b}$.\n\n## **Definitions**\n\n- [**Spectral radius**](../notes/w06/spectral-radius.html):  \n  The spectral radius $\\rho(A)$ of a square matrix $A$ is the maximum magnitude of its eigenvalues.\n\n- [**Infinity or max norm**](../notes/w07/norms/infinity-vector-norm.html):  \n  For a vector $\\mathbf{x} \\in \\mathbb{R}^n$, the infinity norm is $\\|\\mathbf{x}\\|_\\infty = \\max_{1 \\leq i \\leq n} |x_i|$.\n\n## **Proof**\n\nRecall that the Jacobi Method for solving $A \\mathbf{x} = \\mathbf{b}$ is\n\n$$\n\\mathbf{x}_{k+1} = -D^{-1}(L + U) \\mathbf{x}_k + D^{-1} \\mathbf{b},\n$$\n\nwhere\n\n$$\nA = L + D + U,\n$$\n\n$L$ is the lower triangular part of $A$, $D$ is the diagonal part of $A$, and $U$ is the upper triangular part of $A$.\n\nWe will apply _Theorem A.7_ by showing that the spectral radius of $-D^{-1}(L + U)$ is less than 1:\n\n$$\n\\rho(D^{-1}(L + U)) < 1\n$$\n\nFor notational convenience, let $R = L + U$ denote the non-diagonal part of the matrix $A$. Then we must show that $\\rho(D^{-1}R) < 1$.\n\n::: {.callout-note icon=false}\n\n## **1. Scaled Vector $\\mathbf{v}$:**\n\nGiven any vector $\\mathbf{x}$, we can create a scaled version of $\\mathbf{x}$, say $\\mathbf{v}$, as $\\mathbf{v} = \\frac{\\mathbf{x}}{c}$.\nWhat value of $c$ will guarantee that $\\|\\mathbf{v}\\|_\\infty = 1$?\n\n:::\n\n::: {.callout-tip collapse=\"true\"}\n\n## **Answer**\n\nTo ensure $\\|\\mathbf{v}\\|_\\infty = 1$, define $\\mathbf{v} = \\frac{\\mathbf{x}}{c}$, where $c$ is a scalar.\n\nThe infinity norm of $\\mathbf{v}$ is:\n\n$$\n\\|\\mathbf{v}\\|_\\infty = \\max_{1 \\leq i \\leq n} \\left| \\frac{x_i}{c} \\right|\n$$\n\nSet $\\|\\mathbf{v}\\|_\\infty = 1$, so:\n\n$$\n\\frac{\\max_{1 \\leq i \\leq n} |x_i|}{c} = 1\n$$\n\nSolve for $c$:\n\n$$\nc = \\|\\mathbf{x}\\|_\\infty = \\max_{1 \\leq i \\leq n} |x_i|\n$$\n\nThus, scaling $\\mathbf{x}$ by $c = \\|\\mathbf{x}\\|_\\infty$ guarantees $\\|\\mathbf{v}\\|_\\infty = 1$.\n\n:::\n\n::: {.callout-note icon=false}\n\n## **2. Eigenvalue Analysis:**\n\nLet $\\lambda$ represent an arbitrary eigenvalue of $D^{-1}R$ with corresponding eigenvector $\\mathbf{v}$. Then $D^{-1}R \\mathbf{v} = \\lambda \\mathbf{v}$, or $R \\mathbf{v} = \\lambda D \\mathbf{v}$.\n\nWhy?\n\nWe’ll look at each side of this equation in turn.\nSuppose we scale the eigenvector $\\mathbf{v}$ such that $\\|\\mathbf{v}\\|_\\infty = 1$. Then $|v_i| \\leq 1$ for every index $i$, $1 \\leq i \\leq n$, and $|v_m| = 1$ for at least one index $m$, $1 \\leq m \\leq n$.\n\nUsing this index $m$, explain why the absolute value of the $m$-th row of $R \\mathbf{v}$ is:\n\n$$\n|r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n|\n$$\n\n:::\n\n::: {.callout-tip collapse=\"true\"}\n\n## **Answer**\n\nTo analyze the absolute value of the $m$-th row of $R \\mathbf{v}$, start with the eigenvalue equation:\n\n$$\nD^{-1} R \\mathbf{v} = \\lambda \\mathbf{v}\n$$\n\nMultiply through by $D$ to rewrite it as:\n\n$$\nR \\mathbf{v} = \\lambda D \\mathbf{v}\n$$\n\nHere:\n\n- $R = L + U$, where $L$ is the strictly lower triangular part of $A$ and $U$ is the strictly upper triangular part of $A$.\n- $D$ is the diagonal part of $A$.\n- $\\mathbf{v}$ is an eigenvector scaled such that $\\|\\mathbf{v}\\|_\\infty = 1$, meaning $|v_i| \\leq 1$ for all $i$, and $|v_m| = 1$ for at least one $m$.\n\nThe $m$-th row of $R$ is:\n\n$$\n\\begin{bmatrix}\nr_{m,1} & r_{m,2} & \\cdots & r_{m,m-1} & 0 & r_{m,m+1} & \\cdots & r_{m,n}\n\\end{bmatrix}\n$$\n\nMultiplying this row by the vector $\\mathbf{v}$, the $m$-th entry of $R \\mathbf{v}$ is:\n\n$$\n\\left( R \\mathbf{v} \\right)_m = \\sum_{i=1}^n r_{m,i} v_i\n$$\n\nSince $r_{m,m} = 0$ (as $R = L + U$ excludes the diagonal), this simplifies to:\n\n$$\n\\left( R \\mathbf{v} \\right)_m = \\sum_{i=1, i \\neq m}^n r_{m,i} v_i\n$$\n\n:::\n\n::: {.callout-note icon=false}\n\n## **3. Scaling with $D$:**\n\nNow, explain why the absolute value of the $m$-th row of $\\lambda D \\mathbf{v}$ is $|\\lambda| |d_{m,m}|$.\n\n:::\n\n::: {.callout-tip collapse=\"true\"}\n\n## **Answer**\n\nTo explain why the absolute value of the $m$-th row of $\\lambda D \\mathbf{v}$ is $|\\lambda| |d_{m,m}|$, begin by recalling the structure of $D$, the diagonal matrix of $A$:\n\n$$\nD =\n\\begin{bmatrix}\nd_{1,1} & 0 & \\cdots & 0 \\\\\n0 & d_{2,2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & d_{n,n}\n\\end{bmatrix}\n$$\n\nWhen $D$ is multiplied by the eigenvector $\\mathbf{v}$, the result is:\n\n$$\nD \\mathbf{v} =\n\\begin{bmatrix}\nd_{1,1} v_1 \\\\\nd_{2,2} v_2 \\\\\n\\vdots \\\\\nd_{n,n} v_n\n\\end{bmatrix}\n$$\n\nNow multiply by $\\lambda$, giving:\n\n$$\n\\lambda D \\mathbf{v} =\n\\begin{bmatrix}\n\\lambda d_{1,1} v_1 \\\\\n\\lambda d_{2,2} v_2 \\\\\n\\vdots \\\\\n\\lambda d_{n,n} v_n\n\\end{bmatrix}\n$$\n\nThe $m$-th row of this result is:\n\n$$\n\\left( \\lambda D \\mathbf{v} \\right)_m = \\lambda d_{m,m} v_m\n$$\n\nTaking the absolute value:\n\n$$\n\\left| \\left( \\lambda D \\mathbf{v} \\right)_m \\right| = |\\lambda| |d_{m,m}| |v_m|\n$$\n\nSince $\\|\\mathbf{v}\\|_\\infty = 1$, we know:\n\n$$\n|v_i| \\leq 1 \\quad \\text{for all } i, \\quad \\text{and} \\quad |v_m| = 1\n$$\n\nSubstitute $|v_m| = 1$:\n\n$$\n\\left| \\left( \\lambda D \\mathbf{v} \\right)_m \\right| = |\\lambda| |d_{m,m}|\n$$\n\nThus, the absolute value of the $m$-th row of $\\lambda D \\mathbf{v}$ is determined by $|\\lambda|$, the eigenvalue, and $|d_{m,m}|$, the diagonal entry of $D$ at row $m$.\n:::\n\n::: {.callout-tip icon=false appearance=\"simple\"}\n\n# **_EQUATION 1_**\n\n**Combining steps (2) and (3), we can write:**\n\n$$\n|\\lambda||d_{m,m}| = |r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n|\n$$\n\n:::\n\n::: {.callout-note icon=false}\n\n## **4. Explain why:**\n\n$$\n|r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n| \\leq \\sum_{j \\neq m} |r_{m,j}|\n$$\n\n:::\n\n::: {.callout-tip collapse=\"true\"}\n\n## **Answer**\n\nTo explain why\n\n$$\n\\big| r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n \\big| \\leq \\sum_{j \\neq m} |r_{m,j}|,\n$$\n\nwe begin by recalling the **triangle inequality** for absolute values. For any sum of terms $a_1, a_2, \\dots, a_k$, the triangle inequality ensures:\n\n$$\n|a_1 + a_2 + \\cdots + a_k| \\leq |a_1| + |a_2| + \\cdots + |a_k|\n$$\n\nIn our case, the sum of interest is:\n\n$$\nr_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n\n$$\n\nApplying the triangle inequality to this sum gives:\n\n$\\quad \\big| r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n \\big|$\n\n$$\n\\leq |r_{m,1}v_1| + |r_{m,2}v_2| + \\cdots + |r_{m,m-1}v_{m-1}| + |r_{m,m+1}v_{m+1}| + \\cdots + |r_{m,n}v_n|\n$$\n\nEach term in the sum has the form $|r_{m,j}v_j|$. Using the property of absolute values $|ab| = |a||b|$, we can rewrite each term as:\n\n$$\n|r_{m,j}v_j| = |r_{m,j}| \\cdot |v_j|\n$$\n\nSince it is assumed that $|v_j| \\leq 1$ for all $j$, it follows that:\n\n$$\n|r_{m,j}v_j| \\leq |r_{m,j}|\n$$\n\nSubstituting this bound for each term into the inequality gives:\n\n$\\quad \\big| r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n \\big|$\n\n$$\n\\leq |r_{m,1}v_1| + |r_{m,2}v_2| + \\cdots + |r_{m,m-1}v_{m-1}| + |r_{m,m+1}v_{m+1}| + \\cdots + |r_{m,n}v_n|\n$$\n\nThe indices $j \\neq m$ correspond to all off-diagonal entries in the $m$-th row of the matrix. Thus, we can express the sum of the absolute values of the coefficients as:\n\n$$\n|r_{m,1}| + |r_{m,2}| + \\cdots + |r_{m,m-1}| + |r_{m,m+1}| + \\cdots + |r_{m,n}| = \\sum_{j \\neq m} |r_{m,j}|\n$$\n\nSubstituting this back, we find:\n\n$$\n\\big| r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n \\big| \\leq \\sum_{j \\neq m} |r_{m,j}|\n$$\n\nThe inequality holds because:\n\n1. The **triangle inequality** ensures that the absolute value of a sum is at most the sum of the absolute values of its terms.\n2. The assumption $|v_j| \\leq 1$ allows us to bound $|r_{m,j}v_j|$ by $|r_{m,j}|$.\n\nThus, the magnitude of the weighted sum of $v_j$ values (for $j \\neq m$) is always less than or equal to the sum of the absolute values of the off-diagonal entries in the $m$-th row of the matrix.\n:::\n\n::: {.callout-note icon=false}\n\n## **5. Explain why:**\n\n$$\n\\sum_{j \\neq m} |r_{m,j}| < |d_{m,m}|\n$$\n\n:::\n\n::: {.callout-tip collapse=\"true\"}\n\n## **Answer**\n\nBy assumption, the matrix $A$ is strictly diagonally dominant. This means that, for each row of $A$, the absolute value of the diagonal entry $|d_{m,m}|$ is strictly greater than the sum of the absolute values of all the off-diagonal entries in that row:\n\n$$\n|d_{m,m}| > \\sum_{j \\neq m} |r_{m,j}|\n$$\n\nIn other words, the diagonal entry $d_{m,m}$ has the largest contribution in the row, ensuring that the total influence of the off-diagonal terms is strictly smaller.\n:::\n\n::: {.callout-note icon=false}\n\n## **6. Use the results from Steps (4) and (5) with _EQUATION 1_ to show show that:**\n\n$$\n|\\lambda||d_{m,m}| < |d_{m,m}|\n$$\n\nWhat does this say about $|\\lambda|$?\n\n:::\n\n::: {.callout-tip collapse=\"true\"}\n\n## **Answer**\n\nTo explain why\n\n$$\n|\\lambda||d_{m,m}| < |d_{m,m}|\n$$\n\nwe combine the results from **Step (4)** and **Step (5)** with **_EQUATION 1_**.\n\n**_EQUATION 1_** states:\n\n$$\n|\\lambda||d_{m,m}| = \\big| r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n \\big|\n$$\n\nFrom **Step (4)**, we know:\n\n$$\n\\big| r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n \\big| \\leq \\sum_{j \\neq m} |r_{m,j}|\n$$\n\nSubstituting this into **_EQUATION 1_**, we have:\n\n$$\n|\\lambda||d_{m,m}| \\leq \\sum_{j \\neq m} |r_{m,j}|\n$$\n\nFrom **Step (5)**, we know:\n\n$$\n\\sum_{j \\neq m} |r_{m,j}| < |d_{m,m}|\n$$\n\nCombining this with the inequality above gives:\n\n$$\n|\\lambda||d_{m,m}| < |d_{m,m}|\n$$\n\nDividing both sides of the inequality by $|d_{m,m}|$ (which is nonzero), we find:\n\n$$\n|\\lambda| < 1\n$$\n\nThis result shows that the magnitude of the eigenvalue $|\\lambda|$ is strictly less than 1 and this implies that the spectral radius $\\rho(D^{-1}R) < 1$.\n:::\n\n::: {.callout-note icon=false}\n\n## **7. Final Conclusion**\n\nSince $\\lambda$ is an arbitrary eigenvalue, then $|\\lambda|_{\\text{max}} < 1$. In other words, the spectral radius $\\rho(D^{-1}R) < 1$. Thus, by the **Spectral Radius Convergence Theorem** (Theorem A.7), the Jacobi Method (iteration with $A = D^{-1}R$) converges for any starting point $\\mathbf{x}_0$.\n\nLet $\\mathbf{x}_* = \\lim_{k \\to \\infty} \\mathbf{x}_k$, and show that $\\mathbf{x}_*$ is the solution to $A \\mathbf{x} = \\mathbf{b}$, so $A$ must be nonsingular. This completes the proof of the **Diagonal Dominance Convergence Theorem** (Theorem 2.10).\n\n:::\n\n::: {.callout-tip collapse=\"true\"}\n\n## **Answer**\n\nFrom earlier, we showed that the spectral radius $\\rho(D^{-1}R)$, which is the largest magnitude of the eigenvalues of $D^{-1}R$, satisfies:\n\n$$\n\\rho(D^{-1}R) < 1\n$$\n\nThis result guarantees that the Jacobi method converges to a solution $\\mathbf{x}_*$ of $A\\mathbf{x} = \\mathbf{b}$ for any starting point $\\mathbf{x}_0$, as stated in the **Diagonal Dominance Convergence Theorem** (_Theorem A.7_).\n\nTherefore, we can write the limit of the iterates $\\mathbf{x}_k$ as:\n\n$$\n\\mathbf{x}_* = \\lim_{k \\to \\infty} \\mathbf{x}_k\n$$\n\nThe Jacobi iteration formula is:\n\n$$\n\\mathbf{x}_{k+1} = D^{-1}(\\mathbf{b} - R\\mathbf{x}_k)\n$$\n\nwhere $A = D - R$, $D$ is the diagonal matrix, and $R$ is the remainder matrix (containing the off-diagonal terms).\n\nSubstituting the limit $\\mathbf{x}_*$ into this equation (as $\\mathbf{x}_{k+1} \\to \\mathbf{x}_*$ and $\\mathbf{x}_k \\to \\mathbf{x}_*$), we get:\n\n$$\n\\mathbf{x}_* = D^{-1}(\\mathbf{b} - R\\mathbf{x}_*)\n$$\n\nExpanding this:\n\n$$\n\\mathbf{x}_* = -D^{-1}R\\mathbf{x}_* + D^{-1}\\mathbf{b}\n$$\n\nRewriting:\n\n::: {.callout-tip icon=false appearance=\"simple\" style=\"background-color: #fffff; border-left: 5px solid #9c27b0; color: #9c27b0; font-weight: 900;\"}\n\n# ‎\n\n$$\n\\mathbf{x}_* = -D^{-1}(L + U)\\mathbf{x}_* + D^{-1}\\mathbf{b}\n$$\n\n:::\n\nwhere $R = L + U$, with $L$ being the strictly lower triangular part of $A$ and $U$ the strictly upper triangular part.\n\nTo verify $\\mathbf{x}_*$ satisfies $A\\mathbf{x}_* = \\mathbf{b}$, substitute $A = D + L + U$ into the system:\n\n$$\n(D + L + U)\\mathbf{x}_* = \\mathbf{b}\n$$\n\nMultiply both sides by $D^{-1}$:\n\n$$\n\\mathbf{x}_* + D^{-1}(L + U)\\mathbf{x}_* = D^{-1}\\mathbf{b}\n$$\n\nRearranging terms gives:\n\n$$\n\\mathbf{x}_* = -D^{-1}(L + U)\\mathbf{x}_* + D^{-1}\\mathbf{b}\n$$\n\nThis matches the Jacobi iteration formula, verifying $\\mathbf{x}_*$ satisfies $A\\mathbf{x}_* = \\mathbf{b}$.\n\n**Conclusion**\n\n1. We proved that $\\rho(D^{-1}R) < 1$, so the Jacobi method converges to $\\mathbf{x}_*$.\n2. Substituting $\\mathbf{x}_*$ into $A\\mathbf{x} = \\mathbf{b}$, we verified that it satisfies the system.\n3. Since $A\\mathbf{x}_* = \\mathbf{b}$ has a solution, $A$ is nonsingular.\n\n:::\n","srcMarkdownNoYaml":"\n\n## **Diagonal Dominance Convergence Theorem**\n\n_Theorem 2.10 (p. 107)_\n\nIf the $n \\times n$ matrix $A$ is strictly diagonally dominant, then:\n\n- $A$ is a nonsingular matrix (invertible matrix).\n- For every vector $b$ and every starting guess, the [**Jacobi Method**](../notes/w07/ax-b-iterative-methods/jacobi-method.html) applied to $A \\mathbf{x} = \\mathbf{b}$ converges to the (unique) solution.\n\n## **Spectral Radius Convergence Theorem**\n\n_Theorem A.7 (p. 588)_\n\nIf the $n \\times n$ matrix $A$ has spectral radius $\\rho(A) < 1$, and $\\mathbf{b}$ is arbitrary, then, for any vector $\\mathbf{x}_0$, the iteration $\\mathbf{x}_{k+1} = A \\mathbf{x}_k + \\mathbf{b}$ converges. In fact, there exists a unique $\\mathbf{x}$, such that $\\lim_{k \\to \\infty} \\mathbf{x}_k = \\mathbf{x}$, and $\\mathbf{x} = A \\mathbf{x} + \\mathbf{b}$.\n\n## **Definitions**\n\n- [**Spectral radius**](../notes/w06/spectral-radius.html):  \n  The spectral radius $\\rho(A)$ of a square matrix $A$ is the maximum magnitude of its eigenvalues.\n\n- [**Infinity or max norm**](../notes/w07/norms/infinity-vector-norm.html):  \n  For a vector $\\mathbf{x} \\in \\mathbb{R}^n$, the infinity norm is $\\|\\mathbf{x}\\|_\\infty = \\max_{1 \\leq i \\leq n} |x_i|$.\n\n## **Proof**\n\nRecall that the Jacobi Method for solving $A \\mathbf{x} = \\mathbf{b}$ is\n\n$$\n\\mathbf{x}_{k+1} = -D^{-1}(L + U) \\mathbf{x}_k + D^{-1} \\mathbf{b},\n$$\n\nwhere\n\n$$\nA = L + D + U,\n$$\n\n$L$ is the lower triangular part of $A$, $D$ is the diagonal part of $A$, and $U$ is the upper triangular part of $A$.\n\nWe will apply _Theorem A.7_ by showing that the spectral radius of $-D^{-1}(L + U)$ is less than 1:\n\n$$\n\\rho(D^{-1}(L + U)) < 1\n$$\n\nFor notational convenience, let $R = L + U$ denote the non-diagonal part of the matrix $A$. Then we must show that $\\rho(D^{-1}R) < 1$.\n\n::: {.callout-note icon=false}\n\n## **1. Scaled Vector $\\mathbf{v}$:**\n\nGiven any vector $\\mathbf{x}$, we can create a scaled version of $\\mathbf{x}$, say $\\mathbf{v}$, as $\\mathbf{v} = \\frac{\\mathbf{x}}{c}$.\nWhat value of $c$ will guarantee that $\\|\\mathbf{v}\\|_\\infty = 1$?\n\n:::\n\n::: {.callout-tip collapse=\"true\"}\n\n## **Answer**\n\nTo ensure $\\|\\mathbf{v}\\|_\\infty = 1$, define $\\mathbf{v} = \\frac{\\mathbf{x}}{c}$, where $c$ is a scalar.\n\nThe infinity norm of $\\mathbf{v}$ is:\n\n$$\n\\|\\mathbf{v}\\|_\\infty = \\max_{1 \\leq i \\leq n} \\left| \\frac{x_i}{c} \\right|\n$$\n\nSet $\\|\\mathbf{v}\\|_\\infty = 1$, so:\n\n$$\n\\frac{\\max_{1 \\leq i \\leq n} |x_i|}{c} = 1\n$$\n\nSolve for $c$:\n\n$$\nc = \\|\\mathbf{x}\\|_\\infty = \\max_{1 \\leq i \\leq n} |x_i|\n$$\n\nThus, scaling $\\mathbf{x}$ by $c = \\|\\mathbf{x}\\|_\\infty$ guarantees $\\|\\mathbf{v}\\|_\\infty = 1$.\n\n:::\n\n::: {.callout-note icon=false}\n\n## **2. Eigenvalue Analysis:**\n\nLet $\\lambda$ represent an arbitrary eigenvalue of $D^{-1}R$ with corresponding eigenvector $\\mathbf{v}$. Then $D^{-1}R \\mathbf{v} = \\lambda \\mathbf{v}$, or $R \\mathbf{v} = \\lambda D \\mathbf{v}$.\n\nWhy?\n\nWe’ll look at each side of this equation in turn.\nSuppose we scale the eigenvector $\\mathbf{v}$ such that $\\|\\mathbf{v}\\|_\\infty = 1$. Then $|v_i| \\leq 1$ for every index $i$, $1 \\leq i \\leq n$, and $|v_m| = 1$ for at least one index $m$, $1 \\leq m \\leq n$.\n\nUsing this index $m$, explain why the absolute value of the $m$-th row of $R \\mathbf{v}$ is:\n\n$$\n|r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n|\n$$\n\n:::\n\n::: {.callout-tip collapse=\"true\"}\n\n## **Answer**\n\nTo analyze the absolute value of the $m$-th row of $R \\mathbf{v}$, start with the eigenvalue equation:\n\n$$\nD^{-1} R \\mathbf{v} = \\lambda \\mathbf{v}\n$$\n\nMultiply through by $D$ to rewrite it as:\n\n$$\nR \\mathbf{v} = \\lambda D \\mathbf{v}\n$$\n\nHere:\n\n- $R = L + U$, where $L$ is the strictly lower triangular part of $A$ and $U$ is the strictly upper triangular part of $A$.\n- $D$ is the diagonal part of $A$.\n- $\\mathbf{v}$ is an eigenvector scaled such that $\\|\\mathbf{v}\\|_\\infty = 1$, meaning $|v_i| \\leq 1$ for all $i$, and $|v_m| = 1$ for at least one $m$.\n\nThe $m$-th row of $R$ is:\n\n$$\n\\begin{bmatrix}\nr_{m,1} & r_{m,2} & \\cdots & r_{m,m-1} & 0 & r_{m,m+1} & \\cdots & r_{m,n}\n\\end{bmatrix}\n$$\n\nMultiplying this row by the vector $\\mathbf{v}$, the $m$-th entry of $R \\mathbf{v}$ is:\n\n$$\n\\left( R \\mathbf{v} \\right)_m = \\sum_{i=1}^n r_{m,i} v_i\n$$\n\nSince $r_{m,m} = 0$ (as $R = L + U$ excludes the diagonal), this simplifies to:\n\n$$\n\\left( R \\mathbf{v} \\right)_m = \\sum_{i=1, i \\neq m}^n r_{m,i} v_i\n$$\n\n:::\n\n::: {.callout-note icon=false}\n\n## **3. Scaling with $D$:**\n\nNow, explain why the absolute value of the $m$-th row of $\\lambda D \\mathbf{v}$ is $|\\lambda| |d_{m,m}|$.\n\n:::\n\n::: {.callout-tip collapse=\"true\"}\n\n## **Answer**\n\nTo explain why the absolute value of the $m$-th row of $\\lambda D \\mathbf{v}$ is $|\\lambda| |d_{m,m}|$, begin by recalling the structure of $D$, the diagonal matrix of $A$:\n\n$$\nD =\n\\begin{bmatrix}\nd_{1,1} & 0 & \\cdots & 0 \\\\\n0 & d_{2,2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & d_{n,n}\n\\end{bmatrix}\n$$\n\nWhen $D$ is multiplied by the eigenvector $\\mathbf{v}$, the result is:\n\n$$\nD \\mathbf{v} =\n\\begin{bmatrix}\nd_{1,1} v_1 \\\\\nd_{2,2} v_2 \\\\\n\\vdots \\\\\nd_{n,n} v_n\n\\end{bmatrix}\n$$\n\nNow multiply by $\\lambda$, giving:\n\n$$\n\\lambda D \\mathbf{v} =\n\\begin{bmatrix}\n\\lambda d_{1,1} v_1 \\\\\n\\lambda d_{2,2} v_2 \\\\\n\\vdots \\\\\n\\lambda d_{n,n} v_n\n\\end{bmatrix}\n$$\n\nThe $m$-th row of this result is:\n\n$$\n\\left( \\lambda D \\mathbf{v} \\right)_m = \\lambda d_{m,m} v_m\n$$\n\nTaking the absolute value:\n\n$$\n\\left| \\left( \\lambda D \\mathbf{v} \\right)_m \\right| = |\\lambda| |d_{m,m}| |v_m|\n$$\n\nSince $\\|\\mathbf{v}\\|_\\infty = 1$, we know:\n\n$$\n|v_i| \\leq 1 \\quad \\text{for all } i, \\quad \\text{and} \\quad |v_m| = 1\n$$\n\nSubstitute $|v_m| = 1$:\n\n$$\n\\left| \\left( \\lambda D \\mathbf{v} \\right)_m \\right| = |\\lambda| |d_{m,m}|\n$$\n\nThus, the absolute value of the $m$-th row of $\\lambda D \\mathbf{v}$ is determined by $|\\lambda|$, the eigenvalue, and $|d_{m,m}|$, the diagonal entry of $D$ at row $m$.\n:::\n\n::: {.callout-tip icon=false appearance=\"simple\"}\n\n# **_EQUATION 1_**\n\n**Combining steps (2) and (3), we can write:**\n\n$$\n|\\lambda||d_{m,m}| = |r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n|\n$$\n\n:::\n\n::: {.callout-note icon=false}\n\n## **4. Explain why:**\n\n$$\n|r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n| \\leq \\sum_{j \\neq m} |r_{m,j}|\n$$\n\n:::\n\n::: {.callout-tip collapse=\"true\"}\n\n## **Answer**\n\nTo explain why\n\n$$\n\\big| r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n \\big| \\leq \\sum_{j \\neq m} |r_{m,j}|,\n$$\n\nwe begin by recalling the **triangle inequality** for absolute values. For any sum of terms $a_1, a_2, \\dots, a_k$, the triangle inequality ensures:\n\n$$\n|a_1 + a_2 + \\cdots + a_k| \\leq |a_1| + |a_2| + \\cdots + |a_k|\n$$\n\nIn our case, the sum of interest is:\n\n$$\nr_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n\n$$\n\nApplying the triangle inequality to this sum gives:\n\n$\\quad \\big| r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n \\big|$\n\n$$\n\\leq |r_{m,1}v_1| + |r_{m,2}v_2| + \\cdots + |r_{m,m-1}v_{m-1}| + |r_{m,m+1}v_{m+1}| + \\cdots + |r_{m,n}v_n|\n$$\n\nEach term in the sum has the form $|r_{m,j}v_j|$. Using the property of absolute values $|ab| = |a||b|$, we can rewrite each term as:\n\n$$\n|r_{m,j}v_j| = |r_{m,j}| \\cdot |v_j|\n$$\n\nSince it is assumed that $|v_j| \\leq 1$ for all $j$, it follows that:\n\n$$\n|r_{m,j}v_j| \\leq |r_{m,j}|\n$$\n\nSubstituting this bound for each term into the inequality gives:\n\n$\\quad \\big| r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n \\big|$\n\n$$\n\\leq |r_{m,1}v_1| + |r_{m,2}v_2| + \\cdots + |r_{m,m-1}v_{m-1}| + |r_{m,m+1}v_{m+1}| + \\cdots + |r_{m,n}v_n|\n$$\n\nThe indices $j \\neq m$ correspond to all off-diagonal entries in the $m$-th row of the matrix. Thus, we can express the sum of the absolute values of the coefficients as:\n\n$$\n|r_{m,1}| + |r_{m,2}| + \\cdots + |r_{m,m-1}| + |r_{m,m+1}| + \\cdots + |r_{m,n}| = \\sum_{j \\neq m} |r_{m,j}|\n$$\n\nSubstituting this back, we find:\n\n$$\n\\big| r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n \\big| \\leq \\sum_{j \\neq m} |r_{m,j}|\n$$\n\nThe inequality holds because:\n\n1. The **triangle inequality** ensures that the absolute value of a sum is at most the sum of the absolute values of its terms.\n2. The assumption $|v_j| \\leq 1$ allows us to bound $|r_{m,j}v_j|$ by $|r_{m,j}|$.\n\nThus, the magnitude of the weighted sum of $v_j$ values (for $j \\neq m$) is always less than or equal to the sum of the absolute values of the off-diagonal entries in the $m$-th row of the matrix.\n:::\n\n::: {.callout-note icon=false}\n\n## **5. Explain why:**\n\n$$\n\\sum_{j \\neq m} |r_{m,j}| < |d_{m,m}|\n$$\n\n:::\n\n::: {.callout-tip collapse=\"true\"}\n\n## **Answer**\n\nBy assumption, the matrix $A$ is strictly diagonally dominant. This means that, for each row of $A$, the absolute value of the diagonal entry $|d_{m,m}|$ is strictly greater than the sum of the absolute values of all the off-diagonal entries in that row:\n\n$$\n|d_{m,m}| > \\sum_{j \\neq m} |r_{m,j}|\n$$\n\nIn other words, the diagonal entry $d_{m,m}$ has the largest contribution in the row, ensuring that the total influence of the off-diagonal terms is strictly smaller.\n:::\n\n::: {.callout-note icon=false}\n\n## **6. Use the results from Steps (4) and (5) with _EQUATION 1_ to show show that:**\n\n$$\n|\\lambda||d_{m,m}| < |d_{m,m}|\n$$\n\nWhat does this say about $|\\lambda|$?\n\n:::\n\n::: {.callout-tip collapse=\"true\"}\n\n## **Answer**\n\nTo explain why\n\n$$\n|\\lambda||d_{m,m}| < |d_{m,m}|\n$$\n\nwe combine the results from **Step (4)** and **Step (5)** with **_EQUATION 1_**.\n\n**_EQUATION 1_** states:\n\n$$\n|\\lambda||d_{m,m}| = \\big| r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n \\big|\n$$\n\nFrom **Step (4)**, we know:\n\n$$\n\\big| r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n \\big| \\leq \\sum_{j \\neq m} |r_{m,j}|\n$$\n\nSubstituting this into **_EQUATION 1_**, we have:\n\n$$\n|\\lambda||d_{m,m}| \\leq \\sum_{j \\neq m} |r_{m,j}|\n$$\n\nFrom **Step (5)**, we know:\n\n$$\n\\sum_{j \\neq m} |r_{m,j}| < |d_{m,m}|\n$$\n\nCombining this with the inequality above gives:\n\n$$\n|\\lambda||d_{m,m}| < |d_{m,m}|\n$$\n\nDividing both sides of the inequality by $|d_{m,m}|$ (which is nonzero), we find:\n\n$$\n|\\lambda| < 1\n$$\n\nThis result shows that the magnitude of the eigenvalue $|\\lambda|$ is strictly less than 1 and this implies that the spectral radius $\\rho(D^{-1}R) < 1$.\n:::\n\n::: {.callout-note icon=false}\n\n## **7. Final Conclusion**\n\nSince $\\lambda$ is an arbitrary eigenvalue, then $|\\lambda|_{\\text{max}} < 1$. In other words, the spectral radius $\\rho(D^{-1}R) < 1$. Thus, by the **Spectral Radius Convergence Theorem** (Theorem A.7), the Jacobi Method (iteration with $A = D^{-1}R$) converges for any starting point $\\mathbf{x}_0$.\n\nLet $\\mathbf{x}_* = \\lim_{k \\to \\infty} \\mathbf{x}_k$, and show that $\\mathbf{x}_*$ is the solution to $A \\mathbf{x} = \\mathbf{b}$, so $A$ must be nonsingular. This completes the proof of the **Diagonal Dominance Convergence Theorem** (Theorem 2.10).\n\n:::\n\n::: {.callout-tip collapse=\"true\"}\n\n## **Answer**\n\nFrom earlier, we showed that the spectral radius $\\rho(D^{-1}R)$, which is the largest magnitude of the eigenvalues of $D^{-1}R$, satisfies:\n\n$$\n\\rho(D^{-1}R) < 1\n$$\n\nThis result guarantees that the Jacobi method converges to a solution $\\mathbf{x}_*$ of $A\\mathbf{x} = \\mathbf{b}$ for any starting point $\\mathbf{x}_0$, as stated in the **Diagonal Dominance Convergence Theorem** (_Theorem A.7_).\n\nTherefore, we can write the limit of the iterates $\\mathbf{x}_k$ as:\n\n$$\n\\mathbf{x}_* = \\lim_{k \\to \\infty} \\mathbf{x}_k\n$$\n\nThe Jacobi iteration formula is:\n\n$$\n\\mathbf{x}_{k+1} = D^{-1}(\\mathbf{b} - R\\mathbf{x}_k)\n$$\n\nwhere $A = D - R$, $D$ is the diagonal matrix, and $R$ is the remainder matrix (containing the off-diagonal terms).\n\nSubstituting the limit $\\mathbf{x}_*$ into this equation (as $\\mathbf{x}_{k+1} \\to \\mathbf{x}_*$ and $\\mathbf{x}_k \\to \\mathbf{x}_*$), we get:\n\n$$\n\\mathbf{x}_* = D^{-1}(\\mathbf{b} - R\\mathbf{x}_*)\n$$\n\nExpanding this:\n\n$$\n\\mathbf{x}_* = -D^{-1}R\\mathbf{x}_* + D^{-1}\\mathbf{b}\n$$\n\nRewriting:\n\n::: {.callout-tip icon=false appearance=\"simple\" style=\"background-color: #fffff; border-left: 5px solid #9c27b0; color: #9c27b0; font-weight: 900;\"}\n\n# ‎\n\n$$\n\\mathbf{x}_* = -D^{-1}(L + U)\\mathbf{x}_* + D^{-1}\\mathbf{b}\n$$\n\n:::\n\nwhere $R = L + U$, with $L$ being the strictly lower triangular part of $A$ and $U$ the strictly upper triangular part.\n\nTo verify $\\mathbf{x}_*$ satisfies $A\\mathbf{x}_* = \\mathbf{b}$, substitute $A = D + L + U$ into the system:\n\n$$\n(D + L + U)\\mathbf{x}_* = \\mathbf{b}\n$$\n\nMultiply both sides by $D^{-1}$:\n\n$$\n\\mathbf{x}_* + D^{-1}(L + U)\\mathbf{x}_* = D^{-1}\\mathbf{b}\n$$\n\nRearranging terms gives:\n\n$$\n\\mathbf{x}_* = -D^{-1}(L + U)\\mathbf{x}_* + D^{-1}\\mathbf{b}\n$$\n\nThis matches the Jacobi iteration formula, verifying $\\mathbf{x}_*$ satisfies $A\\mathbf{x}_* = \\mathbf{b}$.\n\n**Conclusion**\n\n1. We proved that $\\rho(D^{-1}R) < 1$, so the Jacobi method converges to $\\mathbf{x}_*$.\n2. Substituting $\\mathbf{x}_*$ into $A\\mathbf{x} = \\mathbf{b}$, we verified that it satisfies the system.\n3. Since $A\\mathbf{x}_* = \\mathbf{b}$ has a solution, $A$ is nonsingular.\n\n:::\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":{"source":false,"toggle":true,"caption":"See code"},"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../../../styles.css"],"toc":true,"self-contained":true,"toc-depth":3,"number-sections":false,"html-math-method":"katex","embed-resources":true,"output-file":"jacobi-method-convergence.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","theme":{"light":"flatly","dark":"darkly"},"title":"Jacobi Method - Convergence Proof","preview":{"port":3000,"browser":true,"watch-inputs":true,"navigate":true},"page-layout":"full","toc-location":"right","code-summary":"Show Code","code-copy":"hover"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}