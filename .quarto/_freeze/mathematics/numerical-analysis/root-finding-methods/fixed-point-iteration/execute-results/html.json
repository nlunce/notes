{
  "hash": "163fd17a40fa55e1314faefd7c3c0ef7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Fixed-Point Iteration'\nauthor: 'Nathan Lunceford'\nformat:\n  html:\n    self-contained: true\n    page-layout: full\n    toc: true\n    toc-depth: 3\n    toc-location: right\n    number-sections: false\n    html-math-method: katex\n    embed-resources: true\n    code-fold: true\n    code-summary: 'Show Code'\n    code-overflow: wrap\n    code-copy: hover\n    code-tools:\n      source: false\n      toggle: true\n      caption: See code\nengine: jupyter\npreview:\n  port: 3000\n  browser: true\n  watch-inputs: true\n  navigate: true\n---\n\n\n## **Overview**\n\n**Fixed-Point Iteration** is a simple numerical method for solving equations of the form $x = g(x)$. It is based on the idea of iteratively applying a function $g(x)$ to approximate a fixed point $x^*$, where $x^* = g(x^*)$. This method is commonly used in numerical root-finding and optimization.\n\n## **Fixed-Point Iteration Formula**\n\nThe fixed-point iteration method uses the recursive formula:\n\n$$\nx_{k+1} = g(x_k)\n$$\n\nwhere $x_k$ is the $k$-th approximation of the solution. Starting from an initial guess $x_0$, the sequence of approximations is generated iteratively, and convergence is expected when $|x_{k+1} - x_k|$ becomes sufficiently small.\n\n## **Convergence Criteria**\n\nFor the fixed-point iteration to converge, certain conditions must be met:\n\n1. The function $g(x)$ must be continuous.\n2. The derivative $g'(x)$ at the fixed point $x^*$ must satisfy:\n   $$\n   |g'(x^*)| < 1\n   $$\n\nIf $|g'(x^*)| \\geq 1$, the method may fail to converge.\n\n## **Step-by-Step Procedure**\n\n1. Rewrite the given equation $f(x) = 0$ in the form $x = g(x)$.\n2. Choose an initial guess $x_0$.\n3. Apply the iteration formula:\n   $$\n   x_{k+1} = g(x_k)\n   $$\n4. Repeat the iteration until $|x_{k+1} - x_k| < \\epsilon$, where $\\epsilon$ is the tolerance.\n5. The final $x_k$ is an approximate solution.\n\n## **Example**\n\nConsider the equation:\n\n$$\nx^2 - 2 = 0\n$$\n\nRewriting it as $x = g(x)$:\n\n$$\ng(x) = \\frac{2}{x}\n$$\n\n### **Iterative Steps**\n\n1. **Initial Guess**: $x_0 = 1.5$.\n2. **First Iteration**:\n   $$\n   x_1 = g(x_0) = \\frac{2}{1.5} = 1.3333\n   $$\n3. **Second Iteration**:\n   $$\n   x_2 = g(x_1) = \\frac{2}{1.3333} \\approx 1.5\n   $$\n4. **Third Iteration**:\n   $$\n   x_3 = g(x_2) = \\frac{2}{1.5} \\approx 1.3333\n   $$\n\nThe values oscillate around the solution $\\sqrt{2}$. With more iterations and a smaller tolerance $\\epsilon$, the method converges to the actual solution.\n\n## **Convergence and Stability**\n\nFor fixed-point iteration to converge:\n\n- $|g'(x^*)| < 1$ ensures stability near the fixed point.\n- Poorly chosen $g(x)$ or initial guesses can lead to divergence or slow convergence.\n\n### **Example of Divergence**\n\nIf $g(x)$ is poorly chosen such that $|g'(x^*)| > 1$, the method may fail to converge. For instance, using $g(x) = x^2$ for the same equation would cause the iteration to diverge.\n\n## **Applications**\n\n- **Root-Finding**: Solve equations like $f(x) = 0$.\n- **Dynamical Systems**: Analyze equilibrium points.\n- **Optimization**: Solve constraints arising in optimization problems.\n\n## **Advantages and Limitations**\n\n### **Advantages**\n\n1. **Simple Implementation**: Requires minimal computation.\n2. **Versatile**: Applicable to a wide range of problems.\n\n### **Limitations**\n\n1. **Convergence Issues**: Sensitive to $g(x)$ and initial guess.\n2. **Slow Convergence**: May require many iterations for highly accurate solutions.\n\n",
    "supporting": [
      "fixed-point-iteration_files"
    ],
    "filters": [],
    "includes": {}
  }
}