{
  "hash": "624da0b05045e4e47197af307457f33d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers'\nauthor: 'Nathan Lunceford'\nformat:\n  html:\n    self-contained: true\n    page-layout: full\n    toc: true\n    toc-depth: 3\n    toc-location: right\n    number-sections: false\n    html-math-method: katex\n    embed-resources: true\n    code-fold: true\n    code-summary: 'Show the code'\n    code-overflow: wrap\n    code-copy: hover\n    code-tools:\n      source: false\n      toggle: true\n      caption: See code\nengine: jupyter\npreview:\n  port: 3000\n  browser: true\n  watch-inputs: true\n  navigate: true\n---\n\n\n## **Overview**\n\nWhen solving linear systems of the form $A\\mathbf{x} = \\mathbf{b}$, two significant issues may arise:\n\n1. **Controllable Errors**: Errors due to computational methods, which we can manage or minimize.\n2. **Uncontrollable Errors**: Errors inherent to the problem's nature, which we cannot eliminate but need to understand.\n\nThis document explores vector and matrix norms, their importance in numerical computations, and how they relate to error analysis and condition numbers when solving $A\\mathbf{x} = \\mathbf{b}$.\n\n## **Vector Norms**\n\nA **norm** is a function that assigns a non-negative length or size to vectors in a vector space. Norms help measure the magnitude of vectors, which is essential in analyzing algorithms and numerical stability.\n\nFor a vector $\\mathbf{v} = [v_1, v_2, \\dots, v_n]^T$, common norms include:\n\n### **1. $\\ell_2$-Norm (Euclidean Norm)**\n\nMeasures the straight-line distance from the origin to the point $\\mathbf{v}$:\n\n$$\n\\|\\mathbf{v}\\|_2 = \\sqrt{v_1^2 + v_2^2 + \\dots + v_n^2}\n$$\n\n### **2. $\\ell_1$-Norm (Taxicab or Manhattan Norm)**\n\nSums the absolute values of the vector components:\n\n$$\n\\|\\mathbf{v}\\|_1 = |v_1| + |v_2| + \\dots + |v_n|\n$$\n\n### **3. $\\ell_\\infty$-Norm (Maximum or Infinity Norm)**\n\nTakes the maximum absolute value among the components:\n\n$$\n\\|\\mathbf{v}\\|_\\infty = \\max_{1 \\leq i \\leq n} |v_i|\n$$\n\nAlso expressed as:\n\n$$\n\\|\\mathbf{v}\\|_\\infty = \\lim_{p \\to \\infty} \\left( \\sum_{i=1}^n |v_i|^p \\right)^{1/p}.\n$$\n\n## **Matrix Norms**\n\nMatrix norms measure the \"size\" of matrices and help understand how matrices affect vector norms when used as linear transformations.\n\nCommon matrix norms include:\n\n### **1. Spectral Norm ($\\|A\\|_2$)**\n\nInvolves the largest singular value of $A$:\n\n$$\n\\|A\\|_2 = \\sqrt{\\lambda_{\\text{max}}(A^TA)},\n$$\n\nwhere $\\lambda_{\\text{max}}$ is the largest eigenvalue of $A^TA$.\n\n### **2. Maximum Column Sum Norm ($\\|A\\|_1$)**\n\nThe largest sum of absolute values in any column:\n\n$$\n\\|A\\|_1 = \\max_j \\sum_i |a_{ij}|\n$$\n\n### **3. Maximum Row Sum Norm ($\\|A\\|_\\infty$)**\n\nThe largest sum of absolute values in any row:\n\n$$\n\\|A\\|_\\infty = \\max_i \\sum_j |a_{ij}|\n$$\n\n## **The Matrix Infinity Norm and Row Sums**\n\n### **Definition of $\\|A\\|_\\infty$**\n\nFormally defined as:\n\n$$\n\\|A\\|_\\infty = \\sup_{\\mathbf{x} \\neq 0} \\frac{\\|A\\mathbf{x}\\|_\\infty}{\\|\\mathbf{x}\\|_\\infty} = \\sup_{\\|\\mathbf{x}\\|_\\infty = 1} \\|A\\mathbf{x}\\|_\\infty\n$$\n\n### **Key Insight**\n\n- The infinity norm of $A$ is equal to the maximum row sum of $A$.\n- It represents the maximum effect $A$ can have on any vector $\\mathbf{x}$ with $\\|\\mathbf{x}\\|_\\infty = 1$.\n\n## **Proof: $\\|A\\|_\\infty =$ Maximum Row Sum of $A$**\n\nFor matrix $A$:\n\n$$\n\\|A\\|_\\infty = \\max_i \\sum_j |a_{ij}|.\n$$\n\n### **Example**\n\nConsider:\n\n$$\nA = \\begin{bmatrix}\n-1 & -2 & 3 \\\\\n3 & -4 & -5 \\\\\n-2 & 3 & -4\n\\end{bmatrix}\n$$\n\n#### **Compute Row Sums**\n\n1. **Row 1**: $|-1| + |-2| + |3| = 6$\n2. **Row 2**: $|3| + |-4| + |-5| = 12$\n3. **Row 3**: $|-2| + |3| + |-4| = 9$\n\n**Maximum Row Sum**: $12$ (Row 2)\n\nTherefore, $\\|A\\|_\\infty = 12$.\n\n### **Choosing $\\mathbf{\\hat{x}}$**\n\nTo achieve $\\|A\\mathbf{\\hat{x}}\\|_\\infty = \\|A\\|_\\infty$, select $\\mathbf{\\hat{x}}$ with $\\|\\mathbf{\\hat{x}}\\|_\\infty = 1$ that aligns with the signs of Row 2:\n\n$$\n\\mathbf{\\hat{x}} = \\begin{bmatrix}\n1 \\\\\n-1 \\\\\n-1\n\\end{bmatrix}\n$$\n\n#### **Compute $A\\mathbf{\\hat{x}}$:**\n\n$$\nA\\mathbf{\\hat{x}} = \\begin{bmatrix}\n-1(1) + -2(-1) + 3(-1) \\\\\n3(1) + -4(-1) + -5(-1) \\\\\n-2(1) + 3(-1) + -4(-1)\n\\end{bmatrix} = \\begin{bmatrix}\n-2 \\\\\n12 \\\\\n-1\n\\end{bmatrix}\n$$\n\n#### **Compute $\\|A\\mathbf{\\hat{x}}\\|_\\infty$:**\n\n$$\n\\|A\\mathbf{\\hat{x}}\\|_\\infty = \\max(|-2|, |12|, |-1|) = 12 = \\|A\\|_\\infty\n$$\n\n### **Conclusion**\n\nIt is **not possible** to find $\\mathbf{\\hat{x}}$ such that $\\|A\\mathbf{\\hat{x}}\\|_\\infty > \\|A\\|_\\infty$ when $\\|\\mathbf{\\hat{x}}\\|_\\infty = 1$, because $\\|A\\|_\\infty$ is the supremum of $\\|A\\mathbf{x}\\|_\\infty$ over all such $\\mathbf{x}$.\n\n## **Error Analysis and Condition Numbers**\n\n### **Why Care About Errors in $A\\mathbf{x} = \\mathbf{b}$?**\n\nWhen solving $A\\mathbf{x} = \\mathbf{b}$, understanding errors helps improve numerical accuracy and stability.\n\nLet $\\mathbf{x_a}$ be an **approximate solution** to $A\\mathbf{x} = \\mathbf{b}$, meaning:\n\n$$\nA\\mathbf{x_a} \\neq \\mathbf{b}\n$$\n\nWe quantify errors to assess the accuracy of $\\mathbf{x_a}$ and evaluate the impact of approximations.\n\n### **Definitions**\n\n- **Residual**: The difference between $\\mathbf{b}$ and $A\\mathbf{x_a}$:\n\n  $$\n  \\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a}\n  $$\n\n- **Backward Error (BE)**: Measures the infinity norm of the residual:\n\n  $$\n  \\text{BE} = \\|\\mathbf{r}\\|_\\infty = \\|\\mathbf{b} - A\\mathbf{x_a}\\|_\\infty\n  $$\n\n- **Relative Backward Error (RBE)**: Normalizes the backward error relative to $\\mathbf{b}$:\n\n  $$\n  \\text{RBE} = \\frac{\\|\\mathbf{r}\\|_\\infty}{\\|\\mathbf{b}\\|_\\infty}\n  $$\n\n- **Forward Error (FE)**: Measures the difference between the true solution $\\mathbf{x}$ and the approximate solution $\\mathbf{x_a}$:\n\n  $$\n  \\text{FE} = \\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty\n  $$\n\n- **Relative Forward Error (RFE)**: Normalizes the forward error relative to $\\mathbf{x}$:\n\n  $$\n  \\text{RFE} = \\frac{\\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty}{\\|x\\|_\\infty}\n  $$\n\n## **Error Magnification Factor (EMF)**\n\nThe **error magnification factor (EMF)** relates the relative forward error (RFE) to the relative backward error (RBE):\n\n$$\n\\text{EMF} = \\frac{\\text{RFE}}{\\text{RBE}}\n$$\n\nThis quantifies how much the backward error is amplified when reflected in the forward error.\n\n## **Condition Number of a Matrix**\n\nThe **condition number** of a matrix $A$ measures the sensitivity of the solution $\\mathbf{x}$ to changes in $\\mathbf{b}$. It is defined as:\n\n$$\n\\text{cond}(A) = \\|A\\|_\\infty \\cdot \\|A^{-1}\\|_\\infty\n$$\n\n### **Interpretation:**\n\n- A **low condition number** (close to 1) indicates a well-conditioned matrix.\n- A **high condition number** suggests an ill-conditioned matrix, meaning small changes in $\\mathbf{b}$ can result in large changes in $\\mathbf{x}$.\n\n## **Example**\n\nGiven:\n\n$$\n\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}, \\quad\n\\mathbf{x_a} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\quad\nA = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}\n$$\n\n### **Step 1: Compute Errors**\n\n1. **Forward Error (FE)**:\n\n   $$\n   \\text{FE} = \\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty = \\left\\| \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\right\\|_\\infty = 1\n   $$\n\n2. **Relative Forward Error (RFE)**:\n\n   $$\n   \\text{RFE} = \\frac{\\text{FE}}{\\|\\mathbf{x}\\|_\\infty} = \\frac{1}{2} = 0.5\n   $$\n\n3. **Residual**:\n\n   $$\n   r = b - Ax_a = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n   $$\n\n4. **Backward Error (BE)**:\n\n   $$\n   \\text{BE} = \\|\\mathbf{r}\\|_\\infty = 3\n   $$\n\n5. **Relative Backward Error (RBE)**:\n\n   $$\n   \\text{RBE} = \\frac{\\text{BE}}{\\|\\mathbf{b}\\|_\\infty} = \\frac{3}{3} = 1\n   $$\n\n### **Step 2: Compute EMF**\n\nUsing:\n\n$$\n\\text{EMF} = \\frac{\\text{RFE}}{\\text{RBE}}\n$$\n\nwe find:\n\n$$\n\\text{EMF} = \\frac{0.5}{1} = 0.5\n$$\n\n### **Step 3: Compute Condition Number**\n\n1. **Compute $\\|A\\|_\\infty$**:\n\n   $$\n   \\|A\\|_\\infty = \\max\\left( |1| + |1|, |3| + |-4| \\right) = \\max(2, 7) = 7\n   $$\n\n2. **Compute $\\|A^{-1}\\|_\\infty$**:\n\n   From $A^{-1}$, we find:\n\n   $$\n   \\|A^{-1}\\|_\\infty = \\max\\left( \\frac{4}{7} + \\frac{1}{7}, \\frac{3}{7} + \\frac{1}{7} \\right) = \\frac{5}{7}\n   $$\n\n3. **Condition Number**:\n\n   $$\n   \\text{cond}(A) = \\|A\\|_\\infty \\cdot \\|A^{-1}\\|_\\infty = 7 \\cdot \\frac{5}{7} = 5\n   $$\n\n## **Next Steps: $PA = LU$ Decomposition (Partial Pivoting)**\n\n### **What is Partial Pivoting?**\n\nPartial pivoting rearranges rows of $A$ during LU decomposition to place the largest available pivot element on the diagonal. This ensures numerical stability by reducing rounding errors.\n\n### **Consequences of Partial Pivoting:**\n\n1. **Controlled Multipliers**: Ensures all multipliers satisfy $|m_{ij}| \\leq 1$.\n2. **Prevents Swamping**: Avoids large numerical errors caused by small pivot elements.\n\nUnderstanding norms, errors, and condition numbers is foundational for solving $A\\mathbf{x} = \\mathbf{b}$ efficiently and accurately.\n\n",
    "supporting": [
      "2-3_files"
    ],
    "filters": [],
    "includes": {}
  }
}