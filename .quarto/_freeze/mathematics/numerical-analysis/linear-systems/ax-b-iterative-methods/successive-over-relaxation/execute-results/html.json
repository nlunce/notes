{
  "hash": "d05ad67883b17f90fd5195acb3436246",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'The Successive Over-Relaxation (SOR) Method for Solving Linear Systems'\nauthor: 'Nathan Lunceford'\nformat:\n  html:\n    self-contained: true\n    page-layout: full\n    toc: true\n    toc-depth: 3\n    toc-location: right\n    number-sections: false\n    html-math-method: katex\n    embed-resources: true\n    code-fold: true\n    code-summary: 'Show Code'\n    code-overflow: wrap\n    code-copy: hover\n    code-tools:\n      source: false\n      toggle: true\n      caption: See code\nengine: jupyter\npreview:\n  port: 3000\n  browser: true\n  watch-inputs: true\n  navigate: true\n---\n\n\n## **Overview**\n\nThe **Successive Over-Relaxation (SOR) Method** is an extension of the [**Gauss-Seidel Method**](./gauss-seidel-method.html) used to solve [**systems of linear equations**](../../w06/linear-systems.html). By introducing a relaxation parameter $\\omega$, the SOR method accelerates convergence or allows fine-tuning of the iterative process.\n\nThe SOR method is particularly useful when $\\omega$ is chosen appropriately, typically $1 < \\omega < 2$ for over-relaxation.\n\n## **The SOR Method**\n\nConsider the system:\n\n$$\nA\\mathbf{x} = \\mathbf{b}\n$$\n\nwhere $A$ is decomposed into:\n\n- $D$: The diagonal components of $A$,\n- $L$: The strictly lower triangular components of $A$,\n- $U$: The strictly upper triangular components of $A$.\n\nThus:\n\n$$\nA = D + L + U\n$$\n\nThe SOR iterative formula is:\n\n$$\n \\mathbf{x}_{k+1} = (\\omega L + D)^{-1} \\left[ (1 - \\omega)D\\mathbf{x}_{k} - \\omega U\\mathbf{x}_{k} \\right] + \\omega (D + \\omega L)^{-1} \\mathbf{b}\n$$\n\nfor $k = 0, 1, 2, \\dots$, where:\n\n- $\\omega$: Relaxation parameter ($\\omega = 1$ corresponds to the Gauss-Seidel Method).\n\n## **Algorithm**\n\n1. **Initial Guess:**\n   Start with an initial vector $\\mathbf{x}_0$.\n\n2. **Iterative Formula:**\n   For each iteration $k$, compute:\n\n   $$\n   \\mathbf{x}_{k+1} = (\\omega L + D)^{-1} \\left[ (1 - \\omega)D\\mathbf{x}_{k} - \\omega U\\mathbf{x}_{k} \\right] + \\omega (D + \\omega L)^{-1} \\mathbf{b}\n   $$\n\n3. **Relaxation Parameter:**\n   Choose $\\omega$:\n\n   - $\\omega > 1$: Over-relaxation (accelerates convergence).\n   - $\\omega = 1$: Equivalent to the Gauss-Seidel Method.\n   - $\\omega < 1$: Under-relaxation (may be used to stabilize divergence).\n\n4. **Convergence Check:**\n   Stop when the norm of the residual $\\|\\mathbf{b} - A\\mathbf{x}^{(k)}\\|$ is sufficiently small.\n\n## **Example**\n\n### **System of Equations**\n\nConsider the system:\n\n$$\n4u + v + w = 7, \\quad u + 3v + w = 8, \\quad u + v + 5w = 6\n$$\n\n### **Step 1: Decompose $A$**\n\nDecompose the coefficient matrix $A$:\n\n$$\nA = \\begin{bmatrix}\n4 & 1 & 1 \\\\\n1 & 3 & 1 \\\\\n1 & 1 & 5\n\\end{bmatrix}, \\quad\nD = \\begin{bmatrix}\n4 & 0 & 0 \\\\\n0 & 3 & 0 \\\\\n0 & 0 & 5\n\\end{bmatrix}, \\quad\nL = \\begin{bmatrix}\n0 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n1 & 1 & 0\n\\end{bmatrix}, \\quad\nU = \\begin{bmatrix}\n0 & 1 & 1 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n$$\n\n### **Step 2: Iterative Updates**\n\nUsing $\\omega = 1.25$ and an initial guess $\\mathbf{x}_0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$, compute the iterations:\n\n1. **Iteration 1 ($k = 1$):**\n\n   Substitute values into the SOR formula:\n\n   $$\n   u^{(1)} = \\frac{1.25}{4} \\left[ 7 - 0 - 0 \\right] = 2.1875\n   $$\n\n   $$\n   v^{(1)} = \\frac{1.25}{3} \\left[ 8 - 2.1875 - 0 \\right] = 1.9792\n   $$\n\n   $$\n   w^{(1)} = \\frac{1.25}{5} \\left[ 6 - 2.1875 - 1.9792 \\right] = 0.9188\n   $$\n\n   $$\n   \\mathbf{x}^{(1)} = \\begin{bmatrix} 2.1875 \\\\ 1.9792 \\\\ 0.9188 \\end{bmatrix}\n   $$\n\n2. **Iteration 2 ($k = 2$):**\n\n   Using updated values:\n\n   $$\n   u^{(2)} = \\dots, \\quad v^{(2)} = \\dots, \\quad w^{(2)} = \\dots\n   $$\n\n   Continue substituting until convergence.\n\n## **Convergence Conditions**\n\nThe SOR method converges under similar conditions to the Gauss-Seidel Method:\n\n1. If $A$ is **strictly diagonally dominant**, or\n2. If $A$ is **symmetric positive definite**.\n\nAdditionally, convergence depends on the choice of $\\omega$, with $1 < \\omega < 2$ typically achieving the fastest results.\n\n## **Advantages**\n\n- **Adjustable Convergence Speed:** The relaxation parameter $\\omega$ allows tuning for faster convergence.\n- **Efficiency:** For well-chosen $\\omega$, fewer iterations are required compared to the Gauss-Seidel Method.\n\n## **Limitations**\n\n- Requires tuning $\\omega$ for optimal performance.\n- May not converge if $\\omega$ is poorly chosen.\n- Not inherently parallelizable like the Jacobi Method.\n\n## **Summary**\n\nThe **Successive Over-Relaxation (SOR) Method** improves upon the Gauss-Seidel Method by introducing a relaxation parameter $\\omega$, enabling faster convergence for well-conditioned systems. However, the method requires careful parameter selection and is sensitive to the properties of the system matrix $A$.\n\n",
    "supporting": [
      "successive-over-relaxation_files"
    ],
    "filters": [],
    "includes": {}
  }
}