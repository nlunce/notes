[
  {
    "objectID": "mathematics/numerical-analysis/root-finding-methods/newtons-method.html",
    "href": "mathematics/numerical-analysis/root-finding-methods/newtons-method.html",
    "title": "Newton’s Method",
    "section": "",
    "text": "Newton’s Method, also known as the Newton-Raphson Method, is a widely used numerical method for finding successively better approximations to the roots (or zeros) of a real-valued function. It is particularly efficient when the initial guess is close to the actual root and when the function is well-behaved (smooth and differentiable)."
  },
  {
    "objectID": "mathematics/numerical-analysis/root-finding-methods/newtons-method.html#the-newtons-method-formula",
    "href": "mathematics/numerical-analysis/root-finding-methods/newtons-method.html#the-newtons-method-formula",
    "title": "Newton’s Method",
    "section": "The Newton’s Method Formula",
    "text": "The Newton’s Method Formula\nNewton’s Method is based on using the tangent line at an approximation of the root to generate a better approximation. The formula for generating the next approximation x_{k+1} from the current approximation x_k is given by:\n\nx_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}\n\nwhere:\n\nf(x) is the function whose root we are trying to find.\nf'(x) is the derivative of f(x).\nx_k is the current approximation, and x_{k+1} is the next approximation.\n\n\nGeometrical Interpretation\nNewton’s Method can be interpreted geometrically: given an approximation x_k, the tangent line to the curve y = f(x) at the point $ (x*k, f(x_k)) $ is used to estimate where the curve crosses the x-axis, which provides the next approximation x*{k+1}.\n\n\nConvergence Criteria\nNewton’s Method converges quadratically under certain conditions, which means that the number of correct digits roughly doubles with each iteration. However, this fast convergence occurs only if:\n\nThe function f(x) is continuous and differentiable in the vicinity of the root.\nThe derivative f'(x) is non-zero at the root.\nThe initial guess is sufficiently close to the actual root.\n\nIf the initial guess is too far from the root, Newton’s Method may fail to converge or may converge very slowly.\n\n\nStep-by-Step Procedure\n\nInitial Guess: Choose an initial approximation x_0.\nIteration Formula: Compute successive approximations using the formula:\n\n\nx_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}\n\n\nRepeat: Continue iterating until |x_{k+1} - x_k| &lt; \\epsilon, where \\epsilon is a small tolerance value, or until |f(x_k)| &lt; \\epsilon.\n\n\n\nExample\nLet’s solve the equation f(x) = x^2 - 2 = 0 using Newton’s Method, which has a root at x = \\sqrt{2}.\n\nFunction and Derivative:\n\nf(x) = x^2 - 2\n\n\nf'(x) = 2x\n\nInitial Guess: Let x_0 = 1.5.\nFirst Iteration:\n\nx_1 = x_0 - \\frac{f(x_0)}{f'(x_0)} = 1.5 - \\frac{1.5^2 - 2}{2(1.5)} = 1.4167\n\nSecond Iteration:\n\nx_2 = x_1 - \\frac{f(x_1)}{f'(x_1)} = 1.4167 - \\frac{1.4167^2 - 2}{2(1.4167)} = 1.4142\n\nFurther Iterations: Repeat until the difference between successive approximations is less than a specified tolerance (e.g., \\epsilon = 10^{-5}).\n\nIn this case, after just a few iterations, we have a highly accurate approximation of \\sqrt{2}.\n\n\nGeneral Properties of Newton’s Method\n\nQuadratic Convergence: When close to the root, Newton’s Method converges quadratically, meaning that the error decreases roughly as the square of the previous error.\nRequires Derivatives: Unlike the Bisection Method, Newton’s Method requires that the derivative f'(x) be known and be non-zero at the root.\nSensitive to Initial Guess: The method is sensitive to the initial guess, and poor choices of x_0 can lead to divergence or slow convergence.\n\n\n\nApplications of Newton’s Method\n\nRoot Finding: Newton’s Method is widely used to find roots of non-linear equations in mathematics, physics, engineering, and economics.\nOptimization: Newton’s Method is the basis of Newton’s optimization method, which is used to find local minima or maxima of differentiable functions by solving f'(x) = 0.\nEngineering and Modeling: It is used to solve non-linear models and systems, especially in fields like structural engineering, fluid dynamics, and electrical circuit analysis.\n\n\n\nAdvantages of Newton’s Method\n\nFast Convergence: When it converges, Newton’s Method is extremely fast due to its quadratic convergence rate.\nSimple Iterative Formula: The iteration formula is straightforward and easy to implement.\nFew Iterations: For well-behaved functions and good initial guesses, only a few iterations are required to obtain a highly accurate result.\n\n\n\nLimitations of Newton’s Method\n\nRequires Derivatives: The method requires that f(x) is differentiable, and that the derivative f'(x) can be computed analytically or numerically.\nRisk of Divergence: If the initial guess is too far from the root or if f'(x) is zero or near zero, the method may diverge or fail to converge.\nSlow or No Convergence: For functions with inflection points or flat regions near the root, the method may converge very slowly or not at all. In these cases, alternative methods like the Secant Method or Bisection Method may be better suited.\n\n\n\nConclusion\nNewton’s Method is a powerful and efficient tool for finding roots of equations, especially when the initial guess is close to the solution. While it requires the calculation of derivatives, its fast convergence makes it a preferred method when applicable. However, care must be taken with the choice of initial guess to avoid issues with divergence or slow convergence."
  },
  {
    "objectID": "mathematics/numerical-analysis/root-finding-methods/fixed-point-iteration.html",
    "href": "mathematics/numerical-analysis/root-finding-methods/fixed-point-iteration.html",
    "title": "Fixed-Point Iteration",
    "section": "",
    "text": "Fixed-Point Iteration is a simple numerical method for solving equations of the form x = g(x). It is based on the idea of iteratively applying a function g(x) to approximate a fixed point x^*, where x^* = g(x^*). This method is commonly used in numerical root-finding and optimization."
  },
  {
    "objectID": "mathematics/numerical-analysis/root-finding-methods/fixed-point-iteration.html#overview",
    "href": "mathematics/numerical-analysis/root-finding-methods/fixed-point-iteration.html#overview",
    "title": "Fixed-Point Iteration",
    "section": "",
    "text": "Fixed-Point Iteration is a simple numerical method for solving equations of the form x = g(x). It is based on the idea of iteratively applying a function g(x) to approximate a fixed point x^*, where x^* = g(x^*). This method is commonly used in numerical root-finding and optimization."
  },
  {
    "objectID": "mathematics/numerical-analysis/root-finding-methods/fixed-point-iteration.html#fixed-point-iteration-formula",
    "href": "mathematics/numerical-analysis/root-finding-methods/fixed-point-iteration.html#fixed-point-iteration-formula",
    "title": "Fixed-Point Iteration",
    "section": "Fixed-Point Iteration Formula",
    "text": "Fixed-Point Iteration Formula\nThe fixed-point iteration method uses the recursive formula:\n\nx_{k+1} = g(x_k)\n\nwhere x_k is the k-th approximation of the solution. Starting from an initial guess x_0, the sequence of approximations is generated iteratively, and convergence is expected when |x_{k+1} - x_k| becomes sufficiently small."
  },
  {
    "objectID": "mathematics/numerical-analysis/root-finding-methods/fixed-point-iteration.html#convergence-criteria",
    "href": "mathematics/numerical-analysis/root-finding-methods/fixed-point-iteration.html#convergence-criteria",
    "title": "Fixed-Point Iteration",
    "section": "Convergence Criteria",
    "text": "Convergence Criteria\nFor the fixed-point iteration to converge, certain conditions must be met:\n\nThe function g(x) must be continuous.\nThe derivative g'(x) at the fixed point x^* must satisfy: \n|g'(x^*)| &lt; 1\n\n\nIf |g'(x^*)| \\geq 1, the method may fail to converge."
  },
  {
    "objectID": "mathematics/numerical-analysis/root-finding-methods/fixed-point-iteration.html#step-by-step-procedure",
    "href": "mathematics/numerical-analysis/root-finding-methods/fixed-point-iteration.html#step-by-step-procedure",
    "title": "Fixed-Point Iteration",
    "section": "Step-by-Step Procedure",
    "text": "Step-by-Step Procedure\n\nRewrite the given equation f(x) = 0 in the form x = g(x).\nChoose an initial guess x_0.\nApply the iteration formula: \nx_{k+1} = g(x_k)\n\nRepeat the iteration until |x_{k+1} - x_k| &lt; \\epsilon, where \\epsilon is the tolerance.\nThe final x_k is an approximate solution."
  },
  {
    "objectID": "mathematics/numerical-analysis/root-finding-methods/fixed-point-iteration.html#example",
    "href": "mathematics/numerical-analysis/root-finding-methods/fixed-point-iteration.html#example",
    "title": "Fixed-Point Iteration",
    "section": "Example",
    "text": "Example\nConsider the equation:\n\nx^2 - 2 = 0\n\nRewriting it as x = g(x):\n\ng(x) = \\frac{2}{x}\n\n\nIterative Steps\n\nInitial Guess: x_0 = 1.5.\nFirst Iteration: \nx_1 = g(x_0) = \\frac{2}{1.5} = 1.3333\n\nSecond Iteration: \nx_2 = g(x_1) = \\frac{2}{1.3333} \\approx 1.5\n\nThird Iteration: \nx_3 = g(x_2) = \\frac{2}{1.5} \\approx 1.3333\n\n\nThe values oscillate around the solution \\sqrt{2}. With more iterations and a smaller tolerance \\epsilon, the method converges to the actual solution."
  },
  {
    "objectID": "mathematics/numerical-analysis/root-finding-methods/fixed-point-iteration.html#convergence-and-stability",
    "href": "mathematics/numerical-analysis/root-finding-methods/fixed-point-iteration.html#convergence-and-stability",
    "title": "Fixed-Point Iteration",
    "section": "Convergence and Stability",
    "text": "Convergence and Stability\nFor fixed-point iteration to converge:\n\n|g'(x^*)| &lt; 1 ensures stability near the fixed point.\nPoorly chosen g(x) or initial guesses can lead to divergence or slow convergence.\n\n\nExample of Divergence\nIf g(x) is poorly chosen such that |g'(x^*)| &gt; 1, the method may fail to converge. For instance, using g(x) = x^2 for the same equation would cause the iteration to diverge."
  },
  {
    "objectID": "mathematics/numerical-analysis/root-finding-methods/fixed-point-iteration.html#applications",
    "href": "mathematics/numerical-analysis/root-finding-methods/fixed-point-iteration.html#applications",
    "title": "Fixed-Point Iteration",
    "section": "Applications",
    "text": "Applications\n\nRoot-Finding: Solve equations like f(x) = 0.\nDynamical Systems: Analyze equilibrium points.\nOptimization: Solve constraints arising in optimization problems."
  },
  {
    "objectID": "mathematics/numerical-analysis/root-finding-methods/fixed-point-iteration.html#advantages-and-limitations",
    "href": "mathematics/numerical-analysis/root-finding-methods/fixed-point-iteration.html#advantages-and-limitations",
    "title": "Fixed-Point Iteration",
    "section": "Advantages and Limitations",
    "text": "Advantages and Limitations\n\nAdvantages\n\nSimple Implementation: Requires minimal computation.\nVersatile: Applicable to a wide range of problems.\n\n\n\nLimitations\n\nConvergence Issues: Sensitive to g(x) and initial guess.\nSlow Convergence: May require many iterations for highly accurate solutions."
  },
  {
    "objectID": "mathematics/numerical-analysis/quadrature/trapezoidal-rule.html",
    "href": "mathematics/numerical-analysis/quadrature/trapezoidal-rule.html",
    "title": "Trapezoidal Rule",
    "section": "",
    "text": "Overview\nThe trapezoidal rule is a numerical method for approximating definite integrals. It works by dividing the interval of integration into sub-intervals and approximating the area under the curve as a series of trapezoids. This method is particularly useful for complex functions or when an analytical solution is not feasible.\n\n\nGeneral Formula\nGiven the integral:\n\n\\int_a^b f(x) \\, dx\n\nThe trapezoidal rule estimates the integral by approximating the region under the curve with trapezoids.\n\n\nEqual Sub-Intervals (Uniform Spacing)\nWhen the interval [a, b] is divided into n equal sub-intervals of width:\n\n\\Delta x = \\frac{b - a}{n}\n\nThe composite trapezoidal rule can be expressed as:\n\nT_n = \\frac{\\Delta x}{2} \\left( y_0 + 2 y_1 + 2 y_2 + \\dots + 2 y_{n-1} + y_n \\right)\n\nwhere:\n\ny_0 = f(a) and y_n = f(b) are the function values at the endpoints.\ny_1, y_2, \\dots, y_{n-1} are the function values at the interior points, each multiplied by 2 because they are shared by two adjacent trapezoids.\n\n\nStep-by-Step Formula in Function Terms\nUsing function values at each point x_i = a + i \\Delta x, the formula becomes:\n\nT_n = \\frac{\\Delta x}{2} \\left( f(a) + 2 f(a + \\Delta x) + 2 f(a + 2 \\Delta x) + \\dots + 2 f(a + (n - 1) \\Delta x) + f(b) \\right)\n\nThis formula simplifies integration when the sub-intervals are uniformly spaced.\n\n\nSummation Notation for Uniform Spacing\nAlternatively, the composite trapezoidal rule for uniform spacing can be written as:\n\nT_n = \\frac{\\Delta x}{2} \\left( f(x_0) + 2 \\sum_{i=1}^{n-1} f(x_i) + f(x_n) \\right)\n\nwhere:\n\nx_0 = a, x_n = b, and x_i = a + i \\Delta x.\nThe summation accounts for the contributions of interior points.\n\n\n\n\nUnequal Sub-Intervals (Non-Uniform Spacing)\nWhen the sub-intervals are not equally spaced, the formula adjusts to account for varying widths \\Delta x_i between points. The non-uniform trapezoidal rule becomes:\n\nT = \\sum_{i=1}^{n} \\frac{\\Delta x_i}{2} \\left( f(x_{i-1}) + f(x_i) \\right)\n\nwhere:\n\nEach term computes the area of a trapezoid over the individual sub-interval [x_{i-1}, x_i].\nThe width of each sub-interval is \\Delta x_i = x_i - x_{i-1}, which may vary.\n\n\n\nComparison: Equal vs. Unequal Sub-Intervals\n\n\n\n\n\n\n\n\nProperty\nEqual Sub-Intervals\nUnequal Sub-Intervals\n\n\n\n\nWidth of Sub-Intervals\n\\Delta x = \\frac{b - a}{n} (constant)\n\\Delta x_i = x_i - x_{i-1} (varies)\n\n\nFormula\n\\frac{\\Delta x}{2} \\left( y_0 + 2 y_1 + \\dots + 2 y_{n-1} + y_n \\right)\n\\sum \\frac{\\Delta x_i}{2} \\left( f(x_{i-1}) + f(x_i) \\right)\n\n\nComputational Effort\nEasier to compute with uniform spacing\nMore complicated due to variable widths"
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc05/rc05.html",
    "href": "mathematics/numerical-analysis/projects/rc05/rc05.html",
    "title": "REALITY CHECK 05",
    "section": "",
    "text": "The use of Adaptive Quadrature is essential for maintaining constant speed along a specific path. This is a requirement in fields like computer-aided manufacturing, robotics and animation. Smooth and controlled movement is crucial for accuracy, but achieving a constant speed along a curved or complex path is challenging. Dividing a path into equal time intervals does not ensure equal-distance segments because the path’s shape influences the distance covered.\nTo address this, numerical methods are employed to divide the path into equal arc-length segments, ensuring consistent movement. The process involves several key steps:\n\nArc Length Measurement: The total length of the path is calculated using parametric equations, accounting for all curves and directional changes. This measurement provides the foundation for precise segmentation.\nMapping Path Position: To locate a point at a given distance s along the path, the corresponding parameter t is determined using numerical methods like Bisection or Newton’s Method. This ensures precise mapping of arc-length positions to their parametric coordinates.\nSegmenting the Path: The path is divided into segments of equal arc length, a process called equipartitioning. This segmentation ensures uniformity in the spacing of points along the path, regardless of its complexity or curvature.\nSmooth Traversal: Animations or simulations often demonstrate the practical effects of this approach. By comparing movement at constant parameter speed with movement along equal arc-length segments, the benefits of consistent, controlled traversal become clear, showcasing smoother and more predictable motion."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc05/rc05.html#path-equipartitioning-by-arc-length",
    "href": "mathematics/numerical-analysis/projects/rc05/rc05.html#path-equipartitioning-by-arc-length",
    "title": "REALITY CHECK 05",
    "section": "Path Equipartitioning by Arc Length",
    "text": "Path Equipartitioning by Arc Length\n\nProblem Statement\nEquipartition the path of Figure 5.6 into n subpaths of equal length, for n = 4 and n = 20. Plot analogues of Figure 5.6, showing the equipartitions.\n\n\n\n\n\n\n\n\n\n\n\nObjective and Approach\nThe objective is to partition the path, defined by parametric equations x(t) and y(t), into segments of equal arc length for a specified n. This method is valuable in fields requiring consistent movement along a path, such as animation or robotics.\nThe approach includes the following steps:\n\nCalculate Total Arc Length: Compute the total arc length from t = 0 to t = 1 using numerical integration, enabling calculation of the length of each segment.\n\n\\text{Segment length} = \\frac{\\text{Total arc length}}{n}\n\nLocate Partition Points Using the Bisection Method: For each segment i, use the Bisection Method to locate the parameter t_i so that the arc length from t = 0 to t = t_i equals i \\times \\text{Segment length}. This ensures equal arc lengths for each segment.\nPlot the Equipartitioned Path: Calculate x(t_i) and y(t_i) at each partition point and plot for both n = 4 and n = 20, visualizing uniform segmentation.\n\n\n\nSolution Code\n\n# Bisection method to find t for a target arc length fraction\ndef bisection_find_t(target_length, tol=1e-8):\n    a, b = 0, 1\n    while (b - a) / 2 &gt; tol:\n        midpoint = (a + b) / 2\n        if compute_arc_length(midpoint) == target_length:\n            return midpoint\n        elif compute_arc_length(midpoint) &lt; target_length:\n            a = midpoint\n        else:\n            b = midpoint\n    return (a + b) / 2\n\n# Equipartition function\ndef equipartition(n):\n    partition_points = [0]\n    total_length = compute_arc_length(1)\n    segment_length = total_length / n\n    for i in range(1, n):\n        target_length = i * segment_length\n        t_i = bisection_find_t(target_length)\n        partition_points.append(t_i)\n    partition_points.append(1)\n    return partition_points\n\n# Plot function for equipartitioned curve\ndef plot_styled_curve(n):\n    plt.figure(figsize=(8, 8), facecolor='white')\n\n    t_vals = np.linspace(0, 1, 500)\n    x_vals = x(t_vals)\n    y_vals = y(t_vals)\n\n    key_points_t = equipartition(n)\n    key_points_x = [x(t) for t in key_points_t]\n    key_points_y = [y(t) for t in key_points_t]\n\n    # Plot the curve with enhanced styling\n    plt.plot(x_vals, y_vals, color=\"#2196F3\", linewidth=2.5, zorder=3)\n    plt.scatter(key_points_x, key_points_y, color=\"#1565C0\", s=40, zorder=4)\n\n    # Add grid with softer appearance\n    plt.grid(True, linestyle='-', alpha=0.2, color='gray')\n    plt.xticks(np.arange(-1, 1.5, 0.5))\n    plt.yticks(np.arange(0, 2.5, 0.5))\n\n\n    # Enhanced axis lines\n    ax = plt.gca()\n    ax.set_xticklabels(['' if x == 0 else str(x) for x in ax.get_xticks()])\n    ax.set_yticklabels(['' if y == 0 else str(y) for y in ax.get_yticks()])\n\n    ax.spines['left'].set_position('zero')\n    ax.spines['bottom'].set_position('zero')\n    ax.spines['right'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n    ax.spines['left'].set_linewidth(1.5)\n    ax.spines['bottom'].set_linewidth(1.5)\n\n    # Enhance tick appearance\n    plt.tick_params(axis='both', which='major', length=6, width=1, colors='black', direction='out')\n    plt.tick_params(axis='both', which='minor', length=3, width=1, colors='black', direction='out')\n\n    # Label positioning and styling\n    ax.set_ylabel('y', rotation=0, labelpad=15, y=1.02, fontsize=12)\n    ax.set_xlabel('x', x=1.02, fontsize=12)\n\n    plt.xlim(-1.5, 1.5)\n    plt.ylim(-0.5, 2)\n    plt.gca().set_aspect('equal')\n\n    plt.show()\n\n\nEquipartitioned Curve with n = 4\n\n\n\n\n\n\n\n\n\n\n\nEquipartitioned Curve with n = 20\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation of Solution Components\n\nArc Length Calculation: The function compute_arc_length(s) uses numerical integration to compute the arc length from t = 0 to a given t = s.\nBisection Method for Partitioning: bisection_find_t(target_length) finds the parameter t corresponding to a specific arc length, ensuring accurate partition points.\nEquipartition Function: equipartition(n) calculates t-values for partitioning the path into n equal arc-length segments.\nVisualization: plot_styled_curve(n) generates a plot showing the path with points marking each partition.\n\n\n\nResults and Observations\nThe plots for n = 4 and n = 20 illustrate the uniform segmentation of the path into equal-length segments, confirming the effectiveness of the equipartitioning process. This approach achieves constant distances along the path, despite non-uniform parameter spacing.\n\n\nConclusion\nThe solution effectively partitions the path into equal-length segments using numerical integration and the Bisection Method. This method can be further enhanced by employing Newton’s Method for faster convergence or adapting it to three-dimensional paths."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc05/rc05.html#path-equipartitioning-using-newtons-method",
    "href": "mathematics/numerical-analysis/projects/rc05/rc05.html#path-equipartitioning-using-newtons-method",
    "title": "REALITY CHECK 05",
    "section": "Path Equipartitioning Using Newton’s Method",
    "text": "Path Equipartitioning Using Newton’s Method\n\nProblem Statement\nReplace the Bisection Method in Step 2 with Newton’s Method, and repeat Steps 2 and 3. What is the derivative needed? What is a good choice for the initial guess? Is computation time decreased by this replacement?\n\n\nObjective and Approach\n\nObjective: Use Newton’s Method to locate each partition point t_i along the path, ensuring equal arc-length segments for a specified number of partitions n. Newton’s Method is expected to offer faster convergence than the Bisection Method, especially when starting with a good initial guess.\nRequired Derivative: Newton’s Method requires the derivative of the arc length function with respect to t, which is simply the arc length integrand evaluated at t:\n\nf'(t) = \\sqrt{\\left( \\frac{dx}{dt} \\right)^2 + \\left( \\frac{dy}{dt} \\right)^2}\n\nInitial Guess: A reasonable initial guess for each t_i is t_i = \\frac{i}{n}, which provides a uniformly spaced initial estimate along t, aiding the convergence of Newton’s Method.\nPerformance Comparison: To evaluate if Newton’s Method reduces computation time, we will measure the time taken by both the Bisection and Newton’s methods to achieve the same accuracy.\n\n\nWhy t_i = \\frac{i}{n} is a Good Initial Guess\n\nUniform Parameter Distribution: The parameter t varies between 0 and 1 (or the specified range of t), and \\frac{i}{n} provides evenly spaced points within this interval. This ensures that the initial guess is distributed consistently across the parameter space.\nProximity to the True Solution: For smooth and “well-behaved” curves, the true t_i values for equal arc-length segments are often near \\frac{i}{n}. This proximity ensures that Newton’s Method starts “in the ballpark” of the correct value.\nSimplicity and Efficiency: Computing \\frac{i}{n} is computationally trivial and requires no extra effort. This simplicity makes it a practical choice compared to complex initialization schemes.\nImproved Convergence: Starting close to the actual solution allows Newton’s Method to converge quadratically, reducing the number of iterations needed to achieve the desired accuracy.\n\n\n\n\nSolution Code\nThe following Python code implements Newton’s Method to find partition points and compares its performance with the Bisection Method.\n\nimport time\n\n# Newton's Method to find t for a target arc length\ndef newton_find_t(target_length, initial_guess, tol=1e-8, max_iter=100):\n    t = initial_guess\n    for _ in range(max_iter):\n        f_t = compute_arc_length(t) - target_length\n        f_prime_t = integrand(t)\n        if abs(f_t) &lt; tol:\n            return t\n        t -= f_t / f_prime_t  # Update t\n    return t\n\n# Compare performance of Bisection and Newton's methods\ndef compare_performance(target_length):\n    start_time_bisection = time.time()\n    bisection_result = bisection_find_t(target_length)\n    bisection_time = time.time() - start_time_bisection\n\n    start_time_newton = time.time()\n    newton_result = newton_find_t(target_length, initial_guess=0.5)\n    newton_time = time.time() - start_time_newton\n\n    print(f\"Bisection Method Result: {bisection_result:.9f} Time: {bisection_time:.9f} seconds\")\n    print(f\"Newton's Method Result: {newton_result:.9f} Time: {newton_time:.9f} seconds\")\n\n# Example target length (e.g., half the arc length)\ntotal_length = compute_arc_length(1)\ncompare_performance(total_length / 2)\n\nBisection Method Result: 0.800593771 Time: 0.006999016 seconds\nNewton's Method Result: 0.800593767 Time: 0.000000000 seconds\n\n\n\n\nExplanation of Solution Components\n\nNewton’s Method Implementation: The newton_find_t function applies Newton’s Method to locate the parameter t for a given arc length. It iteratively refines t by calculating f(t) and f'(t), adjusting t based on the result.\nPerformance Comparison: The compare_performance function compares the time taken by Bisection and Newton’s methods to find the target t-value. This illustrates the efficiency difference between the two methods.\n\n\n\nResults and Observations\n\nPerformance Gain: Newton’s Method generally converges faster than the Bisection Method due to its quadratic convergence rate.\nAccuracy: With an appropriately chosen initial guess, Newton’s Method efficiently reaches an accurate solution within fewer iterations.\n\n\n\nConclusion\nNewton’s Method provides a more efficient approach for finding the partition points, particularly when an initial guess is available. This reduction in computation time makes it suitable for tasks requiring high precision and quick convergence, such as real-time applications in path traversal and equipartitioning. Future explorations could involve further optimizations by dynamically refining initial guesses based on prior calculations."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc05/rc05.html#path-animation-at-original-and-constant-speed",
    "href": "mathematics/numerical-analysis/projects/rc05/rc05.html#path-animation-at-original-and-constant-speed",
    "title": "REALITY CHECK 05",
    "section": "Path Animation at Original and Constant Speed",
    "text": "Path Animation at Original and Constant Speed\n\nProblem Statement\nUse Python animation commands to demonstrate traveling along the path in two ways:\n\nAt the original speed, based on parameter t for 0 \\leq t \\leq 1, which results in non-uniform speed along the path.\nAt a constant speed using t^*(s) for 0 \\leq s \\leq 1, where the path is re-parameterized to maintain equal arc-length segments.\n\n\n\nObjective and Approach\n\nObjective: To visualize the difference between non-uniform and constant-speed traversal along a path.\n\nOriginal Speed: Animate movement along the path based on evenly spaced t-values, resulting in variable speed.\nConstant Speed: Animate movement along the path with equal arc-length segments by using equipartition points t^*(s).\n\nApproach:\n\nOriginal Speed Animation: Use uniformly spaced t-values from t = 0 to t = 1 to display the natural parameter-based speed.\nConstant Speed Animation: Use the previously calculated equipartition points t^*(s) to animate movement along equal arc-length segments, ensuring a uniform speed.\n\n\n\n\nSolution Code\nThe following Python code generates both animations, showing the path traversal at original and constant speeds.\nimport matplotlib.animation as animation\n\ndef animate_path():\n    fig = plt.figure(figsize=(16, 8), facecolor='white')\n\n    ax1 = fig.add_subplot(121)\n    ax2 = fig.add_subplot(122)\n\n    # Generate data for original and constant speed\n    t_values_original_speed = np.linspace(0, 1, 25)\n    x_vals_original_speed = x(t_values_original_speed)\n    y_vals_original_speed = y(t_values_original_speed)\n\n    t_values_constant_speed = equipartition(25)\n    x_vals_constant_speed = [x(t) for t in t_values_constant_speed]\n    y_vals_constant_speed = [y(t) for t in t_values_constant_speed]\n\n    # Enhanced styling function for subplots\n    def style_subplot(ax, title):\n        ax.grid(True, linestyle='-', alpha=0.2, color='gray')\n        ax.set_xlim(-1.5, 1.5)\n        ax.set_ylim(-0.5, 2)\n        ax.set_xticks(np.arange(-1, 1.5, 0.5))\n        ax.set_yticks(np.arange(0, 2.5, 0.5))\n        ax.set_aspect('equal')\n        ax.set_xticklabels(['' if x == 0 else str(x) for x in ax.get_xticks()])\n        ax.set_yticklabels(['' if y == 0 else str(y) for y in ax.get_yticks()])\n\n\n        # Enhanced axis lines\n        ax.spines['left'].set_position('zero')\n        ax.spines['bottom'].set_position('zero')\n        ax.spines['right'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n        ax.spines['left'].set_linewidth(1.5)\n        ax.spines['bottom'].set_linewidth(1.5)\n\n        # Enhanced ticks\n        ax.tick_params(axis='both', which='major', length=6, width=1, colors='black', direction='out')\n        ax.tick_params(axis='both', which='minor', length=3, width=1, colors='black', direction='out')\n\n        ax.set_title(title, pad=20, fontsize=12, fontweight='bold')\n\n    # Configure first subplot\n    ax1.plot(x_vals_original_speed, y_vals_original_speed, color=\"#2196F3\", linewidth=2.5, zorder=3)\n    original_point, = ax1.plot([], [], 'o', color=\"#1565C0\", markersize=8, zorder=4)\n    style_subplot(ax1, \"Original Speed\")\n\n    # Configure second subplot\n    ax2.plot(x_vals_constant_speed, y_vals_constant_speed, color=\"#2196F3\", linewidth=2.5, zorder=3)\n    constant_point, = ax2.plot([], [], 'go',  markersize=8, zorder=4)\n    style_subplot(ax2, \"Constant Speed\")\n\n    plt.tight_layout()\n\n    def update_original(fnum):\n        original_point.set_data(x_vals_original_speed[:fnum], y_vals_original_speed[:fnum])\n        return original_point,\n\n    def update_constant(fnum):\n        constant_point.set_data(x_vals_constant_speed[:fnum], y_vals_constant_speed[:fnum])\n        return constant_point,\n\n    num_frames = len(x_vals_original_speed)\n    ani = animation.FuncAnimation(fig, lambda fnum: update_original(fnum) + update_constant(fnum),\n                                frames=num_frames, interval=200, blit=True)\n    ani.save('combined_animation.mp4', writer='ffmpeg')\n\n  Your browser does not support the video tag. \n\n\n\nExplanation of Solution Components\n\nAnimation Setup:\n\nOriginal Speed: Uses evenly spaced t-values from 0 to 1, resulting in non-uniform movement along the path.\nConstant Speed: Uses equipartition points t^*(s), calculated to ensure each segment has the same arc length, resulting in uniform movement.\n\nAnimation Update Functions: Each animation frame updates the moving point on the respective path for both original and constant speeds.\n\n\n\nResults and Observations\nThe two animations effectively demonstrate the difference between moving at a variable speed (based on t) and moving at a constant speed along equal arc-length segments. By using equipartition points, the constant-speed animation shows smooth, uniform movement, which can be advantageous for applications requiring consistent traversal rates."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc05/rc05.html#experimenting-with-equipartitioning-on-a-custom-path",
    "href": "mathematics/numerical-analysis/projects/rc05/rc05.html#experimenting-with-equipartitioning-on-a-custom-path",
    "title": "REALITY CHECK 05",
    "section": "Experimenting with Equipartitioning on a Custom Path",
    "text": "Experimenting with Equipartitioning on a Custom Path\n\nProblem Statement\nExperiment with equipartitioning a path of your choice. Choose a path defined by parametric equations, partition it into equal arc-length segments, and animate the traversal as demonstrated in Problem 5.\n\nChosen Equation:\n\nx(t) = 0.4 \\sin(3t + \\frac{\\pi}{2}) + 0.5\n\n\ny(t) = 0.3 \\sin(4t) + 0.5\n\n\n\n\nObjective and Approach\n\nObjective: To apply equipartitioning to the specified path, dividing it into segments of equal arc length and visualizing the traversal at constant speed.\nApproach:\n\nPath Definition: Define x(t) and y(t) based on the given equations.\nEquipartitioning: Use numerical integration and Newton’s Method to divide the path into equal-length segments.\nAnimation: Animate the traversal of the path at a constant speed along the equal arc-length segments and compare it with traversal at the original, parameter-based speed.\n\n\n\n\nSolution Code\nThe Python code below calculates the equipartitioned segments and animates traversal along the path:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom scipy.integrate import quad\n\n# Parameters for the curve\nA = 0.4\na = 3\nf = np.pi / 2\nc = 0.5\nB = 0.3\nb = 4\nD = 0.5\n\n# Maximum value of t for one full loop\nt_max = 2 * np.pi\n\n# Define the functions for x(t) and y(t)\ndef x(t):\n    return A * np.sin(a * t + f) + c\n\ndef y(t):\n    return B * np.sin(b * t) + D\n\n# Derivatives of x(t) and y(t) for arc length calculation\ndef dx_dt(t):\n    return A * a * np.cos(a * t + f)\n\ndef dy_dt(t):\n    return B * b * np.cos(b * t)\n\n# Integrand for arc length calculation\ndef integrand(t):\n    return np.sqrt(dx_dt(t)**2 + dy_dt(t)**2)\n\n# Compute arc length using numerical integration\ndef compute_arc_length(s):\n    arc_length, _ = quad(integrand, 0, s)\n    return arc_length\n\n# Equipartition function to divide path into equal arc-length segments\ndef equipartition(n):\n    total_length = compute_arc_length(2 * np.pi)\n    segment_length = total_length / n\n    partition_points = [0]\n    for i in range(1, n):\n        target_length = i * segment_length\n        partition_points.append(find_t_for_length(target_length, partition_points[-1]))\n    partition_points.append(2 * np.pi)\n    return partition_points\n\n# Find parameter t for a given arc length using Newton's Method\ndef find_t_for_length(target_length, initial_guess=0, tol=1e-8, max_iter=100):\n    t = initial_guess\n    for _ in range(max_iter):\n        f_t = compute_arc_length(t) - target_length\n        f_prime_t = integrand(t)\n        if abs(f_t) &lt; tol:\n            return t\n        t -= f_t / f_prime_t\n        t = max(0, min(2 * np.pi, t))\n    return t\n\n\n# Data for animations\nn_points = 200\nt_values_original = np.linspace(0, 2 * np.pi, n_points)\nx_original = x(t_values_original)\ny_original = y(t_values_original)\n\nt_values_constant = equipartition(n_points)\nx_constant = [x(t) for t in t_values_constant]\ny_constant = [y(t) for t in t_values_constant]\n\n# Set up the figure for side-by-side animation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n# Original speed plot\nax1.plot(x_original, y_original, color=\"#2196F3\", linewidth=2)\npoint1, = ax1.plot([], [], 'o', color=\"#1565C0\")\nax1.set_title(\"Original Speed\")\nax1.set_xlim(0, 1)\nax1.set_ylim(0, 1)\nax1.set_aspect('equal')\nax1.grid(True, linestyle='-', alpha=0.2, color='gray')\n\n# Constant speed plot\nax2.plot(x_constant, y_constant, color=\"#2196F3\", linewidth=2)\npoint2, = ax2.plot([], [], 'go')\nax2.set_title(\"Constant Speed\")\nax2.set_xlim(0, 1)\nax2.set_ylim(0, 1)\nax2.set_aspect('equal')\nax2.grid(True, linestyle='-', alpha=0.2, color='gray')\n\n# Update functions for each animation\ndef update_original(frame):\n    point1.set_data(x_original[:frame], y_original[:frame])\n    return point1,\n\ndef update_constant(frame):\n    point2.set_data(x_constant[:frame], y_constant[:frame])\n    return point2,\n\n# Combine animations into one\nnum_frames = len(x_original)\nani = animation.FuncAnimation(\n    fig,\n    lambda frame: update_original(frame) + update_constant(frame),\n    frames=num_frames,\n    interval=100,\n    blit=True\n)\n\n# Save animation as MP4\nani.save(\"custom_path_animation.mp4\", writer=\"ffmpeg\")\n\n  Your browser does not support the video tag. \n\n\n\nExplanation of Solution Components\n\nPath Definition: The parametric equations for x(t) = 0.4 \\sin(3t + \\frac{\\pi}{2}) + 0.5 and y(t) = 0.3 \\sin(4t) + 0.5 define a periodic curve with sinusoidal behavior, creating a visually interesting pattern with symmetric, tight curves.\nArc Length Calculation: The function compute_arc_length integrates the instantaneous speed along the curve (using derivatives dx/dt and dy/dt) over the interval [0, s] to determine the total distance traveled up to a given s.\nEquipartitioning: The equipartition function divides the path into n segments of equal arc length by calculating the target length of each segment and using Newton’s Method to determine t values corresponding to each target segment length. This ensures the segments are evenly spaced along the curve.\nAnimation: The animate_path function generates side-by-side animations of path traversal at original speed (based on parameter t) and constant speed (based on equal arc-length segments).\n\n\n\nResults and Observations\nIn the animation:\n\nOriginal Speed: The left animation shows traversal based on equally spaced t values, resulting in variable speed along the curve. The point moves faster along straighter sections and slows down significantly in tighter curves.\nConstant Speed: The right animation demonstrates traversal at a constant speed along equal arc-length segments. This movement is smoother and consistent, highlighting how equipartitioning ensures a steady traversal rate even along complex paths.\n\n\n\nConclusion\nThis exercise illustrates the benefits of equipartitioning a path into equal arc-length segments for applications that require consistent speed. By reparameterizing the curve to maintain constant speed, we can avoid the variable movement speed that results from a simple, evenly spaced parameter t. This method has potential applications in animation, robotics, and automated manufacturing, where uniform movement along a path with varying curvature is essential."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04-code.html",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04-code.html",
    "title": "REALITY CHECK 04 - CODE",
    "section": "",
    "text": "#Import Libraries\nimport numpy as np\nfrom scipy.optimize import fsolve\nimport sympy as sp\n\n# -----------------------------------------------------\n# PROBLEM 1: Numerical Root-Finding for GPS Positioning\n# -----------------------------------------------------\n\n# Given constants and satellite data\nc = 299792.458  # Speed of light in km/s\nA = [15600, 18760, 17610, 19170]  # Satellite x-coordinates in km\nB = [7540, 2750, 14630, 610]      # Satellite y-coordinates in km\nC = [20140, 18610, 13480, 18390]  # Satellite z-coordinates in km\nt = [0.07074, 0.07220, 0.07690, 0.07242]  # Signal travel times in seconds\n\n# Function to define the residuals for the nonlinear system\ndef residuals(vars):\n    \"\"\"\n    Residual function for GPS equations:\n    sqrt((x - A_i)^2 + (y - B_i)^2 + (z - C_i)^2) - c * (t_i - d)\n    \"\"\"\n    x, y, z, d = vars  # Unpack the unknowns\n    res = []\n    for i in range(4):  # Loop through the 4 satellites\n        dist = np.sqrt((x - A[i])**2 + (y - B[i])**2 + (z - C[i])**2)\n        res.append(dist - c * (t[i] - d))  # Append each residual\n    return res\n\n# Initial guess for (x, y, z, d)\ninitial_guess = [0, 0, 6370.0, 0]  # Receiver near Earth's surface and d = 0\n\n# Solve the nonlinear system using fsolve\nsol = fsolve(residuals, initial_guess)\n\n# Print the solution for (x, y, z, d)\nprint(\"----- PROBLEM 1: Numerical Solution -----\")\nprint(f\"x = {sol[0]:.6f} km\")\nprint(f\"y = {sol[1]:.6f} km\")\nprint(f\"z = {sol[2]:.6f} km\")\nprint(f\"d = {sol[3]:.6e} seconds\")\nprint(\"-----------------------------------------\\n\")\n\n# ------------------------------------------------------------\n# PROBLEM 2: Determinant-Based Analytical Approach for GPS\n# ------------------------------------------------------------\n\n# Define symbolic variables\nx, y, z, d = sp.symbols('x y z d', real=True)\n\n# Formulate the nonlinear equations\neqs = []\nfor i in range(4):\n    eq = (x - A[i])**2 + (y - B[i])**2 + (z - C[i])**2 - (c * (t[i] - d))**2\n    eqs.append(eq)\n\n# Linearize the system\n# Subtract eqs[1], eqs[2], and eqs[3] from eqs[0] to eliminate x^2, y^2, z^2 terms\nlin_eqs = [sp.simplify(eqs[0] - eqs[i]) for i in range(1, 4)]\n\n# Extract the coefficients of the linear equations\nA_matrix, b_vector = sp.linear_eq_to_matrix(lin_eqs, [x, y, z, d])\n\n# Split the coefficient matrix into components:\nA_xyz = A_matrix[:, :3]  # Coefficients of x, y, z\nA_d = A_matrix[:, 3]     # Coefficient of d\n\n# Solve for x, y, z in terms of d\nxyz_solution = A_xyz.LUsolve(b_vector - A_d * d)\n\n# Simplify solutions for x, y, z as functions of d\nx_d = sp.simplify(xyz_solution[0])\ny_d = sp.simplify(xyz_solution[1])\nz_d = sp.simplify(xyz_solution[2])\n\n# Substitute x(d), y(d), z(d) into the first original equation\nquadratic_eq_d = sp.simplify(eqs[0].subs({x: x_d, y: y_d, z: z_d}))\n\n# Solve the resulting quadratic equation for d\ncoeffs_d = sp.Poly(quadratic_eq_d, d).all_coeffs()\nd_solutions = sp.solve(quadratic_eq_d, d)\n\n# Select the physically meaningful solution for d (real and close to zero)\nd_final = None\nfor candidate in d_solutions:\n    if candidate.is_real:\n        d_final = candidate.evalf()\n        break\n\n# Compute final (x, y, z) by substituting d into x_d, y_d, z_d\nx_final = x_d.subs(d, d_final).evalf()\ny_final = y_d.subs(d, d_final).evalf()\nz_final = z_d.subs(d, d_final).evalf()\n\n# Print the final analytical solution\nprint(\"----- PROBLEM 2: Analytical Solution -----\")\nprint(f\"x = {x_final:.6f} km\")\nprint(f\"y = {y_final:.6f} km\")\nprint(f\"z = {z_final:.6f} km\")\nprint(f\"d = {d_final:.6e} seconds\")\nprint(\"-----------------------------------------\")\n\n# ------------------------------------------------------------\n# PROBLEM 4 & 5: Conditioning Analysis of the GPS Problem\n# ------------------------------------------------------------\n\n# Constants\nc = 299792.458  # Speed of light in km/s\nrho = 26570  # Fixed satellite altitude in km\nreceiver_pos = np.array([0, 0, 6370])  # Receiver fixed at Earth's surface\nd_initial = 0.0001  # Initial clock bias\nperturbation = 1e-8  # Perturbation in seconds\n\n# Function to compute satellite positions in Cartesian coordinates\ndef compute_satellite_positions(phi, theta):\n    A = [rho * np.cos(p) * np.cos(t) for p, t in zip(phi, theta)]\n    B = [rho * np.cos(p) * np.sin(t) for p, t in zip(phi, theta)]\n    C = [rho * np.sin(p) for p in phi]\n    return np.array(A), np.array(B), np.array(C)\n\n# Compute nominal ranges and travel times\ndef compute_nominal_values(A, B, C):\n    R = np.sqrt((A - receiver_pos[0])**2 + (B - receiver_pos[1])**2 + (C - receiver_pos[2])**2)\n    t_nominal = d_initial + R / c\n    return R, t_nominal\n\n# GPS residual function\ndef gps_residuals(vars, t, A, B, C):\n    x, y, z, d = vars\n    residuals = np.sqrt((x - A)**2 + (y - B)**2 + (z - C)**2) - c * (t - d)\n    return residuals\n\n# EMF computation function\ndef compute_emf(t_perturbed, t_nominal, A, B, C):\n    initial_guess = [0, 0, 6370, d_initial]\n    sol_nominal = fsolve(gps_residuals, initial_guess, args=(t_nominal, A, B, C))\n    sol_perturbed = fsolve(gps_residuals, initial_guess, args=(t_perturbed, A, B, C))\n\n    position_error = np.linalg.norm(np.array(sol_perturbed[:3]) - np.array(sol_nominal[:3]))\n    input_error = np.linalg.norm(np.array(t_perturbed) - np.array(t_nominal)) * c\n    return position_error / input_error\n\n# Function to analyze EMF for given satellite configuration\ndef analyze_emf(phi, theta):\n    A, B, C = compute_satellite_positions(phi, theta)\n    _, t_nominal = compute_nominal_values(A, B, C)\n\n    emf_values = []\n    for i in range(4):\n        t_perturbed = t_nominal.copy()\n        t_perturbed[i] += perturbation\n        emf = compute_emf(t_perturbed, t_nominal, A, B, C)\n        emf_values.append(emf)\n\n    return emf_values\n\n# Loose satellite configuration\nphi_loose = [np.pi / 6, np.pi / 4, np.pi / 3, np.pi / 8]\ntheta_loose = [0, np.pi / 2, np.pi, 3 * np.pi / 2]\nemf_loose = analyze_emf(phi_loose, theta_loose)\n\n# Tightly grouped satellite configuration\ndef tightly_grouped_coordinates(phi_base, theta_base, perturb=0.05):\n    np.random.seed(42)  # For reproducibility\n    phi_tight = [phi_base * (1 + np.random.uniform(-perturb, perturb)) for _ in range(4)]\n    theta_tight = [theta_base * (1 + np.random.uniform(-perturb, perturb)) for _ in range(4)]\n    return phi_tight, theta_tight\n\nphi_base, theta_base = np.pi / 4, np.pi / 2\nphi_tight, theta_tight = tightly_grouped_coordinates(phi_base, theta_base)\nemf_tight = analyze_emf(phi_tight, theta_tight)\n\n# Print results\nprint(\"----- PROBLEM 4 & 5: CONDITIONING ANALYSIS COMPARISON -----\")\nprint(\"Loose Satellites:\")\nfor i, emf in enumerate(emf_loose):\n    print(f\"EMF for perturbation in t_{i+1}: {emf:.6f}\")\nprint(f\"Maximum EMF (Loose): {max(emf_loose):.6f}\\n\")\n\nprint(\"Tightly Grouped Satellites:\")\nfor i, emf in enumerate(emf_tight):\n    print(f\"EMF for perturbation in t_{i+1}: {emf:.6f}\")\nprint(f\"Maximum EMF (Tight): {max(emf_tight):.6f}\")\nprint(\"--------------------------------------------\")"
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc01/rc01.html",
    "href": "mathematics/numerical-analysis/projects/rc01/rc01.html",
    "title": "REALITY CHECK 01",
    "section": "",
    "text": "PROBLEM 1\n\n\nWrite a python function for f(\\theta). The parameters L_1, L_2, L_3, \\gamma, x_1, x_2, y_2 are fixed constants, and the strut lengths p_1, p_2, p_3 will be known for a given pose. To test your code, set the parameters L_1 = 2, L_2 = L_3 = \\sqrt{2}, \\gamma = \\pi/2, and p_1 = p_2 = p_3 = \\sqrt{5}. Then, substituting \\theta = -\\pi/4 or \\theta = \\pi/4, should make f(\\theta) = 0.\nI implemented the Python function f(\\theta) by putting all fixed constants into a Constants object. I initialized the constants with the given values in order to verify that \\theta = -\\pi/4 and \\theta = \\pi/4 were roots.\n\n\nCreate function for f(\\theta)\n\n\nShow Code\n# Define the function f(θ) that calculates based on given constants and angle θ\ndef f(theta, constants):\n    \"\"\"\n    Calculates a value based on the given angle theta and constants object.\n\n    Parameters:\n    theta (float): The angle in radians.\n    constants (Constants): An object containing the necessary constants.\n\n    Returns:\n    float: The calculated result.\n    \"\"\"\n    l1, l2, l3 = constants.l1, constants.l2, constants.l3\n    gamma = constants.gamma\n    x1, x2, y2 = constants.x1, constants.x2, constants.y2\n    p1, p2, p3 = constants.p1, constants.p2, constants.p3\n\n    a2 = l3 * np.cos(theta) - x1\n    b2 = l3 * np.sin(theta)\n    a3 = l2 * np.cos(theta + gamma) - x2\n    b3 = l2 * np.sin(theta + gamma) - y2\n    d = 2 * (a2 * b3 - b2 * a3)\n\n    n1 = b3 * (p2**2 - p1**2 - a2**2 - b2**2) - b2 * (p3**2 - p1**2 - a3**2 - b3**2)\n    n2 = -a3 * (p2**2 - p1**2 - a2**2 - b2**2) + a2 * (p3**2 - p1**2 - a3**2 - b3**2)\n\n    return n1**2 + n2**2 - p1**2 * d**2\n\n\n\n\nTest function f(\\theta)\n\n\nShow Code\n# Define constants and evaluate f(θ) at a specific angle θ = π/4 for testing purposes\nconstants = Constants(\n    l1=2,\n    l2=np.sqrt(2),\n    l3=np.sqrt(2),\n    gamma=np.pi / 2,\n    x1=4,\n    x2=0,\n    y2=4,\n    p1=np.sqrt(5),\n    p2=np.sqrt(5),\n    p3=np.sqrt(5)\n)\n\ntheta = np.pi / 4\n# Evaluate\nresult = f(theta, constants)\nprint(f'f(θ=π/4) = {result}')\n\n\nf(θ=π/4) = -4.547473508864641e-13\n\n\n\n\n\nPROBLEM 2\n\n\nPlot f(\\theta) on [-\\pi, \\pi]\nI plotted the function f(\\theta) over the interval [-π, π] by generating a range of \\theta values and computing f(\\theta) for each. The graph clearly shows the behavior of f(\\theta) and highlights that the the roots identified in Problem 1 are in fact roots.\n\n\nCreate Plot\n\n\nShow Code\n# Generate a range of theta values and compute f(θ) for each value to visualize the function\ntheta_values = np.linspace(-np.pi, np.pi, 400)\nresults = [f(theta, constants) for theta in theta_values]\n\n# Plot f(θ) over the range of theta values\nplt.figure(figsize=(10, 6))\nplt.plot(theta_values, results, label=r'$f(\\theta)$', linewidth=2)\nplt.axhline(0, color='black', linestyle='--', linewidth=0.8)\nplt.axvline(-np.pi/4, color='red', linestyle=':', linewidth=2, label=r'$\\theta = -\\pi/4$')\nplt.axvline(np.pi/4, color='red', linestyle=':', linewidth=2, label=r'$\\theta = \\pi/4$')\nplt.xlabel(r'$\\theta$', fontsize=14)\nplt.ylabel(r'$f(\\theta)$', fontsize=14)\nplt.title(r'Function of $\\theta$ for Stewart Platform', fontsize=16)\nplt.legend(fontsize=12)\nplt.grid(True)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nPROBLEM 3\n\n\nReproduce Figure 1.15. Plot a red triangle with vertices (u_1, v_1), (u_2, v_2), (u_3, v_3) and place small blue circles at the strut anchor points (0,0), (x_1, 0), (x_2, y_2):\nI utilized several helper functions to efficiently calculate and visualize the Stewart platform’s configuration. The get_x_y() function computes the x and y coordinates based on the given angle \\theta and the fixed constants, determining the position of one vertex of the triangle. The get_points() function then takes these coordinates, along with \\theta and the constants, to calculate the two other vertices of the red triangle. The get_anchor_points() function gets the fixed anchor points (0,0), (x_1, 0), and (x_2, y_2). The plot_triangle() function takes the calculated triangle vertices and anchor points to plot the red triangle and connect the anchor points with blue lines, while also marking the anchor points with blue circles.\n\n\nCreate Helper Functions\n\n\nShow Code\n# Define helper functions for calculating x, y coordinates and plotting the Stewart platform triangle\ndef get_x_y(theta, constants):\n    \"\"\"\n    Returns the coordinates x and y for the given angle theta and constants object.\n\n    Parameters:\n    theta (float): The angle in radians.\n    constants (Constants): An object containing the necessary constants.\n\n    Returns:\n    tuple: The coordinates (x, y).\n    \"\"\"\n    l1, l2, l3 = constants.l1, constants.l2, constants.l3\n    gamma = constants.gamma\n    x1, x2, y2 = constants.x1, constants.x2, constants.y2\n    p1, p2, p3 = constants.p1, constants.p2, constants.p3\n\n    a2 = l3 * np.cos(theta) - x1\n    b2 = l3 * np.sin(theta)\n    a3 = l2 * np.cos(theta + gamma) - x2\n    b3 = l2 * np.sin(theta + gamma) - y2\n\n    d = 2 * (a2 * b3 - b2 * a3)\n    n1 = b3 * (p2**2 - p1**2 - a2**2 - b2**2) - b2 * (p3**2 - p1**2 - a3**2 - b3**2)\n    n2 = -a3 * (p2**2 - p1**2 - a2**2 - b2**2) + a2 * (p3**2 - p1**2 - a3**2 - b3**2)\n\n    x = n1 / d\n    y = n2 / d\n\n    return x, y\n\n\ndef get_points(x, y, theta, constants):\n    \"\"\"\n    Calculate the three points (vertices) of the triangle in the Stewart platform based on x, y, and θ.\n\n    Parameters:\n    x (float): The x-coordinate.\n    y (float): The y-coordinate.\n    theta (float): The angle in radians.\n    constants (Constants): Object containing the necessary constants.\n\n    Returns:\n    list: A list containing the three vertices (l1_point, l2_point, l3_point) of the triangle.\n    \"\"\"\n    l1, l2, l3 = constants.l1, constants.l2, constants.l3\n    gamma = constants.gamma\n\n    # First vertex (base point)\n    l1_point = (x, y)\n\n    # Second vertex of the triangle\n    l2_x = x + (l3 * np.cos(theta))\n    l2_y = y + (l3 * np.sin(theta))\n    l2_point = (np.round(l2_x, 3), np.round(l2_y))  # Rounded to 3 decimal places for clarity\n\n    # Third vertex of the triangle\n    l3_x = x + (l2 * np.cos(theta + gamma))\n    l3_y = y + (l2 * np.sin(theta + gamma))\n    l3_point = (np.round(l3_x), np.round(l3_y))  # Rounded to 3 decimal places for clarity\n\n    return [l1_point, l2_point, l3_point]\n\ndef get_anchor_points(constants):\n    \"\"\"\n    Get the anchor points for the Stewart platform based on the constants.\n\n    Parameters:\n    constants (Constants): Object containing the necessary constants.\n\n    Returns:\n    list: A list of tuples representing the anchor points.\n    \"\"\"\n    x1, x2, y2 = constants.x1, constants.x2, constants.y2\n\n    return [(0, 0), (x1, 0), (x2, y2)]\n\ndef plot_triangle(ax, points, anchor_points, x_limits=None, y_limits=None, x_step=None, y_step=None):\n    \"\"\"\n    Plots a triangle given the points and anchor points on the provided axis.\n\n    Parameters:\n    ax: The axis on which to plot the triangle.\n    points: The points of the triangle (list of 3 points).\n    anchor_points: The anchor points (list of 2 or more points).\n    x_limits (tuple, optional): Tuple specifying the x-axis limits (x_min, x_max).\n    y_limits (tuple, optional): Tuple specifying the y-axis limits (y_min, y_max).\n    x_step (float, optional): Step size for the x-axis grid.\n    y_step (float, optional): Step size for the y-axis grid.\n\n    Returns:\n    None\n    \"\"\"\n    points = np.array(points)\n    anchor_points = np.array(anchor_points)\n\n    # Extract x and y coordinates for the triangle points\n    x_coords = points[:, 0]\n    y_coords = points[:, 1]\n\n    # Close the triangle by appending the first point at the end\n    x_closed = np.append(x_coords, x_coords[0])\n    y_closed = np.append(y_coords, y_coords[0])\n\n    # Plot the triangle with red lines\n    ax.plot(x_closed, y_closed, 'r-', linewidth=3.5)\n\n    # Plot blue dots at the triangle vertices\n    ax.plot(x_coords, y_coords, 'bo', markersize=8)\n\n    # Plot lines from anchor points to triangle points\n    for i, anchor in enumerate(anchor_points):\n        if i &lt; len(points):  # Ensure we stay within bounds\n            ax.plot([anchor[0], points[i, 0]], [anchor[1], points[i, 1]], 'b-', linewidth=1.5)\n\n    # Plot blue dots at the anchor points\n    ax.plot(anchor_points[:, 0], anchor_points[:, 1], 'bo', markersize=8)\n\n    # Set axis labels\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n\n    # Set x-axis limits if provided\n    if x_limits is not None:\n        ax.set_xlim(x_limits)\n    # Set y-axis limits if provided\n    if y_limits is not None:\n        ax.set_ylim(y_limits)\n    # Set grid step increments if limits are provided\n    if x_step is not None and x_limits is not None:\n        ax.set_xticks(np.arange(x_limits[0], x_limits[1] + x_step, x_step))  # Adjust x-axis ticks\n    if y_step is not None and y_limits is not None:\n        ax.set_yticks(np.arange(y_limits[0], y_limits[1] + y_step, y_step))  # Adjust y-axis ticks\n\n    # Add grid for better visualization\n    ax.grid(True)\n\n\n\n\nCreate Plot\n\n\nShow code\n# Create a plot to visualize the Stewart platform configurations for two different angles\ntheta = np.pi / 4\ntheta_negative = -np.pi / 4\n\n# Calculate the coordinates and points for the triangles\nx, y = get_x_y(theta_negative, constants)\npoints1 = get_points(x, y, theta_negative, constants)\nanchor_points = get_anchor_points(constants)\n\nx, y = get_x_y(theta, constants)\npoints2 = get_points(x, y, theta, constants)\n\n# Create side-by-side subplots to visualize the two triangles\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n# Plot the triangles on each subplot\nplot_triangle(axes[0], points1, anchor_points, x_limits=(-0.25, 4.25), y_limits=(-0.25, 4.25))\nplot_triangle(axes[1], points2, anchor_points, x_limits=(-0.25, 4.25), y_limits=(-0.25, 4.25))\n\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nPROBLEM 4\n\n\nSolve the forward kinematics problem for the planar Stewart platform specified by x_1 = 5, (x_2, y_2) = (0,6), L_1 = L_3 = 3, L_2 = 3\\sqrt{2}, \\gamma = \\pi / 4, p_1 = p_2 = 5, p_3 = 3. Begin by plotting f(\\theta). Use an equation solver of your choice to find all four poses (roots of f(\\theta)), and plot them. Check your answers by verifying that p_1, p_2, p_3 are the lengths of the struts in your plot.\nI organized all the fixed parameters into a Constants object and plotted the function f(\\theta) over the interval [-π, π] to visualize its behavior. Using the fsolve function with strategically chosen initial guesses, I identified all four roots of f(\\theta), each root representing a unique pose of the Stewart platform. For each detected root, I plotted the corresponding triangle configuration and verified that the strut lengths p_1, p_2, p_3 matched the expected values.\n\n\n4A)\n\n\nShow Code\n# Create new constants object\nconstants = Constants(\n    l1=3,\n    l2=3 * np.sqrt(2),\n    l3=3,\n    gamma=np.pi / 4,\n    x1=5,\n    x2=0,\n    y2=6,\n    p1=5,\n    p2=5,\n    p3=3\n)\n\n# Generate an array of θ values between -π and π\ntheta_values = np.linspace(-np.pi, np.pi, 400)\n\n# Plot the function f(θ) over the range of θ values using the given constants\nplt.figure(figsize=(10, 6))\nplt.plot(theta_values, f(theta_values, constants), label=r'$f(\\theta)$')\nplt.axhline(0, color='black', linestyle='--', linewidth=0.8)\nplt.xlabel(r'$\\theta$', fontsize=14)\nplt.ylabel(r'$f(\\theta)$', fontsize=14)\nplt.title(r'Function of $\\theta$ for Stewart Platform', fontsize=16)\nplt.legend(fontsize=12)\nplt.grid(True)\n\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n4B)\n\n\nShow Code\n# Function to find roots of f(θ) using fsolve\ndef find_roots(constants, initial_guesses):\n    \"\"\"\n    Finds roots of f(θ) using different initial guesses and the fsolve method.\n\n    Parameters:\n    constants (Constants): Object containing the necessary constants.\n    initial_guesses (list or array): List of initial guesses for fsolve to start from.\n\n    Returns:\n    list: A list of unique roots.\n    \"\"\"\n\n    # Create an empty list to store the roots found\n    roots = []\n    # Iterate over each initial guess and find the root using fsolve\n    for guess in initial_guesses:\n        root = fsolve(f, guess, args=(constants), xtol=1e-12)[0] # Find root for each guess\n        roots.append(root) # Append the found root to the list\n\n    # Return only unique roots to avoid duplicates\n    unique_roots = np.unique(roots)\n    return unique_roots\n\n# Define initial guesses for fsolve to start the root-finding process\ninitial_guesses = [- 1, np.pi / 3, .5, 2]\n\n# Find and print the roots using the initial guesses\nroots = find_roots(constants, initial_guesses)\nprint(f\"The roots of f(θ) in the interval are : {roots}\")\n\n# Function to calculate the length of the struts\ndef calculate_strut_lengths(points, anchor_points):\n    lengths = []\n    # Loop through the 3 points and calculate the Euclidean distance to each corresponding anchor point\n    for i in range(3):\n        length = np.sqrt((points[i][0] - anchor_points[i][0])**2 + (points[i][1] - anchor_points[i][1])**2)\n        lengths.append(length) # Append each calculated length to the list\n    return lengths\n\n\nThe roots of f(θ) in the interval are : [-0.7208492  -0.33100518  1.14368552  2.11590901]\n\n\n\n\nShow Code\n# Create a 2x2 grid of subplots to visualize the four roots and their corresponding triangles\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\naxes = axes.flatten() # Flatten the 2D array of subplots into a 1D array for easier access\n\n# Get the anchor points for the Stewart platform\nanchor_points = get_anchor_points(constants)\n\n# Loop through up to four roots and plot the corresponding triangles\nfor i, theta in enumerate(roots[:4]):\n    x, y = get_x_y(theta, constants)\n    points = get_points(x, y, theta, constants)\n\n    # Plot the triangle in the corresponding subplot with custom limits\n    plot_triangle(axes[i], points, anchor_points, x_limits=(-2.5, 7.5), y_limits=(-2, 7), x_step=2.5, y_step=2)\n    axes[i].set_title(rf\"$\\theta$ = {theta}\")\n\n    # Calculate and verify strut lengths\n    lengths = calculate_strut_lengths(points, anchor_points)\n    print(f\"For root {np.round(theta, 3)}, strut lengths are: {np.round(lengths)}\")\n    print(f\"Expected: p1={constants.p1}, p2={constants.p2}, p3={constants.p3}\\n\")\n\n# Turn off any unused subplots if fewer than four roots\nfor j in range(len(roots), 4):\n    axes[j].axis('off')\n\n# Adjust layout\nplt.tight_layout()\n\nplt.show()\n\n\nFor root -0.721, strut lengths are: [5. 5. 3.]\nExpected: p1=5, p2=5, p3=3\n\nFor root -0.331, strut lengths are: [5. 5. 3.]\nExpected: p1=5, p2=5, p3=3\n\nFor root 1.144, strut lengths are: [5. 5. 3.]\nExpected: p1=5, p2=5, p3=3\n\nFor root 2.116, strut lengths are: [5. 5. 3.]\nExpected: p1=5, p2=5, p3=3\n\n\n\n\n\n\n\n\n\n\n\n\n\nPROBLEM 5\n\n\nChange strut length to p_2 = 7 and re-solve the problem. For these parameters, there are six poses.\nI updated the strut length p_2 to 7 and re-solved the forward kinematics for the Stewart platform. To do that I modified the Constants object with the new p_2 value and plotted the updated function f(\\theta) over the interval [-π, π] to see its behavior. I made a new set of initial guesses for the find_roots() function and successfully found all six roots corresponding to six possible poses. For each root, I plotted the corresponding triangle configuration and verified that the strut lengths p_1, p_2, p_3 matched the expected values.\n\n\n5A)\n\n\nShow Code\n# Update the constants to reflect the new strut length p2 = 7\nconstants = Constants(\n    l1=3,\n    l2=3 * np.sqrt(2),\n    l3=3,\n    gamma=np.pi / 4,\n    x1=5,\n    x2=0,\n    y2=6,\n    p1=5,\n    p2=7,\n    p3=3\n)\n\n# Generate the θ values again to visualize the updated f(θ)\ntheta_values = np.linspace(-np.pi, np.pi, 400)\n\n# Plot f(θ) for the new strut length\nplt.figure(figsize=(10, 6))\nplt.plot(theta_values, f(theta_values, constants), label=r'$f(\\theta)$')\nplt.axhline(0, color='black', linestyle='--', linewidth=0.8)\nplt.xlabel(r'$\\theta$', fontsize=14)\nplt.ylabel(r'$f(\\theta)$', fontsize=14)\nplt.title(r'Function of $\\theta$ for Stewart Platform', fontsize=16)\nplt.legend(fontsize=12)\nplt.grid(True)\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n5B)\n\n\nShow Code\n# Provide new initial guesses to find six distinct roots for this configuration\ninitial_guesses = [-.7, -.4, .01, .4, .9, 2.5 ]  # Customize this list\n\n# Find and print the roots using the initial guesses\nroots = find_roots(constants, initial_guesses)\nprint(f\"The roots of f(θ) in the interval are : {roots}\")\n\n# Set up the 2x3 grid for plotting the six poses\nfig, axes = plt.subplots(2, 3, figsize=(9, 6))  # Create a 2x3 grid\naxes = axes.flatten()  # Flatten the 2D array of axes for easier access\n\n# Get the anchor points\nanchor_points = get_anchor_points(constants)\n\n# Loop through the six roots and plot each pose\nfor i, theta in enumerate(roots[:6]):\n    x, y = get_x_y(theta, constants)\n    points = get_points(x, y, theta, constants)\n    # Plot the triangle in the corresponding subplot with custom limits\n    plot_triangle(axes[i], points, anchor_points, x_limits=(-5.5, 5.5), y_limits=(-.5, 10), )\n    axes[i].set_title(rf\"$\\theta$ = {theta}\")\n\n    # Calculate and verify strut lengths\n    lengths = calculate_strut_lengths(points, anchor_points)\n    print(f\"For root {np.round(theta, 3)}, strut lengths are: {np.round(lengths)}\")\n    print(f\"Expected: p1={constants.p1}, p2={constants.p2}, p3={constants.p3}\\n\")\n\n# Turn off any unused subplots (though in this case, we should have exactly 6)\nfor j in range(len(roots), 6):\n    axes[j].axis('off')\n\n# Adjust layout\nplt.tight_layout()\n\n\nplt.show()\n\n\nThe roots of f(θ) in the interval are : [-0.67315749 -0.35474027  0.03776676  0.45887818  0.9776729   2.5138528 ]\nFor root -0.673, strut lengths are: [5. 7. 3.]\nExpected: p1=5, p2=7, p3=3\n\nFor root -0.355, strut lengths are: [5. 7. 3.]\nExpected: p1=5, p2=7, p3=3\n\nFor root 0.038, strut lengths are: [5. 7. 3.]\nExpected: p1=5, p2=7, p3=3\n\nFor root 0.459, strut lengths are: [5. 7. 3.]\nExpected: p1=5, p2=7, p3=3\n\nFor root 0.978, strut lengths are: [5. 7. 3.]\nExpected: p1=5, p2=7, p3=3\n\nFor root 2.514, strut lengths are: [5. 7. 3.]\nExpected: p1=5, p2=7, p3=3\n\n\n\n\n\n\n\n\n\n\n\n\n\nPROBLEM 6\n\n\nFind a strut length p_2, with the rest of the parameters as in Step 4, for which there are only two poses.\nTo identify a strut length for p_2 that results in exactly two poses for the Stewart platform I systematically adjusted p_2 and utilizing the fsolve function to find the corresponding roots of the function f(\\theta). This method enabled me to determine a specific p_2 value that achieves the desired two-pose configuration.\n\n\nShow Code\n# Set a threshold for considering a valid root (how close to zero we want f(theta) to be)\nROOT_THRESHOLD = 1e-6\n\n# Function to find roots for a given p2, and check if they are valid\ndef find_roots_for_p2(p2_value, constants, initial_guesses, ax=None):\n    \"\"\"\n    Adjusts p2 in the constants object, finds the roots, and returns the number of unique roots.\n    Also plots f(theta) for the current p2 value on the provided axis.\n    \"\"\"\n    # Update p2 in constants\n    constants.p2 = p2_value\n\n    # Generate theta values and compute f(theta)\n    theta_values = np.linspace(-np.pi, np.pi, 400)\n    f_values = [f(theta, constants) for theta in theta_values]\n\n    # Plot f(theta) for the current p2 value on the provided axis\n    ax.plot(theta_values, f_values,)\n    ax.axhline(0, color='black', linestyle='--', linewidth=0.8)\n    ax.set_xlabel(r'$\\theta$', fontsize=14)\n    ax.set_ylabel(r'$f(\\theta)$', fontsize=14)\n    ax.set_title(fr'$p_2 = {p2_value:.3f}$')\n    ax.legend(fontsize=10)\n    ax.grid(True)\n\n    # Find the roots for the given p2 value\n    roots = []\n    for guess in initial_guesses:\n        root = fsolve(f, guess, args=(constants))[0]\n\n        # Check if the found root is valid (i.e., f(root) is close to zero)\n        if abs(f(root, constants)) &lt; ROOT_THRESHOLD:\n            roots.append(root)\n\n    # Convert to numpy array and round the roots to avoid precision issues\n    roots = np.round(np.array(roots), decimals=6)\n    unique_roots = np.unique(roots)\n\n    # Print the number of valid roots and the roots themselves\n    print(f\"p2 = {p2_value:.3f}: Found {len(unique_roots)} valid roots: {unique_roots}\")\n\n    return unique_roots\n\n# Function to iterate over possible p2 values and append plots in a grid (wrap after 3)\ndef find_p2_with_two_roots(constants, initial_guesses, p2_start=-1, total_plots=6):\n    \"\"\"\n    Iterates over possible p2 values starting at p2_start, plots f(theta), and prints the number of roots.\n    The plots wrap after 3 per row.\n\n    Parameters:\n    - constants: The Constants object.\n    - initial_guesses: List of initial guesses for root finding.\n    - p2_start: Starting value of p2.\n    - total_plots: Number of plots to show before stopping.\n    \"\"\"\n    p2 = p2_start\n    plot_count = 0\n    max_plots_per_row = 3  # Wrap after 3 plots per row\n\n    # Calculate the number of rows needed (wrap after 3)\n    num_rows = (total_plots + max_plots_per_row - 1) // max_plots_per_row\n\n    # Create a figure with a 3xN grid\n    fig, axes = plt.subplots(num_rows, max_plots_per_row, figsize=(10, num_rows * 3))\n    axes = axes.flatten()  # Flatten the 2D array of axes for easier access\n    fig.subplots_adjust(hspace=0.3, wspace=0.3)  # Adjust the space between subplots\n\n    # Iterate to plot p2 and find roots\n    while plot_count &lt; total_plots:\n        # Plot for the current p2 value and check the roots\n        unique_roots = find_roots_for_p2(p2, constants, initial_guesses, ax=axes[plot_count])\n\n        if len(unique_roots) == 2:  # Check if there are exactly 2 unique roots\n            print(f\"Found p2={p2} with two distinct roots: {unique_roots}\")\n\n        # Increment p2 and plot the next iteration\n        p2 += 1\n        plot_count += 1\n\n    # Show the final figure with all appended plots\n    plt.tight_layout()\n\n\n    plt.show()\n\n# Example constants (with p2 placeholder)\nconstants = Constants(\n    l1=3,\n    l2=3 * np.sqrt(2),\n    l3=3,\n    gamma=np.pi / 4,\n    x1=5,\n    x2=0,\n    y2=6,\n    p1=5,\n    p2=None,  # To be found\n    p3=3\n)\n\n# Initial guesses for root finding\ninitial_guesses = [-np.pi/2, 0, np.pi/2]\n\n# Start p2 at -1 and increment by 1 each time, looking for exactly 2 roots\nfind_p2_with_two_roots(constants, initial_guesses, p2_start=-1, total_plots=6)\n\n\np2 = -1.000: Found 0 valid roots: []\np2 = 0.000: Found 0 valid roots: []\np2 = 1.000: Found 0 valid roots: []\np2 = 2.000: Found 0 valid roots: []\np2 = 3.000: Found 0 valid roots: []\np2 = 4.000: Found 2 valid roots: [1.331642 1.777514]\nFound p2=4 with two distinct roots: [1.331642 1.777514]\n\n\n\n\n\n\n\n\n\n\n\n\nPROBLEM 7\n\n\nCalculate the intervals in p_2, with the rest of the parameters as in Step 4, for which there are 0, 2, 4, and 6 poses, respectively.\nIn transitioning from Problem 6 to Problem 7, I found that using fsolve with predefined initial guesses was too inaccurate for reliably identifying roots. This method often missed valid roots or produced duplicates due to its sensitivity to starting points. To improve accuracy, I switched to detecting sign changes in the function f(\\theta) and used the brentq algorithm, which efficiently locates roots where the function changes from positive to negative or vice versa. This approach greatly improved the precision of root detection.\n\n\nShow Code\ndef count_roots(constants, theta_min=-np.pi, theta_max=np.pi, num_points=1000):\n    \"\"\"\n    Counts roots of f(theta) = 0 within [theta_min, theta_max].\n\n    Parameters:\n    constants (Constants): Stewart platform constants.\n    theta_min (float): Lower bound of theta.\n    theta_max (float): Upper bound of theta.\n    num_points (int): Sampling points.\n\n    Returns:\n    int: Number of unique roots.\n    list: Root values.\n    \"\"\"\n    theta_vals = np.linspace(theta_min, theta_max, num_points)\n\n    # Evaluate f(theta) over the range\n    f_vals = np.array([f(theta, constants) for theta in theta_vals])\n\n    roots = []\n\n    # Detect sign changes indicating roots\n    for i in range(len(theta_vals)-1):\n        if np.sign(f_vals[i]) != np.sign(f_vals[i+1]):\n            try:\n                root = brentq(f, theta_vals[i], theta_vals[i+1], args=(constants,))\n                if theta_min &lt;= root &lt;= theta_max:\n                    roots.append(root)\n            except ValueError:\n                pass  # No root in this interval\n\n    # Eliminate duplicate roots\n    unique_roots = []\n    for r in roots:\n        if not any(np.isclose(r, ur, atol=1e-5) for ur in unique_roots):\n            unique_roots.append(r)\n\n    return len(unique_roots), unique_roots\n\ndef find_p2_intervals(constants, p2_min, p2_max, p2_step):\n    \"\"\"\n    Finds p2 intervals with specific numbers of roots.\n\n    Parameters:\n    constants (Constants): Stewart platform constants.\n    p2_min (float): Starting p2 value.\n    p2_max (float): Ending p2 value.\n    p2_step (float): Increment step for p2.\n\n    Returns:\n    dict: Pose counts as keys and p2 lists as values.\n    \"\"\"\n    p2_values = np.arange(p2_min, p2_max + p2_step, p2_step)\n    root_counts = {0: [], 2: [], 4: [], 6: []}\n\n    for p2 in p2_values:\n        constants.p2 = p2\n        num_roots, _ = count_roots(constants)\n        if num_roots in root_counts:\n            root_counts[num_roots].append(p2)\n\n    return root_counts\n\n#### 4. Implement Problem 7\n\n# Initialize constants\nconstants = Constants(\n    l1=3,\n    l2=3 * np.sqrt(2),\n    l3=3,\n    gamma=np.pi / 4,\n    x1=5,\n    x2=0,\n    y2=6,\n    p1=5,\n    p2=5,  # Initial p2; will be varied\n    p3=3\n)\n\n# Set p2 range\np2_min = 0.0\np2_max = 12.98  # Extended to capture p2 &gt;= 9.27\np2_step = 0.01\n\n# Get root counts\nroot_counts = find_p2_intervals(constants, p2_min, p2_max, p2_step)\n\n# Plotting\nplt.figure(figsize=(12, 6))\ncolors = {0: 'blue', 2: 'green', 4: 'orange', 6: 'red'}\n\nfor num_roots, p2_list in root_counts.items():\n    plt.scatter(p2_list, [num_roots]*len(p2_list), label=f'{num_roots} poses', s=10, color=colors.get(num_roots, 'grey'))\n\nplt.xlabel('$p_2$', fontsize=14)\nplt.ylabel('Number of Poses (Roots)', fontsize=14)\nplt.title('Number of Poses vs Length of Strut $p_2$', fontsize=16)\nplt.legend()\nplt.grid(True)\n\nplt.show()\n\n# Identify intervals\nintervals_dict = {0: [], 2: [], 4: [], 6: []}\ntol = 1e-6  # Tolerance for precision\n\nfor num_roots, p2_list in root_counts.items():\n    if p2_list:\n        p2_sorted = np.sort(p2_list)\n        diffs = np.diff(p2_sorted)\n        split_indices = np.where(diffs &gt; (p2_step + tol))[0] + 1\n        intervals = np.split(p2_sorted, split_indices)\n\n        for interval in intervals:\n            p2_start, p2_end = interval[0], interval[-1]\n            intervals_dict[num_roots].append((p2_start, p2_end))\n\n# Print first and last intervals for each pose count\nfor num_roots, intervals in intervals_dict.items():\n    if intervals:\n        print(f\"\\nIntervals with {num_roots} poses:\")\n        if len(intervals) == 1:\n            p2_start, p2_end = intervals[0]\n            p2_end_str = \"infinity\" if np.isclose(p2_end, p2_max, atol=tol) else f\"{p2_end:.2f}\"\n            print(f\"  p2 from {p2_start:.2f} to {p2_end_str}\")\n        else:\n            # First interval\n            p2_start, p2_end = intervals[0]\n            p2_end_str = \"infinity\" if np.isclose(p2_end, p2_max, atol=tol) else f\"{p2_end:.2f}\"\n            print(f\"  p2 from {p2_start:.2f} to {p2_end_str}\")\n\n            # Last interval\n            p2_start, p2_end = intervals[-1]\n            p2_end_str = \"infinity\" if np.isclose(p2_end, p2_max, atol=tol) else f\"{p2_end:.2f}\"\n            print(f\"  p2 from {p2_start:.2f} to {p2_end_str}\")\n\n\n\n\n\n\n\n\n\n\nIntervals with 0 poses:\n  p2 from 0.00 to 3.71\n  p2 from 9.27 to infinity\n\nIntervals with 2 poses:\n  p2 from 3.72 to 4.86\n  p2 from 7.85 to 9.26\n\nIntervals with 4 poses:\n  p2 from 4.87 to 6.96\n  p2 from 7.03 to 7.84\n\nIntervals with 6 poses:\n  p2 from 6.97 to 7.02"
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/runge-phenomenon.html",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/runge-phenomenon.html",
    "title": "Runge Phenomenon",
    "section": "",
    "text": "The Runge Phenomenon refers to the oscillatory behavior that occurs when using high-degree polynomial interpolation on evenly spaced data points, particularly near the endpoints of an interval. The phenomenon is named after Carl Runge, who discovered this issue while studying interpolation of functions with large oscillations near the boundaries of an interval.\n\n\nWhen interpolating a smooth function using polynomials of high degree, the interpolation can become highly oscillatory near the edges of the interval, even if the function being interpolated is smooth and well-behaved. This is particularly problematic with equally spaced points, as the polynomial tries to fit too closely to the data points near the boundaries, leading to large errors.\n\n\n\nThe most famous example illustrating the Runge phenomenon is based on the Runge function:\n\nf(x) = \\frac{1}{1 + 25x^2}\n\nRunge showed that using high-degree polynomial interpolation on this function with evenly spaced points over the interval [-5, 5] leads to significant oscillations near the edges. The interpolation error grows as the degree of the polynomial increases, especially near the endpoints of the interval.\n\n\n\nThe Runge phenomenon arises because high-degree polynomials tend to oscillate more as their degree increases, especially when they are forced to pass through many points. With equally spaced points, the interpolation error is concentrated near the edges of the interval, causing large deviations from the true function in those regions.\nMathematically, for a function f(x) interpolated at n evenly spaced points using a polynomial P_n(x) of degree n-1, the interpolation error is given by:\n\n|f(x) - P_n(x)| = \\frac{|f^{(n)}(c)|}{n!} \\prod_{i=1}^{n}(x - x_i)\n\nAs the number of interpolation points n increases, the product term \\prod_{i=1}^{n}(x - x_i) becomes very large near the endpoints of the interval, causing the interpolation error to increase dramatically.\n\n\n\nThe following figure illustrates the Runge phenomenon. For the Runge function f(x) = \\frac{1}{1 + 25x^2}, the polynomial interpolation for large n shows excessive oscillations near the interval boundaries:\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef runge_function(x):\n    return 1 / (1 + 25 * x**2)\n\nx_values = np.linspace(-5, 5, 1000)\n\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, runge_function(x_values), label=\"Runge Function\", color=\"blue\")\n\nplt.title(\"Runge Phenomenon\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe Runge phenomenon can be mitigated by using Chebyshev nodes instead of equally spaced points. Chebyshev nodes distribute the points more densely near the edges of the interval, where the oscillations are most likely to occur. This results in more accurate interpolation with less oscillation near the boundaries.\nChebyshev nodes x_i for n interpolation points on the interval [-1, 1] are given by:\n\nx_i = \\cos\\left(\\frac{(2i - 1)\\pi}{2n}\\right)\n\nUsing Chebyshev interpolation reduces the risk of large oscillations near the endpoints, providing a more stable approximation.\n\n\n\nThe Runge phenomenon highlights the dangers of using high-degree polynomials for interpolation with evenly spaced points. To avoid this issue, it is recommended to use Chebyshev interpolation or lower-degree polynomial interpolation in smaller intervals (piecewise interpolation)."
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/runge-phenomenon.html#introduction",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/runge-phenomenon.html#introduction",
    "title": "Runge Phenomenon",
    "section": "",
    "text": "The Runge Phenomenon refers to the oscillatory behavior that occurs when using high-degree polynomial interpolation on evenly spaced data points, particularly near the endpoints of an interval. The phenomenon is named after Carl Runge, who discovered this issue while studying interpolation of functions with large oscillations near the boundaries of an interval.\n\n\nWhen interpolating a smooth function using polynomials of high degree, the interpolation can become highly oscillatory near the edges of the interval, even if the function being interpolated is smooth and well-behaved. This is particularly problematic with equally spaced points, as the polynomial tries to fit too closely to the data points near the boundaries, leading to large errors.\n\n\n\nThe most famous example illustrating the Runge phenomenon is based on the Runge function:\n\nf(x) = \\frac{1}{1 + 25x^2}\n\nRunge showed that using high-degree polynomial interpolation on this function with evenly spaced points over the interval [-5, 5] leads to significant oscillations near the edges. The interpolation error grows as the degree of the polynomial increases, especially near the endpoints of the interval.\n\n\n\nThe Runge phenomenon arises because high-degree polynomials tend to oscillate more as their degree increases, especially when they are forced to pass through many points. With equally spaced points, the interpolation error is concentrated near the edges of the interval, causing large deviations from the true function in those regions.\nMathematically, for a function f(x) interpolated at n evenly spaced points using a polynomial P_n(x) of degree n-1, the interpolation error is given by:\n\n|f(x) - P_n(x)| = \\frac{|f^{(n)}(c)|}{n!} \\prod_{i=1}^{n}(x - x_i)\n\nAs the number of interpolation points n increases, the product term \\prod_{i=1}^{n}(x - x_i) becomes very large near the endpoints of the interval, causing the interpolation error to increase dramatically.\n\n\n\nThe following figure illustrates the Runge phenomenon. For the Runge function f(x) = \\frac{1}{1 + 25x^2}, the polynomial interpolation for large n shows excessive oscillations near the interval boundaries:\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef runge_function(x):\n    return 1 / (1 + 25 * x**2)\n\nx_values = np.linspace(-5, 5, 1000)\n\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, runge_function(x_values), label=\"Runge Function\", color=\"blue\")\n\nplt.title(\"Runge Phenomenon\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nThe Runge phenomenon can be mitigated by using Chebyshev nodes instead of equally spaced points. Chebyshev nodes distribute the points more densely near the edges of the interval, where the oscillations are most likely to occur. This results in more accurate interpolation with less oscillation near the boundaries.\nChebyshev nodes x_i for n interpolation points on the interval [-1, 1] are given by:\n\nx_i = \\cos\\left(\\frac{(2i - 1)\\pi}{2n}\\right)\n\nUsing Chebyshev interpolation reduces the risk of large oscillations near the endpoints, providing a more stable approximation.\n\n\n\nThe Runge phenomenon highlights the dangers of using high-degree polynomials for interpolation with evenly spaced points. To avoid this issue, it is recommended to use Chebyshev interpolation or lower-degree polynomial interpolation in smaller intervals (piecewise interpolation)."
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/practice-problems/newtons-method-convergence/newtons-method-convergence.html",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/practice-problems/newtons-method-convergence/newtons-method-convergence.html",
    "title": "Newtons Method - Convergence Proof",
    "section": "",
    "text": "Theorem 1.6 (p. 35)\nAssume that g is continuously differentiable, g(r) = r, and S = |g'(r)| &lt; 1. Under these conditions, Fixed-Point Iteration converges linearly with rate S to the fixed point r for initial guesses sufficiently close to r.\n\n\nLet x_i represent the i-th iteration. By the Mean Value Theorem, there exists some c_i between x_i and r such that:\n\nx_{i+1} - r = g'(c_i)(x_i - r)\n\nHere, we substitute x_{i+1} = g(x_i) and r = g(r). Defining the error e_i = |x_i - r|, this equation can be rewritten as:\n\ne_{i+1} = |g'(c_i)|e_i\n\nIf S = |g'(r)| is less than one, the continuity of g' ensures that there exists a small neighborhood around r where |g'(x)| &lt; (S + 1)/2. This value is slightly larger than S, but still less than one. If x_i lies within this neighborhood, then c_i, being between x_i and r, also lies within this neighborhood. Consequently:\n\ne_{i+1} \\leq \\frac{S + 1}{2} e_i\n\nThis inequality shows that the error decreases by at least a factor of (S + 1)/2 on this and every subsequent step. As a result, \\lim_{i \\to \\infty} x_i = r. Furthermore, taking the limit of the ratio of consecutive errors yields:\n\n\\lim_{i \\to \\infty} \\frac{e_{i+1}}{e_i} = \\lim_{i \\to \\infty} |g'(c_i)| = |g'(r)| = S\n\nThis establishes that the iteration converges linearly with rate S."
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/practice-problems/newtons-method-convergence/newtons-method-convergence.html#fixed-point-iteration-convergence-theorem",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/practice-problems/newtons-method-convergence/newtons-method-convergence.html#fixed-point-iteration-convergence-theorem",
    "title": "Newtons Method - Convergence Proof",
    "section": "",
    "text": "Theorem 1.6 (p. 35)\nAssume that g is continuously differentiable, g(r) = r, and S = |g'(r)| &lt; 1. Under these conditions, Fixed-Point Iteration converges linearly with rate S to the fixed point r for initial guesses sufficiently close to r.\n\n\nLet x_i represent the i-th iteration. By the Mean Value Theorem, there exists some c_i between x_i and r such that:\n\nx_{i+1} - r = g'(c_i)(x_i - r)\n\nHere, we substitute x_{i+1} = g(x_i) and r = g(r). Defining the error e_i = |x_i - r|, this equation can be rewritten as:\n\ne_{i+1} = |g'(c_i)|e_i\n\nIf S = |g'(r)| is less than one, the continuity of g' ensures that there exists a small neighborhood around r where |g'(x)| &lt; (S + 1)/2. This value is slightly larger than S, but still less than one. If x_i lies within this neighborhood, then c_i, being between x_i and r, also lies within this neighborhood. Consequently:\n\ne_{i+1} \\leq \\frac{S + 1}{2} e_i\n\nThis inequality shows that the error decreases by at least a factor of (S + 1)/2 on this and every subsequent step. As a result, \\lim_{i \\to \\infty} x_i = r. Furthermore, taking the limit of the ratio of consecutive errors yields:\n\n\\lim_{i \\to \\infty} \\frac{e_{i+1}}{e_i} = \\lim_{i \\to \\infty} |g'(c_i)| = |g'(r)| = S\n\nThis establishes that the iteration converges linearly with rate S."
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/practice-problems/newtons-method-convergence/newtons-method-convergence.html#newtons-method-convergence-theorem",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/practice-problems/newtons-method-convergence/newtons-method-convergence.html#newtons-method-convergence-theorem",
    "title": "Newtons Method - Convergence Proof",
    "section": "Newton’s Method Convergence Theorem",
    "text": "Newton’s Method Convergence Theorem\nTheorem 1.11 (p. 53)\nLet f be twice continuously differentiable and f(r) = 0. If f'(r) \\neq 0, then Newton’s Method is locally and quadratically convergent to r. The error e_i at step i satisfies:\n\n\\lim_{i \\to \\infty} \\frac{e_{i+1}}{e_i^2} = M\n\nwhere\n\nM = \\left| \\frac{f''(r)}{2f'(r)} \\right|"
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/practice-problems/newtons-method-convergence/newtons-method-convergence.html#proof-1",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/practice-problems/newtons-method-convergence/newtons-method-convergence.html#proof-1",
    "title": "Newtons Method - Convergence Proof",
    "section": "Proof",
    "text": "Proof\n\n\n\n\n\n\n1. Show g'(r) = 0:\n\n\n\nNewton’s Method is\n\nx_{i+1} = x_i - \\frac{f(x_i)}{f'(x_i)}\n\nThis has the form of Fixed-Point Iteration (i.e., x_{i+1} = g(x_i)), where\n\ng(x) = x - \\frac{f(x)}{f'(x)}\n\nShow that g'(r) = 0.\nWhat does this imply about the convergence of Newton’s Method? (Hint: see Theorem 1.6)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nNewton’s Method is given by the iteration formula:\n\nx_{i+1} = x_i - \\frac{f(x_i)}{f'(x_i)}\n\nThis can be expressed in the form of Fixed-Point Iteration by defining:\n\ng(x) = x - \\frac{f(x)}{f'(x)}\n\nTherefore, the iteration becomes:\n\nx_{i+1} = g(x_i)\n\nTo find g'(r), first differentiate g(x) with respect to x:\n\ng'(x) = \\frac{d}{dx} \\left( x - \\frac{f(x)}{f'(x)} \\right) = 1 - \\frac{d}{dx} \\left( \\frac{f(x)}{f'(x)} \\right)\n\nUsing the Quotient Rule for differentiation:\n\n\\frac{d}{dx} \\left( \\frac{f(x)}{f'(x)} \\right) = \\frac{f'(x)f'(x) - f(x)f''(x)}{[f'(x)]^2} = \\frac{[f'(x)]^2 - f(x)f''(x)}{[f'(x)]^2}\n\nSubstituting back into the expression for g'(x):\n\ng'(x) = 1 - \\frac{[f'(x)]^2 - f(x)f''(x)}{[f'(x)]^2} = 1 - 1 + \\frac{f(x)f''(x)}{[f'(x)]^2} = \\frac{f(x)f''(x)}{[f'(x)]^2}\n\nGiven that r is the root of f(x):\n\nf(r) = 0\n\nPlugging x = r into the derivative:\n\ng'(r) = \\frac{f(r)f''(r)}{[f'(r)]^2} = \\frac{0 \\cdot f''(r)}{[f'(r)]^2} = 0\n\nTherefore:\n\ng'(r) = 0\n\nThis implies that convergence is guaranteed locally:\nTheorem 1.6 states that if |g'(r)| &lt; 1, the Fixed-Point Iteration will converge to r, provided the initial guess x_0 is sufficiently close to r. Since g'(r) = 0, this condition is satisfied with |g'(r)| = 0 &lt; 1. Thus, Newton’s Method will converge to the root r if the initial guess is close enough.\n\n\n\n\n\n\n\n\n\n2. First two terms of the Taylor series for f(r):\n\n\n\nTo complete our proof, we need to show that\n\n\\lim_{i \\to \\infty} \\frac{e_{i+1}}{e_i^2} = \\left| \\frac{f''(r)}{2f'(r)} \\right|\n\nBegin by writing out the first two terms of the Taylor series for f(r) centered at x_i, along with the remainder term (see p. 21, with x = r, x_0 = x_i, and c = c_i).\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe Taylor series expansion of a function f(x) around a point x_i up to the second term is given by:\n\nf(r) = f(x_i) + f'(x_i)(r - x_i) + \\frac{f''(c_i)}{2}(r - x_i)^2\n\nHere:\n\nx_i is the current approximation.\nc_i is a point between x_i and r.\n\n\n\n\n\n\n\n\n\n\n3. Manipulate result:\n\n\n\nManipulate your result so it looks like this (note: the LHS of this equation is the RHS of Newton’s Method):\n\nx_i - \\frac{f(x_i)}{f'(x_i)} = r + \\frac{f''(c_i)}{2f'(x_i)}(r - x_i)^2\n\nIn order for these expressions to be defined, we must have f'(x_i) \\neq 0. By assumption, we have f'(r) \\neq 0. What allows us to conclude that the same is true for f'(x_i)?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFrom the Taylor series expansion of f(r) centered at x_i, we have:\n\nf(r) = f(x_i) + f'(x_i)(r - x_i) + \\frac{f''(c_i)}{2}(r - x_i)^2\n\nSince r is the root of f(x), f(r) = 0. Substitute f(r) = 0:\n\n0 = f(x_i) + f'(x_i)(r - x_i) + \\frac{f''(c_i)}{2}(r - x_i)^2\n\nRearrange to solve for f(x_i):\n\nf(x_i) = -f'(x_i)(r - x_i) - \\frac{f''(c_i)}{2}(r - x_i)^2\n\nThe formula for Newton’s Method is:\n\nx_{i+1} = x_i - \\frac{f(x_i)}{f'(x_i)}\n\nSubstitute f(x_i) = -f'(x_i)(r - x_i) - \\frac{f''(c_i)}{2}(r - x_i)^2 into x_i - \\frac{f(x_i)}{f'(x_i)}:\n\nx_i - \\frac{f(x_i)}{f'(x_i)} = x_i - \\frac{-f'(x_i)(r - x_i) - \\frac{f''(c_i)}{2}(r - x_i)^2}{f'(x_i)}\n\nBreak this into two terms:\n\nThe first term:\n\n-\\frac{-f'(x_i)(r - x_i)}{f'(x_i)} = r - x_i\n\nThe second term: \n-\\frac{\\frac{f''(c_i)}{2}(r - x_i)^2}{f'(x_i)} = \\frac{f''(c_i)}{2f'(x_i)}(r - x_i)^2\n\n\nCombine the terms:\n\nx_i - \\frac{f(x_i)}{f'(x_i)} = r + \\frac{f''(c_i)}{2f'(x_i)}(r - x_i)^2\n\nAs the index i \\to \\infty, x_i \\to r. Since f'(x) is continuous and f'(r) \\neq 0, it follows that f'(x_i) \\neq 0 for all sufficiently large i.\n\n\n\n\n\n\n\n\n\n4. Combine results:\n\n\n\nLet e_i = |r - x_i| (so e_{i+1} = |r - x_{i+1}|). Use these together with the definition of Newton’s method in step (1) and your result from step (3) to obtain:\n\ne_{i+1} = \\left| \\frac{f''(c_i)}{2f'(x_i)} \\right| e_i^2\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nStart with the error definition at step i+1:\n\ne_{i+1} = |r - x_{i+1}|\n\nFrom the earlier derived formula for x_{i+1}, we have:\n\nx_{i+1} = r + \\frac{f''(c_i)}{2f'(x_i)}(r - x_i)^2\n\nSubstituting this into the error formula:\n\ne_{i+1} = |r - (r + \\frac{f''(c_i)}{2f'(x_i)}(r - x_i)^2)|\n\nSimplify the terms:\n\ne_{i+1} = |\\frac{f''(c_i)}{2f'(x_i)}(r - x_i)^2|\n\nUsing the definition of e_i = |r - x_i|, substitute e_i for |r - x_i|:\n\ne_{i+1} = \\left| \\frac{f''(c_i)}{2f'(x_i)} \\right| e_i^2\n\nThis matches the desired formula:\n\ne_{i+1} = \\left| \\frac{f''(c_i)}{2f'(x_i)} \\right| e_i^2\n\n\n\n\n\n\n\n\n\n\n5. Explain why:\n\n\n\nDividing both sides by e_i^2 and taking the limit as i \\to \\infty will complete the proof. However, evaluating the limit on the right-hand side requires some care. Justify each of the following steps in this process:\n\n\\lim_{i \\to \\infty} \\frac{e_{i+1}}{e_i^2} = \\lim_{i \\to \\infty} \\left| \\frac{f''(c_i)}{2f'(x_i)} \\right|\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nTo solve the limit\n\n\\lim_{i \\to \\infty} \\frac{e_{i+1}}{e_i^2} = \\lim_{i \\to \\infty} \\left| \\frac{f''(c_i)}{2f'(x_i)} \\right|\n\nproceed as follows:\n\n\\lim_{i \\to \\infty} \\left| \\frac{f''(c_i)}{2f'(x_i)} \\right| = \\lim_{i \\to \\infty} \\frac{|f''(c_i)|}{|2f'(x_i)|}\n\nThis works because the absolute value of a fraction is equal to the fraction of the absolute values.\n\n\\lim_{i \\to \\infty} \\frac{|f''(c_i)|}{|2f'(x_i)|} = \\frac{\\lim_{i \\to \\infty} |f''(c_i)|}{\\lim_{i \\to \\infty} |2f'(x_i)|}\n\nThis step follows from the property of limits that allows the limit of a fraction to be written as the fraction of the limits, provided the limits of both the numerator and denominator exist and the denominator does not approach zero.\n\n\\lim_{i \\to \\infty} |f''(c_i)| = |f''(r)|\n\nThe point c_i approaches r as i \\to \\infty because c_i is always between x_i and r, and Newton’s Method ensures x_i \\to r, causing the interval (x_i, r) to shrink to a single point, while the continuity of f''(x) guarantees f''(c_i) \\to f''(r).\n\n\\lim_{i \\to \\infty} |2f'(x_i)| = 2|f'(r)|\n\nThis is valid because as i \\to \\infty, x_i \\to r, and f'(x_i) is continuous. The factor of 2 in the denominator is a constant, so it remains unchanged during the limit process.\n\n\\frac{\\lim_{i \\to \\infty} |f''(c_i)|}{\\lim_{i \\to \\infty} |2f'(x_i)|} = \\left|\\frac{f''(r)}{2f'(r)}\\right|\n\nThus, the solution to the limit is:\n\n\\lim_{i \\to \\infty} \\frac{e_{i+1}}{e_i^2} = \\left|\\frac{f''(r)}{2f'(r)}\\right|\n\nThis completes the proof by showing that the error e_{i+1} at each step decreases quadratically in relation to the square of the error e_i at the previous step, demonstrating the quadratic convergence of Newton’s Method."
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/lagrange-interpolation.html",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/lagrange-interpolation.html",
    "title": "Lagrange Interpolation",
    "section": "",
    "text": "Lagrange Interpolation is a method of constructing a polynomial that passes through a given set of points. It is particularly useful when you have a small number of data points and want to determine the polynomial function that exactly fits those points."
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/lagrange-interpolation.html#the-lagrange-interpolating-polynomial",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/lagrange-interpolation.html#the-lagrange-interpolating-polynomial",
    "title": "Lagrange Interpolation",
    "section": "The Lagrange Interpolating Polynomial",
    "text": "The Lagrange Interpolating Polynomial\nGiven n distinct data points (x_1, y_1), (x_2, y_2), ..., (x_n, y_n), the Lagrange interpolating polynomial is the polynomial P(x) of degree at most n-1 that passes through all the points, meaning:\n\nP(x_i) = y_i \\quad \\text{for each} \\ i = 1, 2, ..., n\n\nThe Lagrange form of the polynomial is given by:\n\nP(x) = \\sum_{i=1}^{n} y_i L_i(x)\n\nwhere L_i(x) are the Lagrange basis polynomials, defined as:\n\nL_i(x) = \\prod_{j=1, j \\neq i}^{n} \\frac{x - x_j}{x_i - x_j}\n\nHere, the product is taken over all j \\neq i, ensuring that L_i(x_j) = 0 for j \\neq i and L_i(x_i) = 1. This ensures that P(x_i) = y_i for each i.\n\nStep-by-Step Calculation\nSuppose we are given a set of three points: (x_1, y_1), (x_2, y_2), (x_3, y_3). The Lagrange interpolating polynomial is:\n\nP(x) = y_1 L_1(x) + y_2 L_2(x) + y_3 L_3(x)\n\nwhere the Lagrange basis polynomials are:\n\nL_1(x) = \\frac{(x - x_2)(x - x_3)}{(x_1 - x_2)(x_1 - x_3)}\n\n\nL_2(x) = \\frac{(x - x_1)(x - x_3)}{(x_2 - x_1)(x_2 - x_3)}\n\n\nL_3(x) = \\frac{(x - x_1)(x - x_2)}{(x_3 - x_1)(x_3 - x_2)}\n\nBy evaluating these expressions and plugging in the values of y_1, y_2, and y_3, we obtain the polynomial P(x) that passes through all the given points.\n\n\nExample\nLet’s go through an example where we are given three points: (1, 2), (2, 3), and (3, 5).\n\nPoints:\n(x_1, y_1) = (1, 2)\n(x_2, y_2) = (2, 3)\n(x_3, y_3) = (3, 5)\nLagrange basis polynomials:\n\n\nL_1(x) = \\frac{(x - 2)(x - 3)}{(1 - 2)(1 - 3)} = \\frac{(x - 2)(x - 3)}{2}\n\n\nL_2(x) = \\frac{(x - 1)(x - 3)}{(2 - 1)(2 - 3)} = \\frac{(x - 1)(x - 3)}{-1}\n\n\nL_3(x) = \\frac{(x - 1)(x - 2)}{(3 - 1)(3 - 2)} = \\frac{(x - 1)(x - 2)}{2}\n\n\nLagrange interpolating polynomial:\n\n\nP(x) = 2 \\cdot L_1(x) + 3 \\cdot L_2(x) + 5 \\cdot L_3(x)\n\nSubstituting the values for L_1(x), L_2(x), and L_3(x):\n\nP(x) = 2 \\cdot \\frac{(x - 2)(x - 3)}{2} + 3 \\cdot \\frac{(x - 1)(x - 3)}{-1} + 5 \\cdot \\frac{(x - 1)(x - 2)}{2}\n\nSimplifying this expression will give you the final polynomial P(x) that passes through all three points.\n\n\nGeneral Properties of Lagrange Interpolation\n\nUniqueness: There is exactly one polynomial of degree n-1 that interpolates n points. This is guaranteed by the fundamental theorem of algebra, which states that a polynomial of degree n-1 is uniquely determined by n distinct points.\nEfficiency: Lagrange interpolation is not the most computationally efficient method for large datasets, because each term depends on all the data points, making the calculation costly for large n. Methods like Newton’s divided differences are generally preferred for interpolation with larger datasets.\nAccuracy: Interpolation works well if the points are well-distributed and the function is smooth. However, for unevenly spaced points or functions with high curvature, the interpolation polynomial may oscillate significantly, a phenomenon known as Runge’s phenomenon.\n\n\n\nApplications of Lagrange Interpolation\n\nCurve Fitting: Lagrange interpolation can be used to construct a polynomial that exactly fits a given set of data points.\nNumerical Integration: The interpolating polynomial can be used to approximate integrals through methods such as Newton-Cotes formulas.\nGraphics and Animation: In computer graphics, Lagrange interpolation is used to smoothly interpolate between keyframes in animations.\nSignal Processing: It is used in digital signal processing for reconstructing missing samples from a set of known data points.\n\n\n\nDrawbacks\n\nRunge’s Phenomenon: Lagrange interpolation can lead to significant oscillation, especially when interpolating over large intervals with a high degree polynomial.\nNot Easily Updateable: If a new point is added, the entire Lagrange polynomial must be recalculated. In contrast, methods like Newton’s divided differences allow for easier updates when new points are added.\n\n\n\nConclusion\nLagrange interpolation is a powerful tool for constructing polynomials that pass through a set of points, but it can suffer from inefficiencies and oscillations for large datasets. It’s important to understand its benefits and limitations to use it effectively in applications."
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/index.html",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/index.html",
    "title": "POLYNOMIAL INTERPOLATION",
    "section": "",
    "text": "Lagrange Interpolation\nNewton’s Divided Differences\nInterpolation Error Formula\nRunge Phenomenon\nChebyshev Interpolation"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/spectral-radius.html",
    "href": "mathematics/numerical-analysis/linear-systems/spectral-radius.html",
    "title": "Spectral Radius",
    "section": "",
    "text": "The spectral radius is a fundamental concept in linear algebra and matrix analysis, particularly in understanding the behavior of iterative methods like the Jacobi Method for solving linear systems."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/spectral-radius.html#overview",
    "href": "mathematics/numerical-analysis/linear-systems/spectral-radius.html#overview",
    "title": "Spectral Radius",
    "section": "",
    "text": "The spectral radius is a fundamental concept in linear algebra and matrix analysis, particularly in understanding the behavior of iterative methods like the Jacobi Method for solving linear systems."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/spectral-radius.html#definition-of-spectral-radius",
    "href": "mathematics/numerical-analysis/linear-systems/spectral-radius.html#definition-of-spectral-radius",
    "title": "Spectral Radius",
    "section": "Definition of Spectral Radius",
    "text": "Definition of Spectral Radius\nThe spectral radius of a square matrix A is defined as the largest absolute value of its eigenvalues:\n\n\\rho(A) = \\max \\{ |\\lambda| : \\lambda \\text{ is an eigenvalue of } A \\}\n\n\nInterpretation: It measures the “largest influence” of the eigenvalues of A, which governs the convergence properties of matrix-related iterative processes."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/spectral-radius.html#properties-of-the-spectral-radius",
    "href": "mathematics/numerical-analysis/linear-systems/spectral-radius.html#properties-of-the-spectral-radius",
    "title": "Spectral Radius",
    "section": "Properties of the Spectral Radius",
    "text": "Properties of the Spectral Radius\n\nNon-Negativity:\n\n\\rho(A) \\geq 0\n\nsince the spectral radius is the maximum of the absolute values of eigenvalues.\nBehavior Under Similarity Transformations: If B = P^{-1}AP, then:\n\n\\rho(B) = \\rho(A)\n\nbecause eigenvalues are invariant under similarity transformations.\nNorm Relationship: The spectral radius is related to matrix norms but generally satisfies: \n\\rho(A) \\leq \\|A\\|\n for certain matrix norms. Equality holds in some cases, such as the spectral norm for symmetric matrices."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/spectral-radius.html#importance-in-iterative-methods",
    "href": "mathematics/numerical-analysis/linear-systems/spectral-radius.html#importance-in-iterative-methods",
    "title": "Spectral Radius",
    "section": "Importance in Iterative Methods",
    "text": "Importance in Iterative Methods\nThe spectral radius plays a key role in determining the convergence of iterative methods for solving systems of linear equations.\n\nConvergence Criterion\nFor an iterative method defined by:\n\nx_{k+1} = Gx_k + c\n\nwhere G is the iteration matrix, the method converges if and only if:\n\n\\rho(G) &lt; 1\n\n\nExplanation: This ensures that successive iterations diminish in magnitude, eventually converging to the solution."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/spectral-radius.html#example-computing-the-spectral-radius",
    "href": "mathematics/numerical-analysis/linear-systems/spectral-radius.html#example-computing-the-spectral-radius",
    "title": "Spectral Radius",
    "section": "Example: Computing the Spectral Radius",
    "text": "Example: Computing the Spectral Radius\nConsider the matrix:\n\nA = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 3\n\\end{bmatrix}\n\n\nStep 1: Find Eigenvalues\nSolve \\det(A - \\lambda I) = 0:\n\n\\det\\left(\\begin{bmatrix}\n2-\\lambda & 1 \\\\\n1 & 3-\\lambda\n\\end{bmatrix}\\right) = 0\n\nExpanding the determinant:\n\n(2-\\lambda)(3-\\lambda) - 1 = \\lambda^2 - 5\\lambda + 5 = 0\n\nThe eigenvalues are:\n\n\\lambda = \\frac{5 \\pm \\sqrt{5}}{2}\n\n\n\nStep 2: Compute Spectral Radius\nThe eigenvalues are approximately:\n\n\\lambda_1 \\approx 4.618, \\quad \\lambda_2 \\approx 0.382\n\nThus, the spectral radius is:\n\n\\rho(A) = \\max(|\\lambda_1|, |\\lambda_2|) = 4.618"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/spectral-radius.html#geometric-interpretation",
    "href": "mathematics/numerical-analysis/linear-systems/spectral-radius.html#geometric-interpretation",
    "title": "Spectral Radius",
    "section": "Geometric Interpretation",
    "text": "Geometric Interpretation\nThe spectral radius represents the largest “stretching factor” of a matrix when applied to a vector. For a square matrix A, eigenvectors corresponding to the eigenvalue with the largest magnitude indicate the direction in which A has the most influence."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/spectral-radius.html#applications-of-spectral-radius",
    "href": "mathematics/numerical-analysis/linear-systems/spectral-radius.html#applications-of-spectral-radius",
    "title": "Spectral Radius",
    "section": "Applications of Spectral Radius",
    "text": "Applications of Spectral Radius\n\nIterative Solvers:\n\nDetermines the convergence of methods like Jacobi and Gauss-Seidel.\n\nStability Analysis:\n\nUsed in analyzing the stability of dynamical systems where A represents a system’s state transition.\n\nNetwork Analysis:\n\nIn graph theory, the spectral radius of adjacency matrices provides insights into network properties."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/spectral-radius.html#conclusion",
    "href": "mathematics/numerical-analysis/linear-systems/spectral-radius.html#conclusion",
    "title": "Spectral Radius",
    "section": "Conclusion",
    "text": "Conclusion\nThe spectral radius is a powerful tool for understanding the properties of matrices, particularly in the context of iterative methods and stability analysis. Mastering this concept is essential for applications ranging from solving linear systems to analyzing complex networks and dynamical systems."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/taxicab-vector-norm.html",
    "href": "mathematics/numerical-analysis/linear-systems/norms/taxicab-vector-norm.html",
    "title": "Taxicab Norm",
    "section": "",
    "text": "The Taxicab norm (also known as the Manhattan norm or \\ell_1-norm) is a way of measuring the “distance” of a vector in a grid-like space. Unlike the Euclidean norm, which measures the straight-line distance, the Taxicab norm sums the absolute values of a vector’s components. It is often used in scenarios where movement is constrained to axes, like navigating a city grid."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/taxicab-vector-norm.html#overview",
    "href": "mathematics/numerical-analysis/linear-systems/norms/taxicab-vector-norm.html#overview",
    "title": "Taxicab Norm",
    "section": "",
    "text": "The Taxicab norm (also known as the Manhattan norm or \\ell_1-norm) is a way of measuring the “distance” of a vector in a grid-like space. Unlike the Euclidean norm, which measures the straight-line distance, the Taxicab norm sums the absolute values of a vector’s components. It is often used in scenarios where movement is constrained to axes, like navigating a city grid."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/taxicab-vector-norm.html#definition",
    "href": "mathematics/numerical-analysis/linear-systems/norms/taxicab-vector-norm.html#definition",
    "title": "Taxicab Norm",
    "section": "Definition",
    "text": "Definition\nFor a vector \\mathbf{v} = [v_1, v_2, \\dots, v_n] in \\mathbb{R}^n, the Taxicab norm is defined as:\n\n\\|\\mathbf{v}\\|_1 = \\sum_{i=1}^n |v_i|\n\nThis measures the total “travel distance” required along the axes to reach the endpoint of the vector."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/taxicab-vector-norm.html#properties",
    "href": "mathematics/numerical-analysis/linear-systems/norms/taxicab-vector-norm.html#properties",
    "title": "Taxicab Norm",
    "section": "Properties",
    "text": "Properties\nThe Taxicab norm satisfies the following properties:\n\nNon-negativity: \\|\\mathbf{v}\\|_1 \\geq 0, and \\|\\mathbf{v}\\|_1 = 0 if and only if \\mathbf{v} = \\mathbf{0}.\nHomogeneity: For any scalar c, \\|c \\mathbf{v}\\|_1 = |c| \\|\\mathbf{v}\\|_1.\nTriangle Inequality: \\|\\mathbf{u} + \\mathbf{v}\\|_1 \\leq \\|\\mathbf{u}\\|_1 + \\|\\mathbf{v}\\|_1."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/taxicab-vector-norm.html#examples",
    "href": "mathematics/numerical-analysis/linear-systems/norms/taxicab-vector-norm.html#examples",
    "title": "Taxicab Norm",
    "section": "Examples",
    "text": "Examples\n\n1. 2D Example\nFor \\mathbf{v} = [3, -4]:\n\n\\|\\mathbf{v}\\|_1 = |3| + |-4| = 3 + 4 = 7\n\n\n\n2. 3D Example\nFor \\mathbf{v} = [1, -2, 3]:\n\n\\|\\mathbf{v}\\|_1 = |1| + |-2| + |3| = 1 + 2 + 3 = 6\n\n\n\n3. General Case\nFor \\mathbf{v} = [v_1, v_2, \\dots, v_n]:\n\n\\|\\mathbf{v}\\|_1 = |v_1| + |v_2| + \\cdots + |v_n|"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/taxicab-vector-norm.html#applications",
    "href": "mathematics/numerical-analysis/linear-systems/norms/taxicab-vector-norm.html#applications",
    "title": "Taxicab Norm",
    "section": "Applications",
    "text": "Applications\n\n1. Urban Navigation\nThe Taxicab norm models distance in grid-based systems, such as city streets, where movement is restricted to horizontal and vertical directions.\n\n\n2. Machine Learning\nIn machine learning, the \\ell_1-norm is used as a regularization term (Lasso regression) to encourage sparsity in models.\n\n\n3. Optimization\nThe Taxicab norm is used in linear programming and optimization problems where constraints align with the \\ell_1-metric.\n\n\n4. Signal Processing\nThe \\ell_1-norm is used in compressed sensing and sparse recovery to find solutions with minimal non-zero components."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/taxicab-vector-norm.html#visualization",
    "href": "mathematics/numerical-analysis/linear-systems/norms/taxicab-vector-norm.html#visualization",
    "title": "Taxicab Norm",
    "section": "Visualization",
    "text": "Visualization\nIn 2D, the set of points at a fixed Taxicab norm distance from the origin forms a diamond shape (rotated square). For example, all points satisfying \\|\\mathbf{v}\\|_1 = 3 in \\mathbb{R}^2 would form the square:\n\n|x| + |y| = 3"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/taxicab-vector-norm.html#example-problem",
    "href": "mathematics/numerical-analysis/linear-systems/norms/taxicab-vector-norm.html#example-problem",
    "title": "Taxicab Norm",
    "section": "Example Problem",
    "text": "Example Problem\nProblem: Compute the Taxicab norm of \\mathbf{v} = [-3, 4, -5].\n\nSolution:\n\nTake the absolute values of the components: |-3| = 3, |4| = 4, |-5| = 5.\nSum them: \\|\\mathbf{v}\\|_1 = 3 + 4 + 5 = 12."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/taxicab-vector-norm.html#conclusion",
    "href": "mathematics/numerical-analysis/linear-systems/norms/taxicab-vector-norm.html#conclusion",
    "title": "Taxicab Norm",
    "section": "Conclusion",
    "text": "Conclusion\nThe Taxicab norm is a useful measure of distance in grid-based systems and finds applications in various fields such as geometry, optimization, and machine learning. Its simplicity and intuitive interpretation make it a valuable tool for analyzing vector magnitudes under \\ell_1-metrics."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/infinity-norm-matrices.html",
    "href": "mathematics/numerical-analysis/linear-systems/norms/infinity-norm-matrices.html",
    "title": "Infinity Norm for Matrices",
    "section": "",
    "text": "The infinity norm (or maximum row sum norm) of a matrix is a measure of its size, calculated as the maximum absolute row sum. It is often used to analyze the stability of numerical algorithms and the behavior of matrices in linear transformations. The infinity norm provides an upper bound on the effect a matrix can have on a vector in terms of row-wise contributions."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/infinity-norm-matrices.html#overview",
    "href": "mathematics/numerical-analysis/linear-systems/norms/infinity-norm-matrices.html#overview",
    "title": "Infinity Norm for Matrices",
    "section": "",
    "text": "The infinity norm (or maximum row sum norm) of a matrix is a measure of its size, calculated as the maximum absolute row sum. It is often used to analyze the stability of numerical algorithms and the behavior of matrices in linear transformations. The infinity norm provides an upper bound on the effect a matrix can have on a vector in terms of row-wise contributions."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/infinity-norm-matrices.html#definition",
    "href": "mathematics/numerical-analysis/linear-systems/norms/infinity-norm-matrices.html#definition",
    "title": "Infinity Norm for Matrices",
    "section": "Definition",
    "text": "Definition\nFor a matrix A = [a_{ij}] of size m \\times n, the infinity norm is defined as:\n\n\\|A\\|_\\infty = \\max_{1 \\leq i \\leq m} \\sum_{j=1}^n |a_{ij}|\n\nThis is equivalent to finding the largest sum of absolute values of the elements in any row of the matrix.\n\nGeometric Interpretation\nThe infinity norm quantifies the maximum influence of a row of the matrix when applied to a vector. It answers the question: “What is the largest total contribution of any single row?”"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/infinity-norm-matrices.html#properties",
    "href": "mathematics/numerical-analysis/linear-systems/norms/infinity-norm-matrices.html#properties",
    "title": "Infinity Norm for Matrices",
    "section": "Properties",
    "text": "Properties\nThe infinity norm satisfies the following key properties:\n\nNon-negativity: \\|A\\|_\\infty \\geq 0, and \\|A\\|_\\infty = 0 if and only if A = 0 (a zero matrix).\nHomogeneity: For any scalar c, \\|cA\\|_\\infty = |c| \\|A\\|_\\infty.\nTriangle Inequality: \\|A + B\\|_\\infty \\leq \\|A\\|_\\infty + \\|B\\|_\\infty."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/infinity-norm-matrices.html#example",
    "href": "mathematics/numerical-analysis/linear-systems/norms/infinity-norm-matrices.html#example",
    "title": "Infinity Norm for Matrices",
    "section": "Example",
    "text": "Example\nGiven a matrix A:\n\nA = \\begin{pmatrix}\n1 & -2 & 3 \\\\\n-4 & 5 & -6 \\\\\n7 & -8 & 9\n\\end{pmatrix}\n\n\nCompute the row sums:\n\nRow 1: |1| + |-2| + |3| = 1 + 2 + 3 = 6,\nRow 2: |-4| + |5| + |-6| = 4 + 5 + 6 = 15,\nRow 3: |7| + |-8| + |9| = 7 + 8 + 9 = 24.\n\nFind the maximum row sum: \n\\|A\\|_\\infty = \\max(6, 15, 24) = 24\n\n\nThus, the infinity norm of A is 24."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/infinity-norm-matrices.html#applications",
    "href": "mathematics/numerical-analysis/linear-systems/norms/infinity-norm-matrices.html#applications",
    "title": "Infinity Norm for Matrices",
    "section": "Applications",
    "text": "Applications\n\nNumerical Stability:\n\nThe infinity norm is often used to measure the sensitivity of solutions to changes in the matrix.\n\nCondition Numbers:\n\nIt contributes to the calculation of matrix condition numbers, which measure how close a matrix is to being singular.\n\nError Analysis:\n\nThe infinity norm is used to bound errors in iterative algorithms and numerical solutions to matrix equations.\n\nOptimization:\n\nIt appears in optimization problems where constraints are defined by row-wise contributions."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/infinity-norm-matrices.html#visualization",
    "href": "mathematics/numerical-analysis/linear-systems/norms/infinity-norm-matrices.html#visualization",
    "title": "Infinity Norm for Matrices",
    "section": "Visualization",
    "text": "Visualization\nFor a matrix, the infinity norm represents the largest “row weight” when summing all the absolute values of the row’s entries. It gives insight into the matrix’s effect along the rows."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/infinity-norm-matrices.html#example-problem",
    "href": "mathematics/numerical-analysis/linear-systems/norms/infinity-norm-matrices.html#example-problem",
    "title": "Infinity Norm for Matrices",
    "section": "Example Problem",
    "text": "Example Problem\nProblem: Compute the infinity norm of:\n\nA = \\begin{pmatrix}\n2 & -1 \\\\\n-3 & 4\n\\end{pmatrix}\n\n\nSolution:\n\nCompute the row sums:\n\nRow 1: |2| + |-1| = 2 + 1 = 3,\nRow 2: |-3| + |4| = 3 + 4 = 7.\n\nFind the maximum row sum: \n\\|A\\|_\\infty = \\max(3, 7) = 7\n\n\nThus, \\|A\\|_\\infty = 7."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/infinity-norm-matrices.html#conclusion",
    "href": "mathematics/numerical-analysis/linear-systems/norms/infinity-norm-matrices.html#conclusion",
    "title": "Infinity Norm for Matrices",
    "section": "Conclusion",
    "text": "Conclusion\nThe infinity norm provides a simple yet powerful way to measure the row-wise magnitude of a matrix. Its computational simplicity and relevance to numerical analysis make it an essential tool in matrix computations."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/euclidean-vector-norm.html",
    "href": "mathematics/numerical-analysis/linear-systems/norms/euclidean-vector-norm.html",
    "title": "Euclidean Vector Norm",
    "section": "",
    "text": "The Euclidean vector norm (or 2-norm) is a measure of the magnitude (or length) of a vector in Euclidean space. It is widely used in mathematics, physics, and engineering for calculating distances, measuring error, and normalizing vectors. This note covers the definition, properties, computation, and applications of the Euclidean norm."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/euclidean-vector-norm.html#overview",
    "href": "mathematics/numerical-analysis/linear-systems/norms/euclidean-vector-norm.html#overview",
    "title": "Euclidean Vector Norm",
    "section": "",
    "text": "The Euclidean vector norm (or 2-norm) is a measure of the magnitude (or length) of a vector in Euclidean space. It is widely used in mathematics, physics, and engineering for calculating distances, measuring error, and normalizing vectors. This note covers the definition, properties, computation, and applications of the Euclidean norm."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/euclidean-vector-norm.html#definition",
    "href": "mathematics/numerical-analysis/linear-systems/norms/euclidean-vector-norm.html#definition",
    "title": "Euclidean Vector Norm",
    "section": "Definition",
    "text": "Definition\nFor a vector \\mathbf{v} = [v_1, v_2, \\dots, v_n] in \\mathbb{R}^n, the Euclidean norm is defined as:\n\n\\|\\mathbf{v}\\|_2 = \\sqrt{\\sum_{i=1}^n v_i^2}\n\nThis formula calculates the straight-line distance from the origin to the point represented by \\mathbf{v} in n-dimensional space."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/euclidean-vector-norm.html#properties",
    "href": "mathematics/numerical-analysis/linear-systems/norms/euclidean-vector-norm.html#properties",
    "title": "Euclidean Vector Norm",
    "section": "Properties",
    "text": "Properties\nThe Euclidean norm satisfies several key properties:\n\nNon-negativity: \\|\\mathbf{v}\\|_2 \\geq 0, and \\|\\mathbf{v}\\|_2 = 0 if and only if \\mathbf{v} = \\mathbf{0}.\nHomogeneity: For any scalar c, \\|c \\mathbf{v}\\|_2 = |c| \\|\\mathbf{v}\\|_2.\nTriangle Inequality: \\|\\mathbf{u} + \\mathbf{v}\\|_2 \\leq \\|\\mathbf{u}\\|_2 + \\|\\mathbf{v}\\|_2."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/euclidean-vector-norm.html#examples",
    "href": "mathematics/numerical-analysis/linear-systems/norms/euclidean-vector-norm.html#examples",
    "title": "Euclidean Vector Norm",
    "section": "Examples",
    "text": "Examples\n\n1. 2D Vector\nFor \\mathbf{v} = [3, 4]:\n\n\\|\\mathbf{v}\\|_2 = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = 5\n\n\n\n2. 3D Vector\nFor \\mathbf{v} = [1, 2, 3]:\n\n\\|\\mathbf{v}\\|_2 = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{1 + 4 + 9} = \\sqrt{14}\n\n\n\n3. General Case\nFor \\mathbf{v} = [v_1, v_2, \\dots, v_n]:\n\n\\|\\mathbf{v}\\|_2 = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/euclidean-vector-norm.html#applications",
    "href": "mathematics/numerical-analysis/linear-systems/norms/euclidean-vector-norm.html#applications",
    "title": "Euclidean Vector Norm",
    "section": "Applications",
    "text": "Applications\n\n1. Measuring Distance\nThe Euclidean norm is used to calculate the distance between two points \\mathbf{u} and \\mathbf{v} in space:\n\n\\|\\mathbf{u} - \\mathbf{v}\\|_2 = \\sqrt{\\sum_{i=1}^n (u_i - v_i)^2}\n\n\n\n2. Normalizing Vectors\nTo convert a vector to unit length, divide it by its Euclidean norm:\n\n\\mathbf{u} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|_2}\n\n\n\n3. Error Measurement\nIn numerical analysis, the Euclidean norm measures the error or residual of a solution.\n\n\n4. Machine Learning\nThe Euclidean norm underpins metrics like the Euclidean distance, used in clustering and regression."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/euclidean-vector-norm.html#visualization",
    "href": "mathematics/numerical-analysis/linear-systems/norms/euclidean-vector-norm.html#visualization",
    "title": "Euclidean Vector Norm",
    "section": "Visualization",
    "text": "Visualization\nIn two-dimensional space, the Euclidean norm corresponds to the length of the hypotenuse of a right triangle formed by the vector’s components. This is equivalent to the straight-line distance from the origin to the point represented by the vector."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/euclidean-vector-norm.html#example-problem",
    "href": "mathematics/numerical-analysis/linear-systems/norms/euclidean-vector-norm.html#example-problem",
    "title": "Euclidean Vector Norm",
    "section": "Example Problem",
    "text": "Example Problem\nProblem: Compute the Euclidean norm of \\mathbf{v} = [2, -3, 6].\n\nSolution:\n\nSquare each component: 2^2 = 4, (-3)^2 = 9, 6^2 = 36.\nSum the squares: 4 + 9 + 36 = 49.\nTake the square root: \\|\\mathbf{v}\\|_2 = \\sqrt{49} = 7."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/euclidean-vector-norm.html#conclusion",
    "href": "mathematics/numerical-analysis/linear-systems/norms/euclidean-vector-norm.html#conclusion",
    "title": "Euclidean Vector Norm",
    "section": "Conclusion",
    "text": "Conclusion\nThe Euclidean vector norm is an essential concept in mathematics and its applications, providing a straightforward measure of vector magnitude. Its simplicity and intuitive geometric interpretation make it a fundamental tool across diverse fields such as physics, engineering, and machine learning."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/lu-factorization.html",
    "href": "mathematics/numerical-analysis/linear-systems/lu-factorization.html",
    "title": "LU Factorization",
    "section": "",
    "text": "LU Factorization (or LU Decomposition) is a powerful technique in linear algebra for breaking down a matrix A into the product of a lower triangular matrix L and an upper triangular matrix U. This factorization is commonly used for solving linear systems, computing determinants, and inverting matrices."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/lu-factorization.html#overview",
    "href": "mathematics/numerical-analysis/linear-systems/lu-factorization.html#overview",
    "title": "LU Factorization",
    "section": "",
    "text": "LU Factorization (or LU Decomposition) is a powerful technique in linear algebra for breaking down a matrix A into the product of a lower triangular matrix L and an upper triangular matrix U. This factorization is commonly used for solving linear systems, computing determinants, and inverting matrices."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/lu-factorization.html#definition-and-decomposition",
    "href": "mathematics/numerical-analysis/linear-systems/lu-factorization.html#definition-and-decomposition",
    "title": "LU Factorization",
    "section": "Definition and Decomposition",
    "text": "Definition and Decomposition\nFor a square matrix A, LU Factorization is given by:\n\nA = LU\n\nwhere:\n\nL is a lower triangular matrix with ones on the diagonal.\nU is an upper triangular matrix.\n\nIf A cannot be decomposed directly, partial pivoting may be applied, resulting in:\n\nPA = LU\n\nwhere P is a permutation matrix that records row exchanges."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/lu-factorization.html#why-you-subtract-rows",
    "href": "mathematics/numerical-analysis/linear-systems/lu-factorization.html#why-you-subtract-rows",
    "title": "LU Factorization",
    "section": "Why You Subtract Rows",
    "text": "Why You Subtract Rows\nWhen performing row operations to compute LU, the operation for row elimination is explicitly written as:\n\nR_j \\to R_j - L_{ij} R_i\n\nwhere:\n\nR_i is the pivot row.\nR_j is the row being updated.\nL_{ij} = \\frac{a_{ji}}{a_{ii}} is the multiplier used to eliminate the element a_{ji}.\n\n\nImportance of Subtraction:\n\nCorrect Multiplier Signs: Subtracting ensures that the multiplier L_{ij} is stored with its correct sign in the L-matrix.\nConsistency in Computation: Adding the negative version (i.e., R_j \\to R_j + (-L_{ij} R_i)) can lead to confusion and sign errors in L_{ij}, even though it seems conceptually similar.\nPreventing Errors: Adding instead of subtracting might inadvertently reverse the signs of the entries in L, causing errors in the final decomposition.\n\n\n\nExample of a Sign Error:\nSuppose we start with the matrix:\n\nA = \\begin{bmatrix}\n4 & 8 & -4 \\\\\n-1 & 1 & -2 \\\\\n2 & 3 & 1\n\\end{bmatrix}\n\n\nCorrect Subtraction:\nCompute L_{21} = \\frac{-1}{4}, and perform R_2 \\to R_2 - L_{21} R_1:\n\nL_{21} = \\frac{-1}{4}, \\quad R_2 \\to R_2 - L_{21} R_1\n\nResult:\n\n\\begin{bmatrix}\n4 & 8 & -4 \\\\\n0 & 3 & -3 \\\\\n2 & 3 & 1\n\\end{bmatrix}\n\nThe multiplier L_{21} = \\frac{-1}{4} is correctly stored.\nAdding the Negative Version:\nIf we compute R_2 \\to R_2 + (-L_{21} R_1), we might misinterpret -L_{21} = \\frac{1}{4} as +\\frac{1}{4}:\n\nL_{21} = \\frac{1}{4}, \\quad R_2 \\to R_2 + (-L_{21} R_1)\n\nResult:\n\n\\begin{bmatrix}\n4 & 8 & -4 \\\\\n0 & -3 & +3 \\\\\n2 & 3 & 1\n\\end{bmatrix}\n\nHere, L_{21} would have the wrong sign in the L-matrix, leading to incorrect results."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/lu-factorization.html#conditions-for-lu-factorization",
    "href": "mathematics/numerical-analysis/linear-systems/lu-factorization.html#conditions-for-lu-factorization",
    "title": "LU Factorization",
    "section": "Conditions for LU Factorization",
    "text": "Conditions for LU Factorization\nLU Factorization is valid when:\n\nMatrix is Square: A must be a square matrix.\nNon-Singular Leading Submatrices: Each leading principal submatrix (upper-left submatrix) of A must be non-singular.\n\nWhen these conditions are not met, row pivoting enables decomposition."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/lu-factorization.html#factorization-process",
    "href": "mathematics/numerical-analysis/linear-systems/lu-factorization.html#factorization-process",
    "title": "LU Factorization",
    "section": "Factorization Process",
    "text": "Factorization Process\nTo factorize A into L and U:\n\nEliminate Elements: Perform row operations to create zeros below the main diagonal of U.\nStore Multipliers: Record the multipliers in L.\n\n\nExample\nGiven a 3 \\times 3 matrix:\n\nA = \\begin{pmatrix} 2 & 3 & 1 \\\\ 4 & 7 & -1 \\\\ -2 & 3 & 5 \\end{pmatrix}\n\n\nTransform A into U using row operations.\nRecord elimination factors in L.\nThe result satisfies A = LU."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/lu-factorization.html#solving-systems-with-lu-factorization",
    "href": "mathematics/numerical-analysis/linear-systems/lu-factorization.html#solving-systems-with-lu-factorization",
    "title": "LU Factorization",
    "section": "Solving Systems with LU Factorization",
    "text": "Solving Systems with LU Factorization\nLU Factorization allows us to solve Ax = b by breaking it down into two simpler systems:\n\nSolve Ly = b: Use forward substitution, as L is lower triangular.\nSolve Ux = y: Use back substitution with U as an upper triangular matrix."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/lu-factorization.html#pivoting-and-permutation",
    "href": "mathematics/numerical-analysis/linear-systems/lu-factorization.html#pivoting-and-permutation",
    "title": "LU Factorization",
    "section": "Pivoting and Permutation",
    "text": "Pivoting and Permutation\nIn cases where A has zeros or small values on the diagonal, partial pivoting improves numerical stability by reordering rows to place a larger element on the diagonal:\n\nPA = LU\n\nwhere P is a permutation matrix that tracks row exchanges."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/lu-factorization.html#conclusion",
    "href": "mathematics/numerical-analysis/linear-systems/lu-factorization.html#conclusion",
    "title": "LU Factorization",
    "section": "Conclusion",
    "text": "Conclusion\nLU Factorization is an essential tool in linear algebra, providing a simplified method to solve linear systems efficiently. By breaking matrices into triangular forms, it reduces computational complexity and lays the groundwork for more advanced numerical techniques. Always remember to subtract row operations consistently to avoid sign errors and ensure correct results."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/least-squares.html",
    "href": "mathematics/numerical-analysis/linear-systems/least-squares.html",
    "title": "Least Squares Solution for Inconsistent Systems",
    "section": "",
    "text": "The least squares solution minimizes the distance between the vector \\mathbf{b} and the column space of A. The column space of A, denoted \\mathrm{Col}(A), is equivalent to the span of the columns of A, written as \\mathrm{span}(A_1, A_2, \\dots, A_n), where A_1, A_2, \\dots, A_n are the columns of A.\nThis distance corresponds to the residual \\|\\mathbf{b} - \\mathbf{\\hat{b}}\\|, where \\mathbf{\\hat{b}} = A\\mathbf{\\hat{x}} is the projection of \\mathbf{b} onto \\mathrm{Col}(A)."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/least-squares.html#overview",
    "href": "mathematics/numerical-analysis/linear-systems/least-squares.html#overview",
    "title": "Least Squares Solution for Inconsistent Systems",
    "section": "",
    "text": "The least squares solution minimizes the distance between the vector \\mathbf{b} and the column space of A. The column space of A, denoted \\mathrm{Col}(A), is equivalent to the span of the columns of A, written as \\mathrm{span}(A_1, A_2, \\dots, A_n), where A_1, A_2, \\dots, A_n are the columns of A.\nThis distance corresponds to the residual \\|\\mathbf{b} - \\mathbf{\\hat{b}}\\|, where \\mathbf{\\hat{b}} = A\\mathbf{\\hat{x}} is the projection of \\mathbf{b} onto \\mathrm{Col}(A)."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/least-squares.html#visualizing-the-least-squares-solution",
    "href": "mathematics/numerical-analysis/linear-systems/least-squares.html#visualizing-the-least-squares-solution",
    "title": "Least Squares Solution for Inconsistent Systems",
    "section": "Visualizing the Least Squares Solution",
    "text": "Visualizing the Least Squares Solution\nThe figure below illustrates how the least squares solution \\mathbf{\\hat{x}} projects \\mathbf{b} onto \\mathrm{Col}(A), which is spanned by the columns of A. The vector \\mathbf{r} = \\mathbf{b} - \\mathbf{\\hat{b}} is orthogonal to \\mathrm{Col}(A).\n\n\n\\mathbf{b}: The target vector outside \\mathrm{Col}(A).\n\\mathbf{\\hat{b}}: The projection of \\mathbf{b} onto \\mathrm{Col}(A), calculated as \\mathbf{\\hat{b}} = A(A^\\top A)^{-1}A^\\top \\mathbf{b}.\n\\mathbf{r}: The residual vector, orthogonal to \\mathrm{Col}(A).\nA_1, A_2, \\dots, A_n: Basis vectors spanning \\mathrm{Col}(A)."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/least-squares.html#intuitive-walkthrough-of-the-proof",
    "href": "mathematics/numerical-analysis/linear-systems/least-squares.html#intuitive-walkthrough-of-the-proof",
    "title": "Least Squares Solution for Inconsistent Systems",
    "section": "Intuitive Walkthrough of the Proof",
    "text": "Intuitive Walkthrough of the Proof\n\n1. The Problem\nGiven a matrix A with columns A_1, A_2, \\dots, A_n and a vector \\mathbf{b}, the system A\\mathbf{x} = \\mathbf{b} is inconsistent if \\mathbf{b} does not lie in \\mathrm{Col}(A). The objective is to find \\mathbf{\\hat{x}}, which minimizes the distance between \\mathbf{b} and \\mathrm{Col}(A).\n\n\n2. Projection and Residual\nMinimizing the distance involves projecting \\mathbf{b} onto \\mathrm{Col}(A). The projection is denoted \\mathbf{\\hat{b}} = A\\mathbf{\\hat{x}}. The residual \\mathbf{r} = \\mathbf{b} - \\mathbf{\\hat{b}} represents the difference, and the goal is to minimize \\|\\mathbf{r}\\|^2, which can be written as:\n\n\\|\\mathbf{r}\\|^2 = \\|\\mathbf{b} - A\\mathbf{\\hat{x}}\\|^2.\n\n\n\n3. Orthogonality Condition\nThe residual \\mathbf{r} is orthogonal to \\mathrm{Col}(A) when \\mathbf{\\hat{b}} is the best approximation of \\mathbf{b}. This condition is expressed as:\n\nA^\\top \\mathbf{r} = 0 \\quad \\text{or equivalently} \\quad A^\\top (\\mathbf{b} - A\\mathbf{\\hat{x}}) = 0.\n\n\n\n4. Normal Equations\nExpanding the orthogonality condition yields:\n\nA^\\top \\mathbf{b} - A^\\top A\\mathbf{\\hat{x}} = 0 \\quad \\Rightarrow \\quad A^\\top A\\mathbf{\\hat{x}} = A^\\top \\mathbf{b}.\n\nThis is the normal equation. Solving it provides \\mathbf{\\hat{x}}, the least squares solution.\n\n\n5. Computing \\mathbf{\\hat{x}}\nIf A^\\top A is invertible, the solution for \\mathbf{\\hat{x}} is:\n\n\\mathbf{\\hat{x}} = (A^\\top A)^{-1}A^\\top \\mathbf{b}.\n\n\n\n6. Geometric Interpretation\nThe projection \\mathbf{\\hat{b}} lies in \\mathrm{Col}(A), and the residual \\mathbf{r} = \\mathbf{b} - \\mathbf{\\hat{b}} is orthogonal to this space. This makes \\mathbf{\\hat{b}} the closest vector in \\mathrm{Col}(A) to \\mathbf{b}."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/least-squares.html#summary",
    "href": "mathematics/numerical-analysis/linear-systems/least-squares.html#summary",
    "title": "Least Squares Solution for Inconsistent Systems",
    "section": "Summary",
    "text": "Summary\nThe least squares method projects \\mathbf{b} onto \\mathrm{Col}(A), solving the equation:\n\nA^\\top A\\mathbf{\\hat{x}} = A^\\top \\mathbf{b}.\n\nThe solution is:\n\n\\mathbf{\\hat{x}} = (A^\\top A)^{-1}A^\\top \\mathbf{b}.\n\nThis projection minimizes the residual \\|\\mathbf{b} - \\mathbf{\\hat{b}}\\|, making it the best approximation of \\mathbf{b} within \\mathrm{Col}(A). The accompanying diagram visually demonstrates this projection and the orthogonal residual."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/gram-schmidt-orthogonalization.html",
    "href": "mathematics/numerical-analysis/linear-systems/gram-schmidt-orthogonalization.html",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "",
    "text": "The Gram-Schmidt Orthogonalization process is a fundamental technique in linear algebra for transforming a set of linearly independent vectors into an orthogonal (or orthonormal) set that spans the same subspace. This method is widely used in applications such as QR factorization, solving least squares problems, and numerical linear algebra. This note explores the definition, properties, computation, and applications of the Gram-Schmidt process."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/gram-schmidt-orthogonalization.html#overview",
    "href": "mathematics/numerical-analysis/linear-systems/gram-schmidt-orthogonalization.html#overview",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "",
    "text": "The Gram-Schmidt Orthogonalization process is a fundamental technique in linear algebra for transforming a set of linearly independent vectors into an orthogonal (or orthonormal) set that spans the same subspace. This method is widely used in applications such as QR factorization, solving least squares problems, and numerical linear algebra. This note explores the definition, properties, computation, and applications of the Gram-Schmidt process."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/gram-schmidt-orthogonalization.html#definition-and-process",
    "href": "mathematics/numerical-analysis/linear-systems/gram-schmidt-orthogonalization.html#definition-and-process",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "Definition and Process",
    "text": "Definition and Process\nGiven a set of linearly independent vectors A_1, A_2, \\dots, A_n, the Gram-Schmidt process produces an orthogonal (or orthonormal) set of vectors q_1, q_2, \\dots, q_n such that:\n\nEach vector q_i is orthogonal to the previous vectors q_1, q_2, \\dots, q_{i-1}.\nThe span of q_1, q_2, \\dots, q_n is the same as the span of A_1, A_2, \\dots, A_n.\n\nThis orthogonal set can also be normalized to create an orthonormal basis."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/gram-schmidt-orthogonalization.html#steps-of-the-gram-schmidt-process",
    "href": "mathematics/numerical-analysis/linear-systems/gram-schmidt-orthogonalization.html#steps-of-the-gram-schmidt-process",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "Steps of the Gram-Schmidt Process",
    "text": "Steps of the Gram-Schmidt Process\nThe Gram-Schmidt process involves the following steps:\n\nInitialize with the First Vector: Set q_1 as the normalized version of A_1:\n\nq_1 = \\frac{A_1}{\\|A_1\\|}\n\nCompute Subsequent Vectors: For each vector A_i, construct a new vector u_i by subtracting components that align with previously computed q-vectors. Normalize u_i to obtain q_i:\n\nDefine the non-normalized vector u_i: \nu_i = A_i - \\sum_{j=1}^{i-1} (q_j \\cdot A_i) \\, q_j\n\nNormalize u_i to obtain q_i: \nq_i = \\frac{u_i}{\\|u_i\\|}\n\n\n\nEach vector q_i is thus orthogonal to the preceding q-vectors and has unit length if normalized."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/gram-schmidt-orthogonalization.html#example",
    "href": "mathematics/numerical-analysis/linear-systems/gram-schmidt-orthogonalization.html#example",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "Example",
    "text": "Example\nGiven vectors A_1 and A_2, the Gram-Schmidt process works as follows:\n\nCalculate q_1:\n\nq_1 = \\frac{A_1}{\\|A_1\\|}\n\nCalculate q_2:\n\nFirst, remove the component of A_2 in the direction of q_1 to obtain u_2: \nu_2 = A_2 - (q_1 \\cdot A_2) q_1\n\nThen, normalize u_2 to get q_2: \nq_2 = \\frac{u_2}{\\|u_2\\|}"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/gram-schmidt-orthogonalization.html#properties-of-gram-schmidt-orthogonalization",
    "href": "mathematics/numerical-analysis/linear-systems/gram-schmidt-orthogonalization.html#properties-of-gram-schmidt-orthogonalization",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "Properties of Gram-Schmidt Orthogonalization",
    "text": "Properties of Gram-Schmidt Orthogonalization\n\nOrthogonality: Each vector q_i is orthogonal to all previously generated vectors q_1, \\dots, q_{i-1}.\nSpan Preservation: The set \\{q_1, q_2, \\dots, q_n\\} spans the same subspace as the original set \\{A_1, A_2, \\dots, A_n\\}.\nOrthonormal Basis: By normalizing each u_i to get q_i, the process yields an orthonormal basis for the subspace."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/gram-schmidt-orthogonalization.html#applications-of-gram-schmidt-orthogonalization",
    "href": "mathematics/numerical-analysis/linear-systems/gram-schmidt-orthogonalization.html#applications-of-gram-schmidt-orthogonalization",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "Applications of Gram-Schmidt Orthogonalization",
    "text": "Applications of Gram-Schmidt Orthogonalization\nGram-Schmidt orthogonalization is widely applied in various fields due to its utility in creating orthogonal bases:\n\nQR Factorization: Used to decompose a matrix into an orthogonal matrix Q and an upper triangular matrix R.\nLeast Squares Problems: Assists in minimizing the error in fitting data to a model by creating orthogonal projections.\nSignal Processing and Data Compression: Forms the foundation for methods that reduce redundancy by representing data in orthogonal bases.\nMachine Learning and Statistics: Simplifies computations by projecting data onto orthogonal components."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/gram-schmidt-orthogonalization.html#example-problem",
    "href": "mathematics/numerical-analysis/linear-systems/gram-schmidt-orthogonalization.html#example-problem",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "Example Problem",
    "text": "Example Problem\nProblem: Given the vectors\n\nA_1 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}, \\quad A_2 = \\begin{pmatrix} 4 \\\\ 5 \\\\ 6 \\end{pmatrix}\n\n\nUse Gram-Schmidt to find orthogonal vectors q_1 and q_2.\nNormalize q_1 and q_2 to form an orthonormal basis.\n\n\nSolution Steps\n\nCompute q_1 by normalizing A_1.\nCalculate u_2 by removing the component of A_2 in the direction of q_1.\nNormalize u_2 to obtain q_2."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/gram-schmidt-orthogonalization.html#conclusion",
    "href": "mathematics/numerical-analysis/linear-systems/gram-schmidt-orthogonalization.html#conclusion",
    "title": "Gram-Schmidt Orthogonalization",
    "section": "Conclusion",
    "text": "Conclusion\nThe Gram-Schmidt process is a valuable tool in linear algebra for constructing orthogonal (or orthonormal) bases. By transforming a set of linearly independent vectors, it simplifies many matrix operations and lays the groundwork for QR factorization, data projections, and error minimization in least squares problems."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/residual-linear-systems.html",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/residual-linear-systems.html",
    "title": "Residual in Linear Systems",
    "section": "",
    "text": "The residual is a fundamental concept in numerical linear algebra, used to quantify how far an approximate solution \\mathbf{x_a} to a linear system A\\mathbf{x} = \\mathbf{b} is from satisfying the system. It provides a direct measure of the “error” in the system when the computed solution \\mathbf{x_a} is substituted back into the equation.\nThe residual is defined as:\n\n\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a}\n\nwhere:\n\n\\mathbf{b}: The right-hand side vector of the system.\nA\\mathbf{x_a}: The result of substituting the approximate solution \\mathbf{x_a} into the system.\n\nIf \\mathbf{r} = \\mathbf{0}, the approximate solution is exact; otherwise, \\mathbf{r} quantifies the degree of error."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/residual-linear-systems.html#overview",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/residual-linear-systems.html#overview",
    "title": "Residual in Linear Systems",
    "section": "",
    "text": "The residual is a fundamental concept in numerical linear algebra, used to quantify how far an approximate solution \\mathbf{x_a} to a linear system A\\mathbf{x} = \\mathbf{b} is from satisfying the system. It provides a direct measure of the “error” in the system when the computed solution \\mathbf{x_a} is substituted back into the equation.\nThe residual is defined as:\n\n\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a}\n\nwhere:\n\n\\mathbf{b}: The right-hand side vector of the system.\nA\\mathbf{x_a}: The result of substituting the approximate solution \\mathbf{x_a} into the system.\n\nIf \\mathbf{r} = \\mathbf{0}, the approximate solution is exact; otherwise, \\mathbf{r} quantifies the degree of error."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/residual-linear-systems.html#what-the-residual-represents",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/residual-linear-systems.html#what-the-residual-represents",
    "title": "Residual in Linear Systems",
    "section": "What the Residual Represents",
    "text": "What the Residual Represents\nThe residual measures how far \\mathbf{x_a} is from satisfying the system A\\mathbf{x} = \\mathbf{b}. Each component of \\mathbf{r} indicates the mismatch for the corresponding equation in the system.\n\nKey Points\n\nResidual as a Vector:\n\nThe residual \\mathbf{r} is a vector with the same dimensions as \\mathbf{b}.\nEach entry r_i measures the difference between b_i and the corresponding value of (A\\mathbf{x_a})_i.\n\nExact Solution:\n\nIf \\mathbf{x_a} = \\mathbf{x} (the exact solution), then: \n\\mathbf{r} = \\mathbf{0}\n\n\nApproximate Solution:\n\nFor an approximate solution \\mathbf{x_a}, the residual \\mathbf{r} \\neq \\mathbf{0}."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/residual-linear-systems.html#norm-of-the-residual",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/residual-linear-systems.html#norm-of-the-residual",
    "title": "Residual in Linear Systems",
    "section": "Norm of the Residual",
    "text": "Norm of the Residual\nThe size of the residual can be measured using norms, such as the infinity norm:\n\n\\|\\mathbf{r}\\|_\\infty = \\max_{i} |r_i|\n\nThis provides a scalar measure of the largest discrepancy in the system. A smaller residual norm indicates that \\mathbf{x_a} is closer to satisfying the system."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/residual-linear-systems.html#example",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/residual-linear-systems.html#example",
    "title": "Residual in Linear Systems",
    "section": "Example",
    "text": "Example\nConsider the system:\n\nA = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}, \\quad\n\\mathbf{x_a} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n\n\nStep 1: Compute A\\mathbf{x_a}\nMultiply A by \\mathbf{x_a}:\n\nA\\mathbf{x_a} = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}\n\n\n\nStep 2: Compute the Residual \\mathbf{r}\nSubtract A\\mathbf{x_a} from \\mathbf{b}:\n\n\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n\nThe residual is:\n\n\\mathbf{r} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n\n\n\nStep 3: Compute the Residual Norm\nThe infinity norm of \\mathbf{r} is:\n\n\\|\\mathbf{r}\\|_\\infty = \\max(|1|, |3|) = 3\n\nThis indicates that the largest mismatch in the system is 3."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/residual-linear-systems.html#applications-of-the-residual",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/residual-linear-systems.html#applications-of-the-residual",
    "title": "Residual in Linear Systems",
    "section": "Applications of the Residual",
    "text": "Applications of the Residual\n\nError Analysis:\n\nThe residual is used to assess the accuracy of an approximate solution.\n\nIterative Methods:\n\nResiduals are central to iterative solvers, such as the Jacobi and Gauss-Seidel methods, to track convergence.\n\nNumerical Stability:\n\nA large residual often indicates instability or poor conditioning in the matrix A.\n\nRefining Solutions:\n\nResidual-based refinement techniques iteratively adjust \\mathbf{x_a} to minimize \\|\\mathbf{r}\\|."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/residual-linear-systems.html#conclusion",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/residual-linear-systems.html#conclusion",
    "title": "Residual in Linear Systems",
    "section": "Conclusion",
    "text": "Conclusion\nThe residual \\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a} is a key tool in numerical linear algebra for evaluating the accuracy of an approximate solution. By analyzing the residual and its norm, we can diagnose errors, refine solutions, and ensure stability in solving linear systems."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-backward-error.html",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-backward-error.html",
    "title": "Relative Backward Error in Linear Systems",
    "section": "",
    "text": "The relative backward error is a normalized version of the backward error. It measures how large the residual vector \\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a} is relative to the size of the right-hand side vector \\mathbf{b}. This normalization ensures the backward error is interpreted in the context of the magnitude of the original problem.\nThe relative backward error is defined as:\n\n\\text{RBE} = \\frac{\\|\\mathbf{r}\\|_\\infty}{\\|\\mathbf{b}\\|_\\infty}\n\nwhere:\n\n\\|\\mathbf{r}\\|_\\infty: The infinity norm of the residual vector, \\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a},\n\\|\\mathbf{b}\\|_\\infty: The infinity norm of the right-hand side vector \\mathbf{b}."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-backward-error.html#overview",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-backward-error.html#overview",
    "title": "Relative Backward Error in Linear Systems",
    "section": "",
    "text": "The relative backward error is a normalized version of the backward error. It measures how large the residual vector \\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a} is relative to the size of the right-hand side vector \\mathbf{b}. This normalization ensures the backward error is interpreted in the context of the magnitude of the original problem.\nThe relative backward error is defined as:\n\n\\text{RBE} = \\frac{\\|\\mathbf{r}\\|_\\infty}{\\|\\mathbf{b}\\|_\\infty}\n\nwhere:\n\n\\|\\mathbf{r}\\|_\\infty: The infinity norm of the residual vector, \\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a},\n\\|\\mathbf{b}\\|_\\infty: The infinity norm of the right-hand side vector \\mathbf{b}."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-backward-error.html#what-relative-backward-error-represents",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-backward-error.html#what-relative-backward-error-represents",
    "title": "Relative Backward Error in Linear Systems",
    "section": "What Relative Backward Error Represents",
    "text": "What Relative Backward Error Represents\n\nScale-Invariant Error:\n\nBy dividing the backward error by \\|\\mathbf{b}\\|_\\infty, the relative backward error accounts for the scale of \\mathbf{b}. This is useful when comparing systems with different magnitudes of \\mathbf{b}.\n\nExact Solution:\n\nIf \\mathbf{x_a} is the exact solution, then the residual \\mathbf{r} = \\mathbf{0}, and:\n\n\\text{RBE} = 0\n\n\nError Normalization:\n\nA small relative backward error indicates that the residual is negligible compared to the size of \\mathbf{b}, suggesting a high-quality solution."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-backward-error.html#why-relative-backward-error-matters",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-backward-error.html#why-relative-backward-error-matters",
    "title": "Relative Backward Error in Linear Systems",
    "section": "Why Relative Backward Error Matters",
    "text": "Why Relative Backward Error Matters\n\nAssessing Solution Quality:\n\nThe relative backward error is a scale-invariant metric, making it easier to compare errors across systems of different sizes.\n\nNumerical Stability:\n\nA small relative backward error ensures that the approximate solution \\mathbf{x_a} satisfies a nearby system with respect to the scale of \\mathbf{b}."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-backward-error.html#example",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-backward-error.html#example",
    "title": "Relative Backward Error in Linear Systems",
    "section": "Example",
    "text": "Example\nConsider the system:\n\nA = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}, \\quad\n\\mathbf{x_a} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n\n\nStep 1: Compute A\\mathbf{x_a}\nMultiply A by \\mathbf{x_a}:\n\nA\\mathbf{x_a} = \\begin{bmatrix} 1 \\times 1 + 1 \\times 1 \\\\ 3 \\times 1 + (-4) \\times 1 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}\n\n\n\nStep 2: Compute the Residual \\mathbf{r}\nSubtract A\\mathbf{x_a} from \\mathbf{b}:\n\n\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a} = \\begin{bmatrix} 3 - 2 \\\\ 2 - (-1) \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n\n\n\nStep 3: Compute the Relative Backward Error\n\nCompute the infinity norms:\n\n\\|\\mathbf{r}\\|_\\infty = \\max(|1|, |3|) = 3,\n\\|\\mathbf{b}\\|_\\infty = \\max(|3|, |2|) = 3.\n\nCalculate the relative backward error:\n\n\\text{RBE} = \\frac{\\|\\mathbf{r}\\|_\\infty}{\\|\\mathbf{b}\\|_\\infty} = \\frac{3}{3} = 1\n\n\n\n\nStep 4: Interpretation\n\nA relative backward error of 1 indicates that the residual \\mathbf{r} is as large as the largest component of \\mathbf{b}."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-backward-error.html#conclusion",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-backward-error.html#conclusion",
    "title": "Relative Backward Error in Linear Systems",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe relative backward error provides a normalized measure of how much the right-hand side \\mathbf{b} must be perturbed for \\mathbf{x_a} to satisfy the system exactly.\nBy comparing the size of \\mathbf{r} to \\mathbf{b}, the relative backward error allows for consistent error analysis across problems of different scales."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/forward-error.html",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/forward-error.html",
    "title": "Forward Error in Linear Systems",
    "section": "",
    "text": "The forward error is a critical concept in numerical linear algebra, measuring the difference between the approximate solution \\mathbf{x_a} and the true solution \\mathbf{x} of a linear system A\\mathbf{x} = \\mathbf{b}. It quantifies how far the computed solution is from the exact solution, providing insight into the accuracy of numerical methods.\nThe forward error is defined as:\n\n\\text{FE} = \\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty\n\nwhere:\n\n\\mathbf{x}: The true solution of the system A\\mathbf{x} = \\mathbf{b},\n\\mathbf{x_a}: The approximate solution,\n\\|\\cdot\\|_\\infty: The infinity norm, measuring the maximum absolute entry of the vector."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/forward-error.html#overview",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/forward-error.html#overview",
    "title": "Forward Error in Linear Systems",
    "section": "",
    "text": "The forward error is a critical concept in numerical linear algebra, measuring the difference between the approximate solution \\mathbf{x_a} and the true solution \\mathbf{x} of a linear system A\\mathbf{x} = \\mathbf{b}. It quantifies how far the computed solution is from the exact solution, providing insight into the accuracy of numerical methods.\nThe forward error is defined as:\n\n\\text{FE} = \\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty\n\nwhere:\n\n\\mathbf{x}: The true solution of the system A\\mathbf{x} = \\mathbf{b},\n\\mathbf{x_a}: The approximate solution,\n\\|\\cdot\\|_\\infty: The infinity norm, measuring the maximum absolute entry of the vector."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/forward-error.html#what-forward-error-represents",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/forward-error.html#what-forward-error-represents",
    "title": "Forward Error in Linear Systems",
    "section": "What Forward Error Represents",
    "text": "What Forward Error Represents\n\nSolution Accuracy:\n\nThe forward error reflects the maximum difference between the components of the true solution and the approximate solution.\n\nComponent-wise Deviation:\n\nIt indicates the largest deviation in any component of the solution vector.\n\nExact Solution:\n\nIf \\mathbf{x_a} is the exact solution, then:\n\n\\text{FE} = 0"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/forward-error.html#why-forward-error-matters",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/forward-error.html#why-forward-error-matters",
    "title": "Forward Error in Linear Systems",
    "section": "Why Forward Error Matters",
    "text": "Why Forward Error Matters\n\nAssessing Solution Quality:\n\nThe forward error directly measures the accuracy of the approximate solution, helping determine how close it is to the true solution.\n\nError Propagation:\n\nUnderstanding the forward error aids in analyzing how errors in computations propagate through the solution process.\n\nNumerical Stability:\n\nA small forward error indicates that the numerical method is producing reliable results."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/forward-error.html#example",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/forward-error.html#example",
    "title": "Forward Error in Linear Systems",
    "section": "Example",
    "text": "Example\nTo enhance the visualization, we’ll adjust the approximate solution \\mathbf{x_a} so that the differences in both components are whole numbers but not equal, providing a clearer depiction of forward error in multiple dimensions.\nConsider the system:\n\nA = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}, \\quad\n\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}, \\quad\n\\mathbf{x_a} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\n\n\nStep 1: Verify the True Solution\nCheck that \\mathbf{x} satisfies A\\mathbf{x} = \\mathbf{b}:\n\nA\\mathbf{x} = \\begin{bmatrix} 1 \\times 2 + 1 \\times 1 \\\\ 3 \\times 2 + (-4) \\times 1 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} = \\mathbf{b}\n\n\n\nStep 2: Compute the Forward Error\nCompute the difference between \\mathbf{x} and \\mathbf{x_a}:\n\n\\mathbf{e} = \\mathbf{x} - \\mathbf{x_a} = \\begin{bmatrix} 2 - 1 \\\\ 1 - (-1) \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\n\nThe forward error is the infinity norm of \\mathbf{e}:\n\n\\text{FE} = \\|\\mathbf{e}\\|_\\infty = \\max(|1|, |2|) = 2\n\n\n\nStep 3: Interpretation\n\nThe maximum deviation between the true and approximate solutions is 2, occurring in the second component.\nThe approximate solution \\mathbf{x_a} differs from \\mathbf{x} by 1 in the x-component and 2 in the y-component."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/forward-error.html#visualization-of-forward-error",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/forward-error.html#visualization-of-forward-error",
    "title": "Forward Error in Linear Systems",
    "section": "Visualization of Forward Error",
    "text": "Visualization of Forward Error\nTo illustrate the forward error, we will visualize the true solution \\mathbf{x} and the approximate solution \\mathbf{x_a}, along with the error vector \\mathbf{e} = \\mathbf{x} - \\mathbf{x_a}. By plotting these vectors, we can see how the approximate solution deviates from the true solution in both components.\n\nBlack Vector (\\mathbf{x}): The true solution vector.\nGray Vector (\\mathbf{x_a}): The approximate solution vector.\nOrange Vector (\\mathbf{e} = \\mathbf{x} - \\mathbf{x_a}): The error vector.\nError Components: Projections of \\mathbf{e} onto the x and y axes.\n\n\n\nShow Code\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx_true = np.array([2, 1])\nx_a = np.array([1, -1])\ne = x_true - x_a\n\nplt.figure(figsize=(12, 8))\norigin = np.zeros(2)\n\nplt.quiver(*origin, *x_true, color='black', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{x}$ (True Solution)', width=0.01, zorder=5)\nplt.quiver(*origin, *x_a, color='gray', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{x}_a$ (Approximate Solution)', width=0.01, zorder=5)\nplt.quiver(*x_a, *e, color='orange', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{e} = \\mathbf{x} - \\mathbf{x}_a$', width=0.01, zorder=5)\nplt.quiver(*x_a, e[0], 0, color='red', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{e_1}$', width=0.01, zorder=5)\nplt.quiver(x_a[0] + e[0], x_a[1], 0, e[1], color='purple', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{e_2}$', width=0.01, zorder=5)\n\nplt.annotate(r'$\\mathbf{x}_a$', (x_a[0], x_a[1]), textcoords=\"offset points\", xytext=(-30, -15), ha='center', color='gray', fontsize=14, zorder=6)\nplt.annotate(r'$\\mathbf{x}$', (x_true[0], x_true[1]), textcoords=\"offset points\", xytext=(0, 10), ha='center', color='black', fontsize=14, zorder=6)\nplt.annotate(r'$\\mathbf{e}$', (x_a[0] + e[0]/2, x_a[1] + e[1]/2), textcoords=\"offset points\", xytext=(15, -15), ha='center', color='orange', fontsize=14, zorder=6)\nplt.annotate(r'$\\mathbf{e_1}$', (x_a[0] + e[0]/2, x_a[1] - 0.2), textcoords=\"offset points\", xytext=(0, -20), ha='center', color='red', fontsize=14, zorder=6)\nplt.annotate(r'$\\mathbf{e_2}$', (x_a[0] + e[0] + 0.1, x_a[1] + e[1]/2), textcoords=\"offset points\", xytext=(20, 0), ha='center', color='purple', fontsize=14, zorder=6)\n\nplt.text(x_a[0] + e[0] + 0.3, x_a[1] + e[1] + 0.3, r'$\\max(|\\mathbf{e_1}|, |\\mathbf{e_2}|) = |\\mathbf{e_2}| = 2$', color='purple', fontsize=14, zorder=6)\n\nplt.plot([x_a[0], x_true[0]], [x_a[1], x_true[1]], color='orange', linewidth=1.5, zorder=4)  # Keep the connecting line below vectors\nplt.xlim(-1, 5)\nplt.ylim(-2, 3)\nplt.axhline(0, color='black', linewidth=0.8, zorder=1)\nplt.axvline(0, color='black', linewidth=0.8, zorder=1)\nplt.grid(color='lightgray', linestyle='--', linewidth=0.7, zorder=0)\nplt.legend(loc='upper left', fontsize=12)\nplt.title('Visualization of Forward Error with Error Components', fontsize=18)\nplt.xlabel('x-axis', fontsize=14)\nplt.ylabel('y-axis', fontsize=14)\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nExplanation of the Visualization\n\nVector Addition:\n\nThe error vector \\mathbf{e} is drawn starting from \\mathbf{x_a} and pointing towards \\mathbf{x}, demonstrating that:\n\n\\mathbf{x_a} + \\mathbf{e} = \\mathbf{x}\n\n\nError Components:\n\nThe error vector \\mathbf{e} is decomposed into its x-component e_1 (red dashed arrow) and y-component e_2 (purple dashed arrow).\nThe components are:\n\ne_1 = 1, \\quad e_2 = 2\n\n\nInfinity Norm Highlighted:\n\nThe maximum absolute component of the error is |e_2| = 2, which is the forward error \\text{FE} = \\|\\mathbf{e}\\|_\\infty.\nThis is highlighted in the graph with a text annotation.\n\nUnderstanding the Forward Error:\n\nBy visualizing the error components, we see that the deviation occurs in both the x and y components.\nThe largest error is in the y-component, which determines the forward error."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/forward-error.html#conclusion",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/forward-error.html#conclusion",
    "title": "Forward Error in Linear Systems",
    "section": "Conclusion",
    "text": "Conclusion\n\nForward Error Significance:\n\nThe forward error provides a direct measure of the accuracy of the approximate solution \\mathbf{x_a} in relation to the true solution \\mathbf{x}.\n\nVisualization Enhancements:\n\nBy adjusting \\mathbf{x_a} to differ by whole numbers in both components, the visualization effectively demonstrates how errors in multiple dimensions contribute to the overall forward error.\nDecomposing the error vector into its components and highlighting the infinity norm offers a clearer understanding of how the forward error is calculated.\n\nPractical Implications:\n\nIn numerical computations, minimizing the forward error is crucial for obtaining accurate solutions."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/condition-number-matrix.html",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/condition-number-matrix.html",
    "title": "Understanding the Condition Number of a Matrix",
    "section": "",
    "text": "In numerical linear algebra, efficiently and accurately solving systems of equations is crucial. The Condition Number of a matrix is a fundamental concept that quantifies the sensitivity of the solution of a system of linear equations to errors in the input data. Understanding the condition number helps in assessing the reliability of numerical computations and in designing stable algorithms.\nThis note focuses on the Condition Number using the Infinity Norm, providing insights into its definition, interpretation, and practical computation."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/condition-number-matrix.html#overview",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/condition-number-matrix.html#overview",
    "title": "Understanding the Condition Number of a Matrix",
    "section": "",
    "text": "In numerical linear algebra, efficiently and accurately solving systems of equations is crucial. The Condition Number of a matrix is a fundamental concept that quantifies the sensitivity of the solution of a system of linear equations to errors in the input data. Understanding the condition number helps in assessing the reliability of numerical computations and in designing stable algorithms.\nThis note focuses on the Condition Number using the Infinity Norm, providing insights into its definition, interpretation, and practical computation."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/condition-number-matrix.html#what-is-the-condition-number",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/condition-number-matrix.html#what-is-the-condition-number",
    "title": "Understanding the Condition Number of a Matrix",
    "section": "What is the Condition Number?",
    "text": "What is the Condition Number?\nThe Condition Number of a matrix A, denoted as \\kappa_\\infty(A), measures how much the output value of a function can change for a small change in the input argument. In the context of linear systems, it indicates how sensitive the solution \\mathbf{x} of A\\mathbf{x} = \\mathbf{b} is to changes or errors in A or \\mathbf{b}.\nMathematically, the condition number using the Infinity Norm is defined as:\n\n\\kappa_\\infty(A) = \\|A\\|_\\infty \\cdot \\|A^{-1}\\|_\\infty\n\nWhere:\n\n\\|A\\|_\\infty is the Infinity Norm of matrix A, defined as the maximum absolute row sum.\n\\|A^{-1}\\|_\\infty is the Infinity Norm of the inverse of matrix A.\n\n\nInfinity Norm (\\|\\cdot\\|_\\infty)\nThe Infinity Norm of a matrix A is calculated as:\n\n\\|A\\|_\\infty = \\max_{1 \\leq i \\leq m} \\sum_{j=1}^{n} |a_{ij}|\n\nWhere a_{ij} are the elements of matrix A, and m and n are the number of rows and columns, respectively."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/condition-number-matrix.html#interpreting-the-condition-number",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/condition-number-matrix.html#interpreting-the-condition-number",
    "title": "Understanding the Condition Number of a Matrix",
    "section": "Interpreting the Condition Number",
    "text": "Interpreting the Condition Number\nThe condition number provides insight into the numerical stability of solving linear systems:\n\nWell-Conditioned Matrix: \\kappa_\\infty(A) is close to 1.\n\nSmall changes in A or \\mathbf{b} lead to small changes in \\mathbf{x}.\nSolutions are reliable and stable.\n\nIll-Conditioned Matrix: \\kappa_\\infty(A) is large (significantly greater than 1).\n\nSmall changes in A or \\mathbf{b} can cause large changes in \\mathbf{x}.\nSolutions are unreliable and sensitive to errors."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/condition-number-matrix.html#why-the-condition-number-matters",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/condition-number-matrix.html#why-the-condition-number-matters",
    "title": "Understanding the Condition Number of a Matrix",
    "section": "Why the Condition Number Matters",
    "text": "Why the Condition Number Matters\nUnderstanding the condition number is essential for several reasons:\n\nError Analysis: It helps predict how errors in data propagate to the solution.\nAlgorithm Selection: Guides the choice of numerical methods that are more stable for certain condition numbers.\nMatrix Inversion: Indicates the feasibility and accuracy of computing the inverse of a matrix.\nOptimization: Plays a role in optimization algorithms where matrix conditioning affects convergence rates."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/condition-number-matrix.html#a-practical-example",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/condition-number-matrix.html#a-practical-example",
    "title": "Understanding the Condition Number of a Matrix",
    "section": "A Practical Example",
    "text": "A Practical Example\nLet’s explore a concrete example to illustrate the concept of the condition number using the Infinity Norm and its implications.\n\nThe Problem Setup\nConsider the system of equations:\n\nA\\mathbf{x} = \\mathbf{b}\n\nWhere:\n\n\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} is the true solution.\n\\mathbf{x_a} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} is the approximate (computed) solution.\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} is the input data.\nA = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix} is the coefficient matrix.\n\n\n\nStep 1: Compute the Infinity Norms\nWe will compute the condition number using the Infinity Norm.\n\nCompute \\|A\\|_\\infty\n\n\\|A\\|_\\infty = \\max \\left\\{ |1| + |1|, \\ |3| + |-4| \\right\\} = \\max \\{ 2, 7 \\} = 7\n\n\n\nCompute \\|A^{-1}\\|_\\infty\nFirst, find the inverse of A:\n\nA^{-1} = \\frac{1}{\\det(A)} \\begin{bmatrix} -4 & -1 \\\\ -3 & 1 \\end{bmatrix}\n\nCompute the determinant:\n\n\\det(A) = (1)(-4) - (1)(3) = -4 - 3 = -7\n\nThus,\n\nA^{-1} = \\frac{1}{-7} \\begin{bmatrix} -4 & -1 \\\\ -3 & 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{4}{7} & \\frac{1}{7} \\\\ \\frac{3}{7} & -\\frac{1}{7} \\end{bmatrix}\n\nNow, compute \\|A^{-1}\\|_\\infty:\n\n\\|A^{-1}\\|_\\infty = \\max \\left\\{ \\left| \\frac{4}{7} \\right| + \\left| \\frac{1}{7} \\right|, \\ \\left| \\frac{3}{7} \\right| + \\left| -\\frac{1}{7} \\right| \\right\\} = \\max \\left\\{ \\frac{5}{7}, \\frac{4}{7} \\right\\} = \\frac{5}{7} \\approx 0.7143\n\n\n\n\nStep 2: Compute the Condition Number \\kappa_\\infty(A)\n\n\\kappa_\\infty(A) = \\|A\\|_\\infty \\cdot \\|A^{-1}\\|_\\infty = 7 \\cdot \\frac{5}{7} = 5\n\n\n\nStep 3: Interpretation\nA condition number \\kappa_\\infty(A) = 5 indicates that the matrix A is moderately well-conditioned. This means that the solution \\mathbf{x} is relatively stable with respect to small perturbations in A or \\mathbf{b}. While some error amplification is possible, it is not excessively large, and the solution can be considered reliable for practical purposes."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/successive-over-relaxation.html",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/successive-over-relaxation.html",
    "title": "The Successive Over-Relaxation (SOR) Method for Solving Linear Systems",
    "section": "",
    "text": "The Successive Over-Relaxation (SOR) Method is an extension of the Gauss-Seidel Method used to solve systems of linear equations. By introducing a relaxation parameter \\omega, the SOR method accelerates convergence or allows fine-tuning of the iterative process.\nThe SOR method is particularly useful when \\omega is chosen appropriately, typically 1 &lt; \\omega &lt; 2 for over-relaxation."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/successive-over-relaxation.html#overview",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/successive-over-relaxation.html#overview",
    "title": "The Successive Over-Relaxation (SOR) Method for Solving Linear Systems",
    "section": "",
    "text": "The Successive Over-Relaxation (SOR) Method is an extension of the Gauss-Seidel Method used to solve systems of linear equations. By introducing a relaxation parameter \\omega, the SOR method accelerates convergence or allows fine-tuning of the iterative process.\nThe SOR method is particularly useful when \\omega is chosen appropriately, typically 1 &lt; \\omega &lt; 2 for over-relaxation."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/successive-over-relaxation.html#the-sor-method",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/successive-over-relaxation.html#the-sor-method",
    "title": "The Successive Over-Relaxation (SOR) Method for Solving Linear Systems",
    "section": "The SOR Method",
    "text": "The SOR Method\nConsider the system:\n\nA\\mathbf{x} = \\mathbf{b}\n\nwhere A is decomposed into:\n\nD: The diagonal components of A,\nL: The strictly lower triangular components of A,\nU: The strictly upper triangular components of A.\n\nThus:\n\nA = D + L + U\n\nThe SOR iterative formula is:\n\n\\mathbf{x}_{k+1} = (\\omega L + D)^{-1} \\left[ (1 - \\omega)D\\mathbf{x}_{k} - \\omega U\\mathbf{x}_{k} \\right] + \\omega (D + \\omega L)^{-1} \\mathbf{b}\n\nfor k = 0, 1, 2, \\dots, where:\n\n\\omega: Relaxation parameter (\\omega = 1 corresponds to the Gauss-Seidel Method)."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/successive-over-relaxation.html#algorithm",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/successive-over-relaxation.html#algorithm",
    "title": "The Successive Over-Relaxation (SOR) Method for Solving Linear Systems",
    "section": "Algorithm",
    "text": "Algorithm\n\nInitial Guess: Start with an initial vector \\mathbf{x}_0.\nIterative Formula: For each iteration k, compute:\n\n\\mathbf{x}_{k+1} = (\\omega L + D)^{-1} \\left[ (1 - \\omega)D\\mathbf{x}_{k} - \\omega U\\mathbf{x}_{k} \\right] + \\omega (D + \\omega L)^{-1} \\mathbf{b}\n\nRelaxation Parameter: Choose \\omega:\n\n\\omega &gt; 1: Over-relaxation (accelerates convergence).\n\\omega = 1: Equivalent to the Gauss-Seidel Method.\n\\omega &lt; 1: Under-relaxation (may be used to stabilize divergence).\n\nConvergence Check: Stop when the norm of the residual \\|\\mathbf{b} - A\\mathbf{x}^{(k)}\\| is sufficiently small."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/successive-over-relaxation.html#example",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/successive-over-relaxation.html#example",
    "title": "The Successive Over-Relaxation (SOR) Method for Solving Linear Systems",
    "section": "Example",
    "text": "Example\n\nSystem of Equations\nConsider the system:\n\n4u + v + w = 7, \\quad u + 3v + w = 8, \\quad u + v + 5w = 6\n\n\n\nStep 1: Decompose A\nDecompose the coefficient matrix A:\n\nA = \\begin{bmatrix}\n4 & 1 & 1 \\\\\n1 & 3 & 1 \\\\\n1 & 1 & 5\n\\end{bmatrix}, \\quad\nD = \\begin{bmatrix}\n4 & 0 & 0 \\\\\n0 & 3 & 0 \\\\\n0 & 0 & 5\n\\end{bmatrix}, \\quad\nL = \\begin{bmatrix}\n0 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n1 & 1 & 0\n\\end{bmatrix}, \\quad\nU = \\begin{bmatrix}\n0 & 1 & 1 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\n\n\nStep 2: Iterative Updates\nUsing \\omega = 1.25 and an initial guess \\mathbf{x}_0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, compute the iterations:\n\nIteration 1 (k = 1):\nSubstitute values into the SOR formula:\n\nu^{(1)} = \\frac{1.25}{4} \\left[ 7 - 0 - 0 \\right] = 2.1875\n\n\nv^{(1)} = \\frac{1.25}{3} \\left[ 8 - 2.1875 - 0 \\right] = 1.9792\n\n\nw^{(1)} = \\frac{1.25}{5} \\left[ 6 - 2.1875 - 1.9792 \\right] = 0.9188\n\n\n\\mathbf{x}^{(1)} = \\begin{bmatrix} 2.1875 \\\\ 1.9792 \\\\ 0.9188 \\end{bmatrix}\n\nIteration 2 (k = 2):\nUsing updated values:\n\nu^{(2)} = \\dots, \\quad v^{(2)} = \\dots, \\quad w^{(2)} = \\dots\n\nContinue substituting until convergence."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/successive-over-relaxation.html#convergence-conditions",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/successive-over-relaxation.html#convergence-conditions",
    "title": "The Successive Over-Relaxation (SOR) Method for Solving Linear Systems",
    "section": "Convergence Conditions",
    "text": "Convergence Conditions\nThe SOR method converges under similar conditions to the Gauss-Seidel Method:\n\nIf A is strictly diagonally dominant, or\nIf A is symmetric positive definite.\n\nAdditionally, convergence depends on the choice of \\omega, with 1 &lt; \\omega &lt; 2 typically achieving the fastest results."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/successive-over-relaxation.html#advantages",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/successive-over-relaxation.html#advantages",
    "title": "The Successive Over-Relaxation (SOR) Method for Solving Linear Systems",
    "section": "Advantages",
    "text": "Advantages\n\nAdjustable Convergence Speed: The relaxation parameter \\omega allows tuning for faster convergence.\nEfficiency: For well-chosen \\omega, fewer iterations are required compared to the Gauss-Seidel Method."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/successive-over-relaxation.html#limitations",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/successive-over-relaxation.html#limitations",
    "title": "The Successive Over-Relaxation (SOR) Method for Solving Linear Systems",
    "section": "Limitations",
    "text": "Limitations\n\nRequires tuning \\omega for optimal performance.\nMay not converge if \\omega is poorly chosen.\nNot inherently parallelizable like the Jacobi Method."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/successive-over-relaxation.html#summary",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/successive-over-relaxation.html#summary",
    "title": "The Successive Over-Relaxation (SOR) Method for Solving Linear Systems",
    "section": "Summary",
    "text": "Summary\nThe Successive Over-Relaxation (SOR) Method improves upon the Gauss-Seidel Method by introducing a relaxation parameter \\omega, enabling faster convergence for well-conditioned systems. However, the method requires careful parameter selection and is sensitive to the properties of the system matrix A."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/index.html",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/index.html",
    "title": "ITERATIVE METHODS FOR SOLVING LINEAR SYSTEMS",
    "section": "",
    "text": "Jacobi Method\nGauss-Seidel Method\nSuccessive Over-Relaxation (SOR)"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/2-3.html",
    "href": "mathematics/numerical-analysis/linear-systems/2-3.html",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "",
    "text": "When solving linear systems of the form A\\mathbf{x} = \\mathbf{b}, two significant issues may arise:\n\nControllable Errors: Errors due to computational methods, which we can manage or minimize.\nUncontrollable Errors: Errors inherent to the problem’s nature, which we cannot eliminate but need to understand.\n\nThis document explores vector and matrix norms, their importance in numerical computations, and how they relate to error analysis and condition numbers when solving A\\mathbf{x} = \\mathbf{b}."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/2-3.html#overview",
    "href": "mathematics/numerical-analysis/linear-systems/2-3.html#overview",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "",
    "text": "When solving linear systems of the form A\\mathbf{x} = \\mathbf{b}, two significant issues may arise:\n\nControllable Errors: Errors due to computational methods, which we can manage or minimize.\nUncontrollable Errors: Errors inherent to the problem’s nature, which we cannot eliminate but need to understand.\n\nThis document explores vector and matrix norms, their importance in numerical computations, and how they relate to error analysis and condition numbers when solving A\\mathbf{x} = \\mathbf{b}."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/2-3.html#vector-norms",
    "href": "mathematics/numerical-analysis/linear-systems/2-3.html#vector-norms",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Vector Norms",
    "text": "Vector Norms\nA norm is a function that assigns a non-negative length or size to vectors in a vector space. Norms help measure the magnitude of vectors, which is essential in analyzing algorithms and numerical stability.\nFor a vector \\mathbf{v} = [v_1, v_2, \\dots, v_n]^T, common norms include:\n\n1. \\ell_2-Norm (Euclidean Norm)\nMeasures the straight-line distance from the origin to the point \\mathbf{v}:\n\n\\|\\mathbf{v}\\|_2 = \\sqrt{v_1^2 + v_2^2 + \\dots + v_n^2}\n\n\n\n2. \\ell_1-Norm (Taxicab or Manhattan Norm)\nSums the absolute values of the vector components:\n\n\\|\\mathbf{v}\\|_1 = |v_1| + |v_2| + \\dots + |v_n|\n\n\n\n3. \\ell_\\infty-Norm (Maximum or Infinity Norm)\nTakes the maximum absolute value among the components:\n\n\\|\\mathbf{v}\\|_\\infty = \\max_{1 \\leq i \\leq n} |v_i|\n\nAlso expressed as:\n\n\\|\\mathbf{v}\\|_\\infty = \\lim_{p \\to \\infty} \\left( \\sum_{i=1}^n |v_i|^p \\right)^{1/p}."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/2-3.html#matrix-norms",
    "href": "mathematics/numerical-analysis/linear-systems/2-3.html#matrix-norms",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Matrix Norms",
    "text": "Matrix Norms\nMatrix norms measure the “size” of matrices and help understand how matrices affect vector norms when used as linear transformations.\nCommon matrix norms include:\n\n1. Spectral Norm (\\|A\\|_2)\nInvolves the largest singular value of A:\n\n\\|A\\|_2 = \\sqrt{\\lambda_{\\text{max}}(A^TA)},\n\nwhere \\lambda_{\\text{max}} is the largest eigenvalue of A^TA.\n\n\n2. Maximum Column Sum Norm (\\|A\\|_1)\nThe largest sum of absolute values in any column:\n\n\\|A\\|_1 = \\max_j \\sum_i |a_{ij}|\n\n\n\n3. Maximum Row Sum Norm (\\|A\\|_\\infty)\nThe largest sum of absolute values in any row:\n\n\\|A\\|_\\infty = \\max_i \\sum_j |a_{ij}|"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/2-3.html#the-matrix-infinity-norm-and-row-sums",
    "href": "mathematics/numerical-analysis/linear-systems/2-3.html#the-matrix-infinity-norm-and-row-sums",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "The Matrix Infinity Norm and Row Sums",
    "text": "The Matrix Infinity Norm and Row Sums\n\nDefinition of \\|A\\|_\\infty\nFormally defined as:\n\n\\|A\\|_\\infty = \\sup_{\\mathbf{x} \\neq 0} \\frac{\\|A\\mathbf{x}\\|_\\infty}{\\|\\mathbf{x}\\|_\\infty} = \\sup_{\\|\\mathbf{x}\\|_\\infty = 1} \\|A\\mathbf{x}\\|_\\infty\n\n\n\nKey Insight\n\nThe infinity norm of A is equal to the maximum row sum of A.\nIt represents the maximum effect A can have on any vector \\mathbf{x} with \\|\\mathbf{x}\\|_\\infty = 1."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/2-3.html#proof-a_infty-maximum-row-sum-of-a",
    "href": "mathematics/numerical-analysis/linear-systems/2-3.html#proof-a_infty-maximum-row-sum-of-a",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Proof: \\|A\\|_\\infty = Maximum Row Sum of A",
    "text": "Proof: \\|A\\|_\\infty = Maximum Row Sum of A\nFor matrix A:\n\n\\|A\\|_\\infty = \\max_i \\sum_j |a_{ij}|.\n\n\nExample\nConsider:\n\nA = \\begin{bmatrix}\n-1 & -2 & 3 \\\\\n3 & -4 & -5 \\\\\n-2 & 3 & -4\n\\end{bmatrix}\n\n\nCompute Row Sums\n\nRow 1: |-1| + |-2| + |3| = 6\nRow 2: |3| + |-4| + |-5| = 12\nRow 3: |-2| + |3| + |-4| = 9\n\nMaximum Row Sum: 12 (Row 2)\nTherefore, \\|A\\|_\\infty = 12.\n\n\n\nChoosing \\mathbf{\\hat{x}}\nTo achieve \\|A\\mathbf{\\hat{x}}\\|_\\infty = \\|A\\|_\\infty, select \\mathbf{\\hat{x}} with \\|\\mathbf{\\hat{x}}\\|_\\infty = 1 that aligns with the signs of Row 2:\n\n\\mathbf{\\hat{x}} = \\begin{bmatrix}\n1 \\\\\n-1 \\\\\n-1\n\\end{bmatrix}\n\n\nCompute A\\mathbf{\\hat{x}}:\n\nA\\mathbf{\\hat{x}} = \\begin{bmatrix}\n-1(1) + -2(-1) + 3(-1) \\\\\n3(1) + -4(-1) + -5(-1) \\\\\n-2(1) + 3(-1) + -4(-1)\n\\end{bmatrix} = \\begin{bmatrix}\n-2 \\\\\n12 \\\\\n-1\n\\end{bmatrix}\n\n\n\nCompute \\|A\\mathbf{\\hat{x}}\\|_\\infty:\n\n\\|A\\mathbf{\\hat{x}}\\|_\\infty = \\max(|-2|, |12|, |-1|) = 12 = \\|A\\|_\\infty\n\n\n\n\nConclusion\nIt is not possible to find \\mathbf{\\hat{x}} such that \\|A\\mathbf{\\hat{x}}\\|_\\infty &gt; \\|A\\|_\\infty when \\|\\mathbf{\\hat{x}}\\|_\\infty = 1, because \\|A\\|_\\infty is the supremum of \\|A\\mathbf{x}\\|_\\infty over all such \\mathbf{x}."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/2-3.html#error-analysis-and-condition-numbers",
    "href": "mathematics/numerical-analysis/linear-systems/2-3.html#error-analysis-and-condition-numbers",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Error Analysis and Condition Numbers",
    "text": "Error Analysis and Condition Numbers\n\nWhy Care About Errors in A\\mathbf{x} = \\mathbf{b}?\nWhen solving A\\mathbf{x} = \\mathbf{b}, understanding errors helps improve numerical accuracy and stability.\nLet \\mathbf{x_a} be an approximate solution to A\\mathbf{x} = \\mathbf{b}, meaning:\n\nA\\mathbf{x_a} \\neq \\mathbf{b}\n\nWe quantify errors to assess the accuracy of \\mathbf{x_a} and evaluate the impact of approximations.\n\n\nDefinitions\n\nResidual: The difference between \\mathbf{b} and A\\mathbf{x_a}:\n\n\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a}\n\nBackward Error (BE): Measures the infinity norm of the residual:\n\n\\text{BE} = \\|\\mathbf{r}\\|_\\infty = \\|\\mathbf{b} - A\\mathbf{x_a}\\|_\\infty\n\nRelative Backward Error (RBE): Normalizes the backward error relative to \\mathbf{b}:\n\n\\text{RBE} = \\frac{\\|\\mathbf{r}\\|_\\infty}{\\|\\mathbf{b}\\|_\\infty}\n\nForward Error (FE): Measures the difference between the true solution \\mathbf{x} and the approximate solution \\mathbf{x_a}:\n\n\\text{FE} = \\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty\n\nRelative Forward Error (RFE): Normalizes the forward error relative to \\mathbf{x}:\n\n\\text{RFE} = \\frac{\\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty}{\\|x\\|_\\infty}"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/2-3.html#error-magnification-factor-emf",
    "href": "mathematics/numerical-analysis/linear-systems/2-3.html#error-magnification-factor-emf",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Error Magnification Factor (EMF)",
    "text": "Error Magnification Factor (EMF)\nThe error magnification factor (EMF) relates the relative forward error (RFE) to the relative backward error (RBE):\n\n\\text{EMF} = \\frac{\\text{RFE}}{\\text{RBE}}\n\nThis quantifies how much the backward error is amplified when reflected in the forward error."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/2-3.html#condition-number-of-a-matrix",
    "href": "mathematics/numerical-analysis/linear-systems/2-3.html#condition-number-of-a-matrix",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Condition Number of a Matrix",
    "text": "Condition Number of a Matrix\nThe condition number of a matrix A measures the sensitivity of the solution \\mathbf{x} to changes in \\mathbf{b}. It is defined as:\n\n\\text{cond}(A) = \\|A\\|_\\infty \\cdot \\|A^{-1}\\|_\\infty\n\n\nInterpretation:\n\nA low condition number (close to 1) indicates a well-conditioned matrix.\nA high condition number suggests an ill-conditioned matrix, meaning small changes in \\mathbf{b} can result in large changes in \\mathbf{x}."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/2-3.html#example-1",
    "href": "mathematics/numerical-analysis/linear-systems/2-3.html#example-1",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Example",
    "text": "Example\nGiven:\n\n\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}, \\quad\n\\mathbf{x_a} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\quad\nA = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}\n\n\nStep 1: Compute Errors\n\nForward Error (FE):\n\n\\text{FE} = \\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty = \\left\\| \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\right\\|_\\infty = 1\n\nRelative Forward Error (RFE):\n\n\\text{RFE} = \\frac{\\text{FE}}{\\|\\mathbf{x}\\|_\\infty} = \\frac{1}{2} = 0.5\n\nResidual:\n\nr = b - Ax_a = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n\nBackward Error (BE):\n\n\\text{BE} = \\|\\mathbf{r}\\|_\\infty = 3\n\nRelative Backward Error (RBE):\n\n\\text{RBE} = \\frac{\\text{BE}}{\\|\\mathbf{b}\\|_\\infty} = \\frac{3}{3} = 1\n\n\n\n\nStep 2: Compute EMF\nUsing:\n\n\\text{EMF} = \\frac{\\text{RFE}}{\\text{RBE}}\n\nwe find:\n\n\\text{EMF} = \\frac{0.5}{1} = 0.5\n\n\n\nStep 3: Compute Condition Number\n\nCompute \\|A\\|_\\infty:\n\n\\|A\\|_\\infty = \\max\\left( |1| + |1|, |3| + |-4| \\right) = \\max(2, 7) = 7\n\nCompute \\|A^{-1}\\|_\\infty:\nFrom A^{-1}, we find:\n\n\\|A^{-1}\\|_\\infty = \\max\\left( \\frac{4}{7} + \\frac{1}{7}, \\frac{3}{7} + \\frac{1}{7} \\right) = \\frac{5}{7}\n\nCondition Number:\n\n\\text{cond}(A) = \\|A\\|_\\infty \\cdot \\|A^{-1}\\|_\\infty = 7 \\cdot \\frac{5}{7} = 5"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/2-3.html#next-steps-pa-lu-decomposition-partial-pivoting",
    "href": "mathematics/numerical-analysis/linear-systems/2-3.html#next-steps-pa-lu-decomposition-partial-pivoting",
    "title": "Understanding Vector and Matrix Norms, Error Analysis, and Condition Numbers",
    "section": "Next Steps: PA = LU Decomposition (Partial Pivoting)",
    "text": "Next Steps: PA = LU Decomposition (Partial Pivoting)\n\nWhat is Partial Pivoting?\nPartial pivoting rearranges rows of A during LU decomposition to place the largest available pivot element on the diagonal. This ensures numerical stability by reducing rounding errors.\n\n\nConsequences of Partial Pivoting:\n\nControlled Multipliers: Ensures all multipliers satisfy |m_{ij}| \\leq 1.\nPrevents Swamping: Avoids large numerical errors caused by small pivot elements.\n\nUnderstanding norms, errors, and condition numbers is foundational for solving A\\mathbf{x} = \\mathbf{b} efficiently and accurately."
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-3b.html",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-3b.html",
    "title": "Exercise 10.2.3b (C10-P4)",
    "section": "",
    "text": "10.2.3b\n\n\n\nFind the trigonometric interpolating function for the given data:\n\n\n\nt\nx\n\n\n\n\n0\n1\n\n\n\\frac{1}{8}\n2\n\n\n\\frac{1}{4}\n1\n\n\n\\frac{3}{8}\n0\n\n\n\\frac{1}{2}\n1\n\n\n\\frac{5}{8}\n2\n\n\n\\frac{3}{4}\n1\n\n\n\\frac{7}{8}\n0"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-3b.html#corollary-10.8",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-3b.html#corollary-10.8",
    "title": "Exercise 10.2.3b (C10-P4)",
    "section": "Corollary 10.8",
    "text": "Corollary 10.8\nFor an even integer n, let\n\nt_j = c + j \\frac{(d - c)}{n}, \\quad \\text{for } j = 0, \\dots, n - 1,\n\nand let\n\nx = (x_0, \\dots, x_{n-1})\n\ndenote a vector of n real numbers. Define\n\na + b i = F_n x,\n\nwhere F_n is the Discrete Fourier Transform (DFT). Then the function\n\nP_n(t) = \\frac{a_0}{\\sqrt{n}} + \\frac{2}{\\sqrt{n}} \\sum_{k=1}^{\\frac{n}{2} - 1} \\left( a_k \\cos\\left(\\frac{2\\pi k (t - c)}{d - c}\\right) - b_k \\sin\\left(\\frac{2\\pi k (t - c)}{d - c}\\right) \\right)\n+ \\frac{a_{n/2}}{\\sqrt{n}} \\cos\\left(\\frac{n\\pi (t - c)}{d - c}\\right)\n\ninterpolates the given data, i.e.\n\nP_n(t_j) = x_j, \\quad \\text{for } j = 0, \\dots, n - 1."
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-3b.html#parameters",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-3b.html#parameters",
    "title": "Exercise 10.2.3b (C10-P4)",
    "section": "Parameters:",
    "text": "Parameters:\n\nt = \\left[ 0, \\tfrac{1}{8}, \\tfrac{1}{4}, \\tfrac{3}{8}, \\tfrac{1}{2}, \\tfrac{5}{8}, \\tfrac{3}{4}, \\tfrac{7}{8} \\right]\nx = [1, 2, 1, 0, 1, 2, 1, 0]\nInterval start: c = 0\nInterval end: d = 1\nNumber of data points: n = 8"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-3b.html#dft-computation",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-3b.html#dft-computation",
    "title": "Exercise 10.2.3b (C10-P4)",
    "section": "DFT Computation:",
    "text": "DFT Computation:\nThe DFT of x is given by y = F_8 x, where F_8 is the 8 \\times 8 DFT matrix defined by:\n\n(F_8)_{j,k} = \\frac{1}{\\sqrt{8}} \\omega^{j k}, \\quad \\omega = e^{-i\\frac{2\\pi}{n}}\n\nCompute powers of \\omega:\n\n\\omega^0 = 1\n\\omega^1 = e^{-i\\pi/4} =  \\frac{\\sqrt{2}}{2} - i\\frac{\\sqrt{2}}{2}\n\\omega^2 = e^{-i\\pi/2} = -i\n\\omega^3 = e^{-3i\\pi/4} = -\\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2}\n\\omega^4 = e^{-i\\pi} = -1\n\\omega^5 = e^{-5i\\pi/4} = -\\frac{\\sqrt{2}}{2} + i\\frac{\\sqrt{2}}{2}\n\\omega^6 = e^{-3i\\pi/2} = i\n\\omega^7 = e^{-7i\\pi/4} = \\frac{\\sqrt{2}}{2} + i\\frac{\\sqrt{2}}{2}\n\n\nFourier Matrix F_8\nSubstitute into F_8:\n\nF_8 = \\frac{1}{\\sqrt{8}}\\begin{bmatrix}\n\\omega^{0\\cdot0} & \\omega^{0\\cdot1} & \\omega^{0\\cdot2} & \\omega^{0\\cdot3} & \\omega^{0\\cdot4} & \\omega^{0\\cdot5} & \\omega^{0\\cdot6} & \\omega^{0\\cdot7}\\\\[6pt]\n\\omega^{1\\cdot0} & \\omega^{1\\cdot1} & \\omega^{1\\cdot2} & \\omega^{1\\cdot3} & \\omega^{1\\cdot4} & \\omega^{1\\cdot5} & \\omega^{1\\cdot6} & \\omega^{1\\cdot7}\\\\[6pt]\n\\omega^{2\\cdot0} & \\omega^{2\\cdot1} & \\omega^{2\\cdot2} & \\omega^{2\\cdot3} & \\omega^{2\\cdot4} & \\omega^{2\\cdot5} & \\omega^{2\\cdot6} & \\omega^{2\\cdot7}\\\\[6pt]\n\\omega^{3\\cdot0} & \\omega^{3\\cdot1} & \\omega^{3\\cdot2} & \\omega^{3\\cdot3} & \\omega^{3\\cdot4} & \\omega^{3\\cdot5} & \\omega^{3\\cdot6} & \\omega^{3\\cdot7}\\\\[6pt]\n\\omega^{4\\cdot0} & \\omega^{4\\cdot1} & \\omega^{4\\cdot2} & \\omega^{4\\cdot3} & \\omega^{4\\cdot4} & \\omega^{4\\cdot5} & \\omega^{4\\cdot6} & \\omega^{4\\cdot7}\\\\[6pt]\n\\omega^{5\\cdot0} & \\omega^{5\\cdot1} & \\omega^{5\\cdot2} & \\omega^{5\\cdot3} & \\omega^{5\\cdot4} & \\omega^{5\\cdot5} & \\omega^{5\\cdot6} & \\omega^{5\\cdot7}\\\\[6pt]\n\\omega^{6\\cdot0} & \\omega^{6\\cdot1} & \\omega^{6\\cdot2} & \\omega^{6\\cdot3} & \\omega^{6\\cdot4} & \\omega^{6\\cdot5} & \\omega^{6\\cdot6} & \\omega^{6\\cdot7}\\\\[6pt]\n\\omega^{7\\cdot0} & \\omega^{7\\cdot1} & \\omega^{7\\cdot2} & \\omega^{7\\cdot3} & \\omega^{7\\cdot4} & \\omega^{7\\cdot5} & \\omega^{7\\cdot6} & \\omega^{7\\cdot7}\n\\end{bmatrix}\n\n\n= \\frac{1}{2\\sqrt{2}}\\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n1 & \\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & -i & -\\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & -1 & -\\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & i & \\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} \\\\\n1 & -i & -1 & i & 1 & -i & -1 & i \\\\\n1 & -\\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & i & \\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & -1 & \\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & -i & -\\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} \\\\\n1 & -1 & 1 & -1 & 1 & -1 & 1 & -1 \\\\\n1 & -\\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & -i & \\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & -1 & -\\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & i & -\\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} \\\\\n1 & i & -1 & -i & 1 & i & -1 & -i \\\\\n1 & \\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & i & -\\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & -1 & -\\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & -i & \\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2}\n\\end{bmatrix}\n\n\n\nApply F_8 to x:\n\nF_8 \\mathbf{x}\n= \\frac{1}{2\\sqrt{2}}\\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n1 & \\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & -i & -\\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & -1 & -\\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & i & \\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} \\\\\n1 & -i & -1 & i & 1 & -i & -1 & i \\\\\n1 & -\\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & i & \\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & -1 & \\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & -i & -\\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} \\\\\n1 & -1 & 1 & -1 & 1 & -1 & 1 & -1 \\\\\n1 & -\\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & -i & \\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & -1 & -\\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & i & -\\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} \\\\\n1 & i & -1 & -i & 1 & i & -1 & -i \\\\\n1 & \\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & i & -\\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & -1 & -\\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & -i & \\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2}\n\\end{bmatrix}\n\\begin{bmatrix}1 \\\\ 2 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 2 \\\\ 1 \\\\ 0\\end{bmatrix}\n\n\n= \\begin{bmatrix}2\\sqrt{2} \\\\ 0 \\\\ -\\sqrt{2}i \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\sqrt{2}i \\\\ 0\\end{bmatrix}\n\n\n\nFind a_k and b_k for Each y_k:\nWrite y_k=a_k+b_k i:\n\ny_0 = 2\\sqrt{2} \\implies a_0 = 2\\sqrt{2}, b_0=0\ny_1 = 0 \\implies a_1=0, b_1=0\ny_2 = -\\sqrt{2} i \\implies a_2=0, b_2=-\\sqrt{2}\ny_3=0 \\implies a_3=0,b_3=0\ny_4=0 \\implies a_4=0,b_4=0\ny_5=0 \\implies a_5=0,b_5=0\ny_6=\\sqrt{2} i \\implies a_6=0,b_6=\\sqrt{2}\ny_7=0 \\implies a_7=0,b_7=0"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-3b.html#forming-the-interpolant",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-3b.html#forming-the-interpolant",
    "title": "Exercise 10.2.3b (C10-P4)",
    "section": "Forming the Interpolant:",
    "text": "Forming the Interpolant:\nFrom Corollary 10.8:\n\nP_8(t) = \\frac{a_0}{\\sqrt{8}} + \\frac{2}{\\sqrt{8}}\\sum_{k=1}^3 [a_k\\cos(2\\pi k t)-b_k\\sin(2\\pi k t)] + \\frac{a_4}{\\sqrt{8}}\\cos(8\\pi t)\n\nWe found:\n\na_0=2\\sqrt{2}\nb_2=-\\sqrt{2}\nb_6 = \\sqrt{2}\nAll others a_k,b_k=0\n\nTherefore:\n\nP_8(t) = \\frac{2\\sqrt{2}}{\\sqrt{8}} + \\frac{2}{\\sqrt{8}}\\left[\\left[0\\cos(2\\pi t)-0\\sin(2\\pi t)\\right] + \\left[0\\cos(4\\pi t) - (-\\sqrt{2})\\sin(4\\pi t)\\right] + \\left[0\\cos(6\\pi t)- 0\\sin(6\\pi t)\\right]\\right] + \\frac{0}{\\sqrt{8}}\\cos(8\\pi t)\n\n\n= \\frac{2\\sqrt{2}}{\\sqrt{8}} + \\frac{2}{\\sqrt{8}}\\left[0 + \\sqrt{2}\\sin(4\\pi t)\\right] = 1+\\sin(4\\pi t)"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-3b.html#check-the-result",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-3b.html#check-the-result",
    "title": "Exercise 10.2.3b (C10-P4)",
    "section": "Check the Result:",
    "text": "Check the Result:\n\nAt t=0: P_8(0) = 1 + \\sin(4\\pi \\cdot 0) = 1 + \\sin(0) = 1 + 0 = 1, matches x_0 = 1\nAt t=\\frac{1}{8}: P_8\\left(\\frac{1}{8}\\right) = 1 + \\sin\\left(\\frac{\\pi}{2}\\right) = 1 + 1 = 2, matches x_1 = 2\nAt t=\\frac{1}{4}: P_8\\left(\\frac{1}{4}\\right) = 1 + \\sin(\\pi) = 1 + 0 = 1, matches x_2 = 1.\nAt t=\\frac{3}{8}: P_8\\left(\\frac{3}{8}\\right) = 1 + \\sin\\left(\\frac{3\\pi}{2}\\right) = 1 + (-1) = 0, matches x_3 = 0\nAt t=\\frac{1}{2}: P_8\\left(\\frac{1}{2}\\right) = 1 + \\sin(2\\pi) = 1 + 0 = 1, matches x_4 = 1\nAt t=\\frac{5}{8}: P_8\\left(\\frac{5}{8}\\right) = 1 + \\sin\\left(\\frac{5\\pi}{2}\\right) = 1 + 1 = 2, matches x_5 = 2\nAt t=\\frac{3}{4}:P_8\\left(\\frac{3}{4}\\right) = 1 + \\sin(3\\pi) = 1 + 0 = 1, matches x_6 = 1\nAt t=\\frac{7}{8}: P_8\\left(\\frac{7}{8}\\right) = 1 + \\sin\\left(\\frac{7\\pi}{2}\\right) = 1 + (-1) = 0, matches x_7 = 0\n\n\n\n\n\n\n\nFinal Answer:\n\n\n\nThe trigonometric interpolating polynomial is:\n\nP_8(t) = 1 + \\sin(4\\pi t)"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-3a.html",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-3a.html",
    "title": "Exercise 10.2.3a (C10-P4)",
    "section": "",
    "text": "10.2.3a\n\n\n\nFind the trigonometric interpolating function for the given data:\n\n\n\nt\nx\n\n\n\n\n0\n0\n\n\n\\frac{1}{8}\n1\n\n\n\\frac{1}{4}\n0\n\n\n\\frac{3}{8}\n-1\n\n\n\\frac{1}{2}\n0\n\n\n\\frac{5}{8}\n1\n\n\n\\frac{3}{4}\n0\n\n\n\\frac{7}{8}\n-1"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-3a.html#corollary-10.8",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-3a.html#corollary-10.8",
    "title": "Exercise 10.2.3a (C10-P4)",
    "section": "Corollary 10.8",
    "text": "Corollary 10.8\nFor an even integer n, let\n\nt_j = c + j \\frac{(d - c)}{n}, \\quad \\text{for } j = 0, \\dots, n-1,\n\nand let\n\nx = (x_0, \\dots, x_{n-1})\n\ndenote a vector of n real numbers. Define\n\na + b i = F_n x,\n\nwhere F_n is the Discrete Fourier Transform (DFT). Then the function\n\nP_n(t) = \\frac{a_0}{\\sqrt{n}} + \\frac{2}{\\sqrt{n}} \\sum_{k=1}^{\\frac{n}{2} - 1} \\left( a_k \\cos\\left(\\frac{2\\pi k (t - c)}{d - c}\\right) - b_k \\sin\\left(\\frac{2\\pi k (t - c)}{d - c}\\right) \\right)\n+ \\frac{a_{n/2}}{\\sqrt{n}} \\cos\\left(\\frac{n\\pi (t - c)}{d - c}\\right)\n\ninterpolates the given data, i.e.\n\nP_n(t_j) = x_j, \\quad \\text{for } j = 0, \\dots, n - 1."
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-3a.html#parameters",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-3a.html#parameters",
    "title": "Exercise 10.2.3a (C10-P4)",
    "section": "Parameters:",
    "text": "Parameters:\n\nt = \\left[ 0, \\frac{1}{8}, \\frac{1}{4}, \\frac{3}{8}, \\frac{1}{2}, \\frac{5}{8}, \\frac{3}{4}, \\frac{7}{8} \\right]\nx = [0, 1, 0, -1, 0, 1, 0, -1]\nInterval start: c = 0\nInterval end: d = 1\nNumber of data points: n = 8"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-3a.html#dft-computation",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-3a.html#dft-computation",
    "title": "Exercise 10.2.3a (C10-P4)",
    "section": "DFT Computation:",
    "text": "DFT Computation:\nThe DFT of x is given by y = F_8 x, where F_8 is the 8\\times 8 DFT matrix defined by:\n\n(F_8)_{j,k} = \\frac{1}{\\sqrt{8}} \\omega^{j k}, \\quad \\omega = e^{-i\\frac{2\\pi}{n}}\n\nCompute powers of \\omega:\n\n\\omega^0 = 1\n\\omega^1 = e^{-i\\pi/4} =  \\frac{\\sqrt{2}}{2} - i\\frac{\\sqrt{2}}{2}\n\\omega^2 = e^{-i\\pi/2} = -i\n\\omega^3 = e^{-3i\\pi/4} = -\\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2}\n\\omega^4 = e^{-i\\pi} = -1\n\\omega^5 = e^{-5i\\pi/4} = -\\frac{\\sqrt{2}}{2} + i\\frac{\\sqrt{2}}{2}\n\\omega^6 = e^{-3i\\pi/2} = i\n\\omega^7 = e^{-7i\\pi/4} = \\frac{\\sqrt{2}}{2} + i\\frac{\\sqrt{2}}{2}\n\n\nFourier Matrix F_8\nSubstitute into F_8:\n\nF_8 = \\frac{1}{\\sqrt{8}}\\begin{bmatrix}\n\\omega^{0\\cdot0} & \\omega^{0\\cdot1} & \\omega^{0\\cdot2} & \\omega^{0\\cdot3} & \\omega^{0\\cdot4} & \\omega^{0\\cdot5} & \\omega^{0\\cdot6} & \\omega^{0\\cdot7}\\\\[6pt]\n\\omega^{1\\cdot0} & \\omega^{1\\cdot1} & \\omega^{1\\cdot2} & \\omega^{1\\cdot3} & \\omega^{1\\cdot4} & \\omega^{1\\cdot5} & \\omega^{1\\cdot6} & \\omega^{1\\cdot7}\\\\[6pt]\n\\omega^{2\\cdot0} & \\omega^{2\\cdot1} & \\omega^{2\\cdot2} & \\omega^{2\\cdot3} & \\omega^{2\\cdot4} & \\omega^{2\\cdot5} & \\omega^{2\\cdot6} & \\omega^{2\\cdot7}\\\\[6pt]\n\\omega^{3\\cdot0} & \\omega^{3\\cdot1} & \\omega^{3\\cdot2} & \\omega^{3\\cdot3} & \\omega^{3\\cdot4} & \\omega^{3\\cdot5} & \\omega^{3\\cdot6} & \\omega^{3\\cdot7}\\\\[6pt]\n\\omega^{4\\cdot0} & \\omega^{4\\cdot1} & \\omega^{4\\cdot2} & \\omega^{4\\cdot3} & \\omega^{4\\cdot4} & \\omega^{4\\cdot5} & \\omega^{4\\cdot6} & \\omega^{4\\cdot7}\\\\[6pt]\n\\omega^{5\\cdot0} & \\omega^{5\\cdot1} & \\omega^{5\\cdot2} & \\omega^{5\\cdot3} & \\omega^{5\\cdot4} & \\omega^{5\\cdot5} & \\omega^{5\\cdot6} & \\omega^{5\\cdot7}\\\\[6pt]\n\\omega^{6\\cdot0} & \\omega^{6\\cdot1} & \\omega^{6\\cdot2} & \\omega^{6\\cdot3} & \\omega^{6\\cdot4} & \\omega^{6\\cdot5} & \\omega^{6\\cdot6} & \\omega^{6\\cdot7}\\\\[6pt]\n\\omega^{7\\cdot0} & \\omega^{7\\cdot1} & \\omega^{7\\cdot2} & \\omega^{7\\cdot3} & \\omega^{7\\cdot4} & \\omega^{7\\cdot5} & \\omega^{7\\cdot6} & \\omega^{7\\cdot7}\n\\end{bmatrix}\n\n\n= \\frac{1}{2\\sqrt{2}}\\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n1 & \\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & -i & -\\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & -1 & -\\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & i & \\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} \\\\\n1 & -i & -1 & i & 1 & -i & -1 & i \\\\\n1 & -\\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & i & \\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & -1 & \\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & -i & -\\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} \\\\\n1 & -1 & 1 & -1 & 1 & -1 & 1 & -1 \\\\\n1 & -\\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & -i & \\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & -1 & -\\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & i & -\\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} \\\\\n1 & i & -1 & -i & 1 & i & -1 & -i \\\\\n1 & \\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & i & -\\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & -1 & -\\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & -i & \\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2}\n\\end{bmatrix}\n\n\n\nApply F_8 to x:\n\nF_8 \\mathbf{x}\n= \\frac{1}{2\\sqrt{2}}\\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n1 & \\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & -i & -\\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & -1 & -\\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & i & \\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} \\\\\n1 & -i & -1 & i & 1 & -i & -1 & i \\\\\n1 & -\\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & i & \\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & -1 & \\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & -i & -\\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} \\\\\n1 & -1 & 1 & -1 & 1 & -1 & 1 & -1 \\\\\n1 & -\\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & -i & \\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & -1 & -\\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & i & -\\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} \\\\\n1 & i & -1 & -i & 1 & i & -1 & -i \\\\\n1 & \\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & i & -\\frac{\\sqrt{2}}{2}+i\\frac{\\sqrt{2}}{2} & -1 & -\\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2} & -i & \\frac{\\sqrt{2}}{2}-i\\frac{\\sqrt{2}}{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n0 \\\\ 1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ -1\n\\end{bmatrix}\n\n\n= \\begin{bmatrix}0 \\\\ 0 \\\\ -\\sqrt{2}i \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\sqrt{2}i \\\\ 0\\end{bmatrix}\n\n\n\nFind a_k and b_k for Each y_k:\nFor each y_k = a_k + b_k i:\n\ny_0 = 0 \\implies a_0 = 0, b_0 = 0\ny_1 = 0 \\implies a_1 = 0, b_1 = 0\ny_2 = -\\sqrt{2}i \\implies a_2 = 0, b_2 = -\\sqrt{2}\ny_3 = 0 \\implies a_3 = 0, b_3 = 0\ny_4 = 0 \\implies a_4 = 0, b_4 = 0\ny_5 = 0 \\implies a_5 = 0, b_5 = 0\ny_6 = \\sqrt{2}i \\implies a_6 = 0, b_6 = \\sqrt{2}\ny_7 = 0 \\implies a_7 = 0, b_7 = 0"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-3a.html#forming-the-interpolant",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-3a.html#forming-the-interpolant",
    "title": "Exercise 10.2.3a (C10-P4)",
    "section": "Forming the Interpolant:",
    "text": "Forming the Interpolant:\nFrom Corollary 10.8:\n\nP_8(t) = \\frac{a_0}{\\sqrt{8}} + \\frac{2}{\\sqrt{8}}\\sum_{k=1}^{3}\\left[a_k\\cos(2\\pi k t)-b_k\\sin(2\\pi k t)\\right] + \\frac{a_4}{\\sqrt{8}}\\cos(8\\pi t)\n\nWe found:\n\na_0=0\nb_2 = -\\sqrt{2}\nb_6 = \\sqrt{2}\nAll other a_k,b_k=0\n\nTherefore:\n\nP_8(t) = \\frac{0}{\\sqrt{8}} + \\frac{2}{\\sqrt{8}}\\left[\\left[0\\cos(2\\pi t)-0\\sin(2\\pi t)\\right] + \\left[0\\cos(4\\pi t) - (-\\sqrt{2})\\sin(4\\pi t)\\right] + \\left[0\\cos(6\\pi t)- 0\\sin(6\\pi t)\\right]\\right] + \\frac{0}{\\sqrt{8}}\\cos(8\\pi t)\n\n\n= 0 + \\frac{2}{\\sqrt{8}}\\left[0 + \\left[0 + \\sqrt{2}\\sin(4\\pi t)\\right] + 0\\right] + 0\n\n\n= \\frac{2}{2\\sqrt{2}}\\left[\\sqrt{2}\\sin(4\\pi t)\\right] = \\sin(4\\pi t)"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-3a.html#check-the-result",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-3a.html#check-the-result",
    "title": "Exercise 10.2.3a (C10-P4)",
    "section": "Check the Result:",
    "text": "Check the Result:\n\nP_8(0)=\\sin(0)=0\nP_8(\\tfrac{1}{8})=\\sin(\\pi/2)=1\nP_8(\\tfrac{1}{4})=\\sin(\\pi)=0\nP_8(\\tfrac{3}{8})=\\sin(3\\pi/2)=-1\nP_8(\\tfrac{1}{2})=\\sin(0)=0\nP_8(\\tfrac{5}{8})=\\sin(\\pi/2)=1\nP_8(\\tfrac{3}{4})=\\sin(\\pi)=0\nP_8(\\tfrac{7}{8})=\\sin(3\\pi/2)=-1\n\n\n\n\n\n\n\nFinal Answer:\n\n\n\nThe trigonometric interpolating polynomial is:\n\nP_8(t) = \\sin(4\\pi t)"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-1b.html",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-1b.html",
    "title": "Exercise 10.2.1b (C10-P3)",
    "section": "",
    "text": "10.2.1b\n\n\n\nUse the Discrete Fourier Transform (DFT) and Corollary 10.8 to find the trigonometric interpolating function for the following data:\n\n\n\nt\nx\n\n\n\n\n0\n1\n\n\n\\frac{1}{4}\n1\n\n\n\\frac{1}{2}\n-1\n\n\n\\frac{3}{4}\n-1"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-1b.html#corollary-10.8",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-1b.html#corollary-10.8",
    "title": "Exercise 10.2.1b (C10-P3)",
    "section": "Corollary 10.8",
    "text": "Corollary 10.8\nFor an even integer n, let\n\nt_j = c + j \\frac{(d - c)}{n}, \\quad \\text{for } j = 0, \\dots, n-1,\n\nand let\n\nx = (x_0, \\dots, x_{n-1})\n\ndenote a vector of n real numbers. Define\n\n\\mathbf{a} + \\mathbf{b} i = F_n x,\n\nwhere F_n is the Discrete Fourier Transform. Then the function\n\nP_n(t) = \\frac{a_0}{\\sqrt{n}} + \\frac{2}{\\sqrt{n}} \\sum_{k=1}^{\\frac{n}{2} - 1} \\left( a_k \\cos\\left(\\frac{2\\pi k (t - c)}{d - c}\\right) - b_k \\sin\\left(\\frac{2\\pi k (t - c)}{d - c}\\right) \\right)\n+ \\frac{a_{n/2}}{\\sqrt{n}} \\cos\\left(\\frac{n\\pi (t - c)}{d - c}\\right)\n\nsatisfies\n\nP_n(t_j) = x_j, \\quad \\text{for } j = 0, \\dots, n - 1"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-1b.html#parameters",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-1b.html#parameters",
    "title": "Exercise 10.2.1b (C10-P3)",
    "section": "Parameters:",
    "text": "Parameters:\n\nt = \\left[0, \\frac{1}{4}, \\frac{1}{2}, \\frac{3}{4} \\right]\nx = [1, 1, -1, -1]\nInterval start: c = 0\nInterval end: d = 1\nNumber of data points: n = 4"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-1b.html#compute-the-dft-of-x_j",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-1b.html#compute-the-dft-of-x_j",
    "title": "Exercise 10.2.1b (C10-P3)",
    "section": "Compute the DFT of x_j",
    "text": "Compute the DFT of x_j\nThe Discrete Fourier Transform (DFT) of x = [x_0, \\dots, x_{n-1}]^T is the n-dimensional vector y = [y_0, \\dots, y_{n-1}], where \\omega = e^{-i 2 \\pi / n} and\n\ny_k = \\frac{1}{\\sqrt{n}} \\sum_{j=0}^{n-1} x_j \\omega^{jk}\n\n\nDFT Calculations:\n\nFor k = 0:\n\ny_0 = \\frac{1}{\\sqrt{4}} \\sum_{j=0}^3 x_j \\cdot \\omega^{0 \\cdot j}\n\nSince \\omega^{0 \\cdot j} = 1:\n\ny_0 = \\frac{1}{2} (1 + 1 - 1 - 1) = 0\n\n\n\nFor k = 1:\n\ny_1 = \\frac{1}{\\sqrt{4}} \\sum_{j=0}^3 x_j \\cdot \\omega^j\n\nSubstitute x = [1, 1, -1, -1] and \\omega = e^{-i \\pi / 2} = -i:\n\ny_1 = \\frac{1}{2} (1 + (-i) + 1 + i) = \\frac{1}{2} (2 - 2i) = 1 - i\n\n\n\nFor k = 2:\n\ny_2 = \\frac{1}{\\sqrt{4}} \\sum_{j=0}^3 x_j \\cdot \\omega^{2j}\n\nSince \\omega^2 = -1:\n\ny_2 = \\frac{1}{2} (1 - 1 - 1 + 1) = 0\n\n\n\nFor k = 3:\n\ny_3 = \\frac{1}{\\sqrt{4}} \\sum_{j=0}^3 x_j \\cdot \\omega^{3j}\n\nSince \\omega^3 = i:\n\ny_3 = \\frac{1}{2} (1 + i + 1 - i) = \\frac{1}{2} (2 + 2i) = 1 + i\n\n\n\n\nDFT Results:\n\ny_0 = 0\ny_1 = 1 - i\ny_2 = 0\ny_3 = 1 + i\n\n\n\nFind a_k and b_k for Each y_k:\nFor each y_k = a_k + b_k i:\n\ny_0 = 0 \\implies a_0 = 0, b_0 = 0\ny_1 = 1 - i \\implies a_1 = 1, b_1 = -1\ny_2 = 0 \\implies a_2 = 0, b_2 = 0\ny_3 = 1 + i \\implies a_3 = 1, b_3 = 1"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-1b.html#construct-the-trigonometric-interpolating-polynomial",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-1b.html#construct-the-trigonometric-interpolating-polynomial",
    "title": "Exercise 10.2.1b (C10-P3)",
    "section": "Construct the Trigonometric Interpolating Polynomial",
    "text": "Construct the Trigonometric Interpolating Polynomial\nUsing Corollary 10.8, the interpolating polynomial is:\n\nP_4(t) = \\frac{a_0}{\\sqrt{n}} + \\frac{2}{\\sqrt{n}} \\left( a_1 \\cos(2 \\pi t) - b_1 \\sin(2 \\pi t) \\right) + \\frac{a_2}{\\sqrt{n}} \\cos(4 \\pi t)\n\n\nFirst term:\n\n\\frac{a_0}{\\sqrt{n}} = \\frac{0}{2} = 0\n\nSecond Term:\n\n\\frac{2}{\\sqrt{n}} \\left( a_1 \\cos(2 \\pi t) - b_1 \\sin(2 \\pi t) \\right)\n\nSubstitute a_1 = 1, b_1 = -1, and \\sqrt{n} = 2:\n\n\\frac{2}{2} \\left( 1 \\cdot \\cos(2 \\pi t) - (-1) \\cdot \\sin(2 \\pi t) \\right) = \\cos(2 \\pi t) + \\sin(2 \\pi t)\n\nThird Term:\n\n\\frac{a_2}{\\sqrt{n}} \\cos(4 \\pi t)\n\nSubstitute a_2 = 0:\n\n\\frac{0}{2} \\cos(4 \\pi t) = 0\n\n\nCombine all terms:\n\nP_4(t) = \\cos(2 \\pi t) + \\sin(2 \\pi t)"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-1b.html#verification",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-1b.html#verification",
    "title": "Exercise 10.2.1b (C10-P3)",
    "section": "Verification",
    "text": "Verification\nCheck P_4(t) at each data point:\n\nAt t = 0: P_4(0) = \\cos(0) + \\sin(0) = 1 + 0 = 1\nAt t = \\frac{1}{4}: P_4\\left( \\frac{1}{4} \\right) = \\cos\\left( \\frac{\\pi}{2} \\right) + \\sin\\left( \\frac{\\pi}{2} \\right) = 0 + 1 = 1\nAt t = \\frac{1}{2}: P_4\\left( \\frac{1}{2} \\right) = \\cos(\\pi) + \\sin(\\pi) = -1 + 0 = -1\nAt t = \\frac{3}{4}: P_4\\left( \\frac{3}{4} \\right) = \\cos\\left( \\frac{3\\pi}{2} \\right) + \\sin\\left( \\frac{3\\pi}{2} \\right) = 0 - 1 = -1\n\n\n\n\n\n\n\nFinal Answer:\n\n\n\nThe trigonometric interpolating polynomial is:\n\nP_4(t) = \\cos(2 \\pi t) + \\sin(2 \\pi t)"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w11/exercise10-1-1d.html",
    "href": "mathematics/numerical-analysis/homework/w11/exercise10-1-1d.html",
    "title": "Exercise 4.3.1d",
    "section": "",
    "text": "4.3.1d\n\n\n\nApply classical Gram–Schmidt orthogonalization to find the full QR factorization of the matrix:\n\n\\mathbf{A} = \\begin{bmatrix} 4 & 8 & 1 \\\\ 0 & 2 & -2 \\\\ 3 & 6 & 7 \\end{bmatrix}"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w11/exercise10-1-1d.html#define-the-columns-of-matrix-mathbfa",
    "href": "mathematics/numerical-analysis/homework/w11/exercise10-1-1d.html#define-the-columns-of-matrix-mathbfa",
    "title": "Exercise 4.3.1d",
    "section": "Define the Columns of Matrix \\mathbf{A}",
    "text": "Define the Columns of Matrix \\mathbf{A}\nRepresent the columns of \\mathbf{A} as vectors:\n\n\\mathbf{a}_1 = \\begin{bmatrix} 4 \\\\ 0 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{a}_2 = \\begin{bmatrix} 8 \\\\ 2 \\\\ 6 \\end{bmatrix}, \\quad \\mathbf{a}_3 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 7 \\end{bmatrix}"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w11/exercise10-1-1d.html#finding-mathbfq_1",
    "href": "mathematics/numerical-analysis/homework/w11/exercise10-1-1d.html#finding-mathbfq_1",
    "title": "Exercise 4.3.1d",
    "section": "Finding \\mathbf{q}_1",
    "text": "Finding \\mathbf{q}_1\nTo find the first orthonormal vector \\mathbf{q}_1, normalize \\mathbf{a}_1.\n\nCalculate the norm of \\mathbf{a}_1:\n\n\\|\\mathbf{a}_1\\| = \\sqrt{4^2 + 0^2 + 3^2} = \\sqrt{16 + 9} = \\sqrt{25} = 5\n\nNormalize \\mathbf{a}_1:\n\n\\mathbf{q}_1 = \\frac{\\mathbf{a}_1}{\\|\\mathbf{a}_1\\|} = \\frac{1}{5} \\begin{bmatrix} 4 \\\\ 0 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} \\frac{4}{5} \\\\ 0 \\\\ \\frac{3}{5} \\end{bmatrix}"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w11/exercise10-1-1d.html#finding-mathbfq_2",
    "href": "mathematics/numerical-analysis/homework/w11/exercise10-1-1d.html#finding-mathbfq_2",
    "title": "Exercise 4.3.1d",
    "section": "Finding \\mathbf{q}_2",
    "text": "Finding \\mathbf{q}_2\nTo find \\mathbf{q}_2, project \\mathbf{a}_2 onto \\mathbf{q}_1 and then subtract this projection from \\mathbf{a}_2.\n\nCalculate the projection of \\mathbf{a}_2 onto \\mathbf{q}_1:\n\n\\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_2 = \\left( \\mathbf{a}_2 \\cdot \\mathbf{q}_1 \\right) \\mathbf{q}_1\n\nSince \\mathbf{q}_1 is a unit vector, \\mathbf{q}_1 \\cdot \\mathbf{q}_1 = 1.\nCompute \\mathbf{a}_2 \\cdot \\mathbf{q}_1:\n\n\\mathbf{a}_2 \\cdot \\mathbf{q}_1 = \\begin{bmatrix} 8 \\\\ 2 \\\\ 6 \\end{bmatrix} \\cdot \\begin{bmatrix} \\frac{4}{5} \\\\ 0 \\\\ \\frac{3}{5} \\end{bmatrix} = \\frac{32}{5} + 0 + \\frac{18}{5} = \\frac{50}{5} = 10\n\nCalculate \\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_2:\n\n\\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_2 = 10 \\begin{bmatrix} \\frac{4}{5} \\\\ 0 \\\\ \\frac{3}{5} \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ 0 \\\\ 6 \\end{bmatrix}\n\nCalculate \\mathbf{u}_2:\nSubtracting the projection from \\mathbf{a}_2 gives \\mathbf{u}_2:\n\n\\mathbf{u}_2 = \\mathbf{a}_2 - \\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_2 = \\begin{bmatrix} 8 \\\\ 2 \\\\ 6 \\end{bmatrix} - \\begin{bmatrix} 8 \\\\ 0 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 2 \\\\ 0 \\end{bmatrix}\n\nNormalize \\mathbf{u}_2 to obtain \\mathbf{q}_2:\nThe norm of \\mathbf{u}_2 is:\n\n\\|\\mathbf{u}_2\\| = \\sqrt{0^2 + 2^2 + 0^2} = 2\n\nThus,\n\n\\mathbf{q}_2 = \\frac{\\mathbf{u}_2}{\\|\\mathbf{u}_2\\|} = \\frac{1}{2} \\begin{bmatrix} 0 \\\\ 2 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w11/exercise10-1-1d.html#finding-mathbfq_3",
    "href": "mathematics/numerical-analysis/homework/w11/exercise10-1-1d.html#finding-mathbfq_3",
    "title": "Exercise 4.3.1d",
    "section": "Finding \\mathbf{q}_3",
    "text": "Finding \\mathbf{q}_3\nTo find \\mathbf{q}_3, project \\mathbf{a}_3 onto both \\mathbf{q}_1 and \\mathbf{q}_2, and then subtract these projections from \\mathbf{a}_3.\n\nCalculate the projection of \\mathbf{a}_3 onto \\mathbf{q}_1:\n\n\\mathbf{a}_3 \\cdot \\mathbf{q}_1 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 7 \\end{bmatrix} \\cdot \\begin{bmatrix} \\frac{4}{5} \\\\ 0 \\\\ \\frac{3}{5} \\end{bmatrix} = \\frac{4}{5} + 0 + \\frac{21}{5} = \\frac{25}{5} = 5\n\nThen,\n\n\\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_3 = 5 \\begin{bmatrix} \\frac{4}{5} \\\\ 0 \\\\ \\frac{3}{5} \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 0 \\\\ 3 \\end{bmatrix}\n\nCalculate the projection of \\mathbf{a}_3 onto \\mathbf{q}_2:\n\n\\mathbf{a}_3 \\cdot \\mathbf{q}_2 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 7 \\end{bmatrix} \\cdot \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = -2\n\nThen,\n\n\\text{proj}_{\\mathbf{q}_2} \\mathbf{a}_3 = -2 \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ -2 \\\\ 0 \\end{bmatrix}\n\nCalculate \\mathbf{u}_3:\nSubtracting both projections from \\mathbf{a}_3 yields \\mathbf{u}_3:\n\n\\mathbf{u}_3 = \\mathbf{a}_3 - \\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_3 - \\text{proj}_{\\mathbf{q}_2} \\mathbf{a}_3 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 7 \\end{bmatrix} - \\begin{bmatrix} 4 \\\\ 0 \\\\ 3 \\end{bmatrix} - \\begin{bmatrix} 0 \\\\ -2 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} -3 \\\\ 0 \\\\ 4 \\end{bmatrix}\n\nNormalize \\mathbf{u}_3 to obtain \\mathbf{q}_3:\nThe norm of \\mathbf{u}_3 is:\n\n\\|\\mathbf{u}_3\\| = \\sqrt{(-3)^2 + 0^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5\n\nThus,\n\n\\mathbf{q}_3 = \\frac{\\mathbf{u}_3}{\\|\\mathbf{u}_3\\|} = \\frac{1}{5} \\begin{bmatrix} -3 \\\\ 0 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} -\\frac{3}{5} \\\\ 0 \\\\ \\frac{4}{5} \\end{bmatrix}"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w11/exercise10-1-1d.html#constructing-mathbfq-and-mathbfr-matrices",
    "href": "mathematics/numerical-analysis/homework/w11/exercise10-1-1d.html#constructing-mathbfq-and-mathbfr-matrices",
    "title": "Exercise 4.3.1d",
    "section": "Constructing \\mathbf{Q} and \\mathbf{R} Matrices",
    "text": "Constructing \\mathbf{Q} and \\mathbf{R} Matrices\n\nOrthogonal Matrix \\mathbf{Q}\nCombine \\mathbf{q}_1, \\mathbf{q}\\_2, and \\mathbf{q}_3 as columns to form \\mathbf{Q}:\n\n\\mathbf{Q} = \\begin{bmatrix} \\frac{4}{5} & 0 & -\\frac{3}{5} \\\\ 0 & 1 & 0 \\\\ \\frac{3}{5} & 0 & \\frac{4}{5} \\end{bmatrix}\n\n\n\nUpper Triangular Matrix \\mathbf{R}\nThe entries of \\mathbf{R} are calculated as the dot products of the original columns of \\mathbf{A} with the orthonormal vectors \\mathbf{q}_1, \\mathbf{q}_2, and \\mathbf{q}_3:\n\n\\mathbf{R} = \\begin{bmatrix} \\mathbf{a}_1 \\cdot \\mathbf{q}_1 & \\mathbf{a}_2 \\cdot \\mathbf{q}_1 & \\mathbf{a}_3 \\cdot \\mathbf{q}_1 \\\\ 0 & \\mathbf{a}_2 \\cdot \\mathbf{q}_2 & \\mathbf{a}_3 \\cdot \\mathbf{q}_2 \\\\ 0 & 0 & \\mathbf{a}_3 \\cdot \\mathbf{q}_3 \\end{bmatrix} = \\begin{bmatrix} 5 & 10 & 5 \\\\ 0 & 2 & -2 \\\\ 0 & 0 & 5 \\end{bmatrix}\n\n\n\n\n\n\n\nFinal Answer:\n\n\n\nThe QR factorization of \\mathbf{A} is:\n\n\\mathbf{A} = \\mathbf{Q} \\mathbf{R} = \\begin{bmatrix} \\frac{4}{5} & 0 & -\\frac{3}{5} \\\\ 0 & 1 & 0 \\\\ \\frac{3}{5} & 0 & \\frac{4}{5} \\end{bmatrix} \\begin{bmatrix} 5 & 10 & 5 \\\\ 0 & 2 & -2 \\\\ 0 & 0 & 5 \\end{bmatrix}"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w09/exercise4-3-4.html",
    "href": "mathematics/numerical-analysis/homework/w09/exercise4-3-4.html",
    "title": "Exercise 4.3.4 (C4-P4)",
    "section": "",
    "text": "Problem\n\n\nSolution"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w08/exercise4-1-2a.html",
    "href": "mathematics/numerical-analysis/homework/w08/exercise4-1-2a.html",
    "title": "Exercise 4.1.2a (C4-P1)",
    "section": "",
    "text": "Problem:\n\n\n\n\n\n\n4.1.2a\n\n\n\nFind the least squares solution \\mathbf{\\hat{x}} and the RMSE of the following system:\n\n\\begin{bmatrix}\n1 & 1 & 0 \\\\\n0 & 1 & 1 \\\\\n1 & 2 & 1 \\\\\n1 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2 \\\\ 2 \\\\ 3 \\\\ 4\n\\end{bmatrix}\n\n\n\n\n\nSolution:\n\nCompute A^\\top A and A^\\top \\mathbf{b}\nThe transpose of A is:\n\nA^\\top =\n\\begin{bmatrix}\n1 & 0 & 1 & 1 \\\\\n1 & 1 & 2 & 0 \\\\\n0 & 1 & 1 & 1\n\\end{bmatrix}\n\nCompute A^\\top A:\n\nA^\\top A =\n\\begin{bmatrix}\n3 & 3 & 2 \\\\\n3 & 6 & 3 \\\\\n2 & 3 & 3\n\\end{bmatrix}\n\nCompute A^\\top \\mathbf{b}:\n\nA^\\top \\mathbf{b} =\n\\begin{bmatrix}\n9 \\\\ 10 \\\\ 9\n\\end{bmatrix}\n\nThe normal equation is:\n\nA^\\top A \\mathbf{\\hat{x}} = A^\\top \\mathbf{b}\n\n\n\nSolve A^\\top A \\mathbf{\\hat{x}} = A^\\top \\mathbf{b} using Row Reduction\nThe augmented matrix for the system is:\n\n\\left[\n\\begin{array}{ccc|c}\n3 & 3 & 2 & 9 \\\\\n3 & 6 & 3 & 10 \\\\\n2 & 3 & 3 & 9\n\\end{array}\n\\right]\n\n\nNormalize Row 1\nDivide Row 1 by 3:\n\n\\left[\n\\begin{array}{ccc|c}\n1 & 1 & \\frac{2}{3} & 3 \\\\\n3 & 6 & 3 & 10 \\\\\n2 & 3 & 3 & 9\n\\end{array}\n\\right]\n\n\n\nEliminate the first column in Rows 2 and 3\n\nR_2 \\to R_2 - 3R_1\nR_3 \\to R_3 - 2R_1\n\n\n\\left[\n\\begin{array}{ccc|c}\n1 & 1 & \\frac{2}{3} & 3 \\\\\n0 & 3 & 1 & 1 \\\\\n0 & 1 & \\frac{5}{3} & 3\n\\end{array}\n\\right]\n\n\n\nNormalize Row 2\nDivide Row 2 by 3:\n\n\\left[\n\\begin{array}{ccc|c}\n1 & 1 & \\frac{2}{3} & 3 \\\\\n0 & 1 & \\frac{1}{3} & \\frac{1}{3} \\\\\n0 & 1 & \\frac{5}{3} & 3\n\\end{array}\n\\right]\n\n\n\nEliminate the second column in Rows 1 and 3\n\nR_1 \\to R_1 - R_2\nR_3 \\to R_3 - R_2\n\n\n\\left[\n\\begin{array}{ccc|c}\n1 & 0 & \\frac{4}{9} & \\frac{8}{3} \\\\\n0 & 1 & \\frac{1}{3} & \\frac{1}{3} \\\\\n0 & 0 & \\frac{4}{3} & \\frac{8}{3}\n\\end{array}\n\\right]\n\n\n\nNormalize Row 3\nDivide Row 3 by \\frac{4}{3}:\n\n\\left[\n\\begin{array}{ccc|c}\n1 & 0 & \\frac{4}{9} & \\frac{8}{3} \\\\\n0 & 1 & \\frac{1}{3} & \\frac{1}{3} \\\\\n0 & 0 & 1 & 2\n\\end{array}\n\\right]\n\n\n\nEliminate the third column in Rows 1 and 2\n\nR_1 \\to R_1 - \\frac{4}{9}R_3\nR_2 \\to R_2 - \\frac{1}{3}R_3\n\n\n\\left[\n\\begin{array}{ccc|c}\n1 & 0 & 0 & 2 \\\\\n0 & 1 & 0 & -\\frac{1}{3} \\\\\n0 & 0 & 1 & 2\n\\end{array}\n\\right]\n\nThus, the least squares solution is:\n\n\\mathbf{\\hat{x}} =\n\\begin{bmatrix}\n2 \\\\ -\\frac{1}{3} \\\\ 2\n\\end{bmatrix}\n\n\n\n\nCompute the Residual\nThe residual is:\n\n\\mathbf{r} = \\mathbf{b} - A\\mathbf{\\hat{x}}.\n\nSubstituting \\mathbf{\\hat{x}}:\n\n\\mathbf{r} =\n\\begin{bmatrix}\n2 \\\\ 2 \\\\ 3 \\\\ 4\n\\end{bmatrix}\n-\n\\begin{bmatrix}\n1 + (-\\frac{1}{3}) \\\\ -\\frac{1}{3} + 2 \\\\ 2 + 2(-\\frac{1}{3}) + 2 \\\\ 2 + 2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{1}{3} \\\\ \\frac{1}{3} \\\\ -\\frac{1}{3} \\\\ 0\n\\end{bmatrix}\n\n\n\nCompute RMSE\nThe RMSE is given by:\n\n\\text{RMSE} = \\sqrt{\\frac{\\|\\mathbf{r}\\|^2}{n}}\n\nFirst, compute \\|\\mathbf{r}\\|^2:\n\n\\|\\mathbf{r}\\|^2 = \\left(\\frac{1}{3}\\right)^2 + \\left(\\frac{1}{3}\\right)^2 + \\left(-\\frac{1}{3}\\right)^2 + 0^2 = \\frac{1}{9} + \\frac{1}{9} + \\frac{1}{9} = \\frac{1}{3}\n\nSubstitute into the RMSE formula:\n\n\\text{RMSE} = \\sqrt{\\frac{\\frac{1}{3}}{4}} = \\sqrt{\\frac{1}{12}}\n\nThe approximate value is:\n\n\\text{RMSE} \\approx 0.2887\n\n\n\n\n\n\n\nFinal Answer:\n\n\n\n\nLeast Squares Solution:\n\n\\hat{x}_1 = 2, \\quad \\hat{x}_2 = -\\frac{1}{3}, \\quad \\hat{x}_3 = 2\n\nResidual:\n\n\\mathbf{r} =\\begin{bmatrix}\\frac{1}{3} \\\\ \\frac{1}{3} \\\\ -\\frac{1}{3} \\\\ 0 \\end{bmatrix}\n\nRMSE: \n\\sqrt{\\frac{1}{12}} \\approx 0.2887"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w07/exercise2-4-4a.html",
    "href": "mathematics/numerical-analysis/homework/w07/exercise2-4-4a.html",
    "title": "Exercise 2.4.4a (C2-P9)",
    "section": "",
    "text": "Solve the system by finding the PA = LU factorization and then carrying out the two-step back substitution:\n\n\\begin{pmatrix} 4 & 2 & 0 \\\\ 4 & 4 & 2 \\\\ 2 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix}"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w07/exercise2-4-4a.html#problem",
    "href": "mathematics/numerical-analysis/homework/w07/exercise2-4-4a.html#problem",
    "title": "Exercise 2.4.4a (C2-P9)",
    "section": "",
    "text": "Solve the system by finding the PA = LU factorization and then carrying out the two-step back substitution:\n\n\\begin{pmatrix} 4 & 2 & 0 \\\\ 4 & 4 & 2 \\\\ 2 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix}"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w07/exercise2-4-4a.html#solution",
    "href": "mathematics/numerical-analysis/homework/w07/exercise2-4-4a.html#solution",
    "title": "Exercise 2.4.4a (C2-P9)",
    "section": "Solution:",
    "text": "Solution:\n\nLU Factorization with Partial Pivoting\nStep 1: First Column\n\nPivot Selection: Both a_{11} = 4 and a_{21} = 4 are tied for the largest absolute value. Choose a_{11} as the pivot (no row swap needed).\nCompute Multipliers and Eliminate Below Pivot:\n\nRow 2:\n\nL_{21} = \\dfrac{a_{21}}{a_{11}} = \\dfrac{4}{4} = 1\n\n\n\\text{Row 2} \\rightarrow \\text{Row 2} - L_{21} \\times \\text{Row 1}\n\n\n\\begin{pmatrix} 4 & 2 & 0 \\\\ 0 & 2 & 2 \\\\ 2 & 2 & 3 \\end{pmatrix}\n\nRow 3: \nL_{31} = \\dfrac{a_{31}}{a_{11}} = \\dfrac{2}{4} = \\dfrac{1}{2}\n \n\\text{Row 3} \\rightarrow \\text{Row 3} - L_{31} \\times \\text{Row 1}\n \n\\begin{pmatrix} 4 & 2 & 0 \\\\ 0 & 2 & 2 \\\\ 0 & 1 & 3 \\end{pmatrix}\n\n\n\nStep 2: Second Column\n\nPivot Selection: U_{22} = 2 is the largest absolute value below the pivot (no row swap needed).\nCompute Multiplier and Eliminate Below Pivot:\n\nRow 3: \nL_{32} = \\dfrac{U_{32}}{U_{22}} = \\dfrac{1}{2}\n \n\\text{Row 3} \\rightarrow \\text{Row 3} - L_{32} \\times \\text{Row 2}\n \n\\begin{pmatrix} 4 & 2 & 0 \\\\ 0 & 2 & 2 \\\\ 0 & 0 & 2 \\end{pmatrix}\n\n\n\nResulting Matrices\n\nLower Triangular Matrix L:\n\nL = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n\\dfrac{1}{2} & \\dfrac{1}{2} & 1\n\\end{pmatrix}\n\nUpper Triangular Matrix U:\n\nU = \\begin{pmatrix}\n4 & 2 & 0 \\\\\n0 & 2 & 2 \\\\\n0 & 0 & 2\n\\end{pmatrix}\n\nPermutation Matrix P: \nP = I \\quad (\\text{identity matrix, since no row swaps were performed})\n\n\n\n\nForward Substitution: Solve Ly = b\n\nL \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\end{pmatrix}\n\n\nEquation 1:\n\ny_1 = 2\n\nEquation 2:\n\ny_2 = b_2 - L_{21} y_1 = 4 - (1)(2) = 2\n\nEquation 3: \ny_3 = b_3 - L_{31} y_1 - L_{32} y_2 = 6 - \\left( \\dfrac{1}{2} \\times 2 \\right) - \\left( \\dfrac{1}{2} \\times 2 \\right) = 6 - 1 - 1 = 4\n\n\nSolution:\n\ny = \\begin{pmatrix} 2 \\\\ 2 \\\\ 4 \\end{pmatrix}"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w07/exercise2-4-4a.html#back-substitution-solve-ux-y",
    "href": "mathematics/numerical-analysis/homework/w07/exercise2-4-4a.html#back-substitution-solve-ux-y",
    "title": "Exercise 2.4.4a (C2-P9)",
    "section": "Back Substitution: Solve Ux = y",
    "text": "Back Substitution: Solve Ux = y\n\nU \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\\\ 4 \\end{pmatrix}\n\n\nEquation 3:\n\n2 x_3 = y_3 \\implies x_3 = \\dfrac{y_3}{2} = \\dfrac{4}{2} = 2\n\nEquation 2:\n\n2 x_2 + 2 x_3 = y_2 \\implies x_2 = \\dfrac{y_2 - 2 x_3}{2} = \\dfrac{2 - (2 \\times 2)}{2} = \\dfrac{-2}{2} = -1\n\nEquation 1: \n4 x_1 + 2 x_2 = y_1 \\implies x_1 = \\dfrac{y_1 - 2 x_2}{4} = \\dfrac{2 - (2 \\times -1)}{4} = \\dfrac{2 + 2}{4} = \\dfrac{4}{4} = 1\n\n\nSolution:\n\nx = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}\n\nFinal Answer\nThe solution to the system is:\n\nx_1 = 1, \\quad x_2 = -1, \\quad x_3 = 2"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w03/exercise3-3-3.html",
    "href": "mathematics/numerical-analysis/homework/w03/exercise3-3-3.html",
    "title": "Exercise 3.3.3 (C3-P10)",
    "section": "",
    "text": "Assume that Chebyshev interpolation is used to find a fifth-degree interpolating polynomial Q_5(x) on the interval [-1, 1] for the function f(x) = e^x. Use the interpolation error formula to find a worst-case estimate for the error |e^x - Q_5(x)| that is valid for x throughout the interval [-1, 1]. How many digits after the decimal point will be correct when Q_5(x) is used to approximate e^x?"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w03/exercise3-3-3.html#problem",
    "href": "mathematics/numerical-analysis/homework/w03/exercise3-3-3.html#problem",
    "title": "Exercise 3.3.3 (C3-P10)",
    "section": "",
    "text": "Assume that Chebyshev interpolation is used to find a fifth-degree interpolating polynomial Q_5(x) on the interval [-1, 1] for the function f(x) = e^x. Use the interpolation error formula to find a worst-case estimate for the error |e^x - Q_5(x)| that is valid for x throughout the interval [-1, 1]. How many digits after the decimal point will be correct when Q_5(x) is used to approximate e^x?"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w03/exercise3-3-3.html#solution",
    "href": "mathematics/numerical-analysis/homework/w03/exercise3-3-3.html#solution",
    "title": "Exercise 3.3.3 (C3-P10)",
    "section": "Solution:",
    "text": "Solution:\nWe need to compute the worst-case error for the interpolation of f(x) = e^x using a Chebyshev interpolating polynomial Q_5(x). We will use the interpolation error formula:\n\n|f(x) - P(x)| \\leq \\frac{M}{(n+1)!} \\cdot \\max_{x \\in [-1,1]} |(x - x_1)(x - x_2) \\cdots (x - x_n)|\n\nWhere M is an upper bound on the 6th derivative of f(x) = e^x over [-1, 1], and x_1, x_2, \\dots, x_n are the Chebyshev nodes.\n\nSteps:\n\nChebyshev Node Bound: For Chebyshev interpolation on the interval [-1, 1], the product (x - x_1)(x - x_2) \\cdots (x - x_n) is bounded by \\frac{1}{2^n}. For n = 5, this becomes:\n\n\\frac{1}{2^5} = \\frac{1}{32}\n\nSixth Derivative of e^x: The 6th derivative of f(x) = e^x is f^{(6)}(x) = e^x, and the maximum value of this derivative on the interval [-1, 1] is at x = 1, where f^{(6)}(1) = e \\approx 2.718.\nFactorial Term: The term 6! = 720.\nError Bound Formula: Now, we plug these values into the error bound formula:\n\n|e^x - Q_5(x)| \\leq \\frac{e}{6!} \\cdot \\frac{1}{32}\n\nSubstituting e \\approx 2.718, we get:\n\n|e^x - Q_5(x)| \\leq \\frac{2.718}{720 \\times 32} \\approx \\frac{2.718}{23,040} \\approx 0.000118\n\nThis is the worst-case error estimate for the approximation of e^x on the interval [-1, 1].\nCorrect Decimal Places: Since the error bound is approximately 0.000118, this means we can expect approximately 3 correct digits after the decimal point. The error affects the fourth decimal place, but the first three digits are expected to be correct.\nTherefore, the approximation Q_5(x) will be accurate to 3 digits after the decimal point when approximating e^x on the interval [-1, 1]."
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w03/exercise3-2-5.html",
    "href": "mathematics/numerical-analysis/homework/w03/exercise3-2-5.html",
    "title": "Exercise 3.1.6 (C3-P6)",
    "section": "",
    "text": "How do we construct a polynomial of degree exactly 5 that interpolates four points?\nGiven four points (1, 1), (2, 3), (3, 3), and (4, 4), the standard interpolation methods (such as Newton’s or Lagrange’s methods) would give us a unique polynomial of degree 3. However, constructing a polynomial of degree exactly 5 requires a different approach.\n\n\n\nTheorem 3.2 (Main Theorem of Polynomial Interpolation) guarantees that a unique polynomial of degree n-1 exists for n distinct points.\nTo create a polynomial of degree 5 when you only have 4 points, you need to add an extra term that still passes through all the original points but elevates the polynomial’s degree.\n\n\n\n\nTo create a degree 5 polynomial that still passes through all four points (1, 1), (2, 3), (3, 3), and (4, 4), follow these steps:\n\nFind the degree 3 polynomial P_3(x):\n\nUse either Newton’s divided differences or Lagrange interpolation to find the polynomial of degree 3 that passes through the given points. Let’s call this polynomial P_3(x).\n\nAdd a higher-degree term:\n\nTo turn this degree 3 polynomial into a degree 5 polynomial, we add a term that evaluates to zero at the given points:\n\n\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nThe new term (x - 1)(x - 2)(x - 3)(x - 4) is zero at x = 1, 2, 3, 4, so it won’t affect the interpolation at those points. By multiplying this term by a constant c, we ensure that the degree of the polynomial is elevated to 5, but the polynomial still interpolates the original points.\n\n\n\n\nThus, the degree 5 polynomial is given by:\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nP_3(x) is the degree 3 polynomial found using standard interpolation methods.\nc is an arbitrary constant that determines the shape of the polynomial outside the interpolation points.\n\n\n\n\nLet’s assume the degree 3 polynomial P_3(x) through the points (1, 1), (2, 3), (3, 3), (4, 4) is:\nP_3(x) = 1 + 2(x - 1) + 3(x - 1)(x - 2)\nThen the degree 5 polynomial becomes:\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\n\n\nNo, the value of c does not affect the interpolation at the given points. Since the additional term evaluates to 0 at x = 1, 2, 3, 4, the polynomial will still pass through the points, regardless of c.\nHowever, changing c affects the behavior of the polynomial outside the interpolation points. For different values of c, the polynomial will look different beyond the four points, but it will still pass through (1, 1), $(2, 3) $, $ (3, 3) $, and $ (4, 4)$."
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w03/exercise3-2-5.html#problem",
    "href": "mathematics/numerical-analysis/homework/w03/exercise3-2-5.html#problem",
    "title": "Exercise 3.1.6 (C3-P6)",
    "section": "",
    "text": "How do we construct a polynomial of degree exactly 5 that interpolates four points?\nGiven four points (1, 1), (2, 3), (3, 3), and (4, 4), the standard interpolation methods (such as Newton’s or Lagrange’s methods) would give us a unique polynomial of degree 3. However, constructing a polynomial of degree exactly 5 requires a different approach.\n\n\n\nTheorem 3.2 (Main Theorem of Polynomial Interpolation) guarantees that a unique polynomial of degree n-1 exists for n distinct points.\nTo create a polynomial of degree 5 when you only have 4 points, you need to add an extra term that still passes through all the original points but elevates the polynomial’s degree.\n\n\n\n\nTo create a degree 5 polynomial that still passes through all four points (1, 1), (2, 3), (3, 3), and (4, 4), follow these steps:\n\nFind the degree 3 polynomial P_3(x):\n\nUse either Newton’s divided differences or Lagrange interpolation to find the polynomial of degree 3 that passes through the given points. Let’s call this polynomial P_3(x).\n\nAdd a higher-degree term:\n\nTo turn this degree 3 polynomial into a degree 5 polynomial, we add a term that evaluates to zero at the given points:\n\n\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nThe new term (x - 1)(x - 2)(x - 3)(x - 4) is zero at x = 1, 2, 3, 4, so it won’t affect the interpolation at those points. By multiplying this term by a constant c, we ensure that the degree of the polynomial is elevated to 5, but the polynomial still interpolates the original points.\n\n\n\n\nThus, the degree 5 polynomial is given by:\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nP_3(x) is the degree 3 polynomial found using standard interpolation methods.\nc is an arbitrary constant that determines the shape of the polynomial outside the interpolation points.\n\n\n\n\nLet’s assume the degree 3 polynomial P_3(x) through the points (1, 1), (2, 3), (3, 3), (4, 4) is:\nP_3(x) = 1 + 2(x - 1) + 3(x - 1)(x - 2)\nThen the degree 5 polynomial becomes:\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\n\n\nNo, the value of c does not affect the interpolation at the given points. Since the additional term evaluates to 0 at x = 1, 2, 3, 4, the polynomial will still pass through the points, regardless of c.\nHowever, changing c affects the behavior of the polynomial outside the interpolation points. For different values of c, the polynomial will look different beyond the four points, but it will still pass through (1, 1), $(2, 3) $, $ (3, 3) $, and $ (4, 4)$."
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w03/c0-p2.html",
    "href": "mathematics/numerical-analysis/homework/w03/c0-p2.html",
    "title": "Exercise 3.1.1c (C3-P1)",
    "section": "",
    "text": "Create a Python function that takes as input three points (six scalars, three pairs, or perhaps a 6-element numpy array–choose a method that makes sense to you) and uses the matplotlib package to create a figure window and then render a triangle with small open circles at each of the points and straight lines between each pair of circles. Include code to save your figure to a .png or .jpg file. Validate your function with the points (1, 2), (2, 1), and (2, 3).\n\n\nCreate function and figure\n# Define the function\ndef plot_triangle(points, save_path='triangle_plot.png'):\n    \"\"\"\n    Takes an input of three points (a list of 3 tuples or a 3x2 numpy array)\n    and plots a triangle with small open circles at each of the points.\n    The triangle is rendered with lines connecting each point.\n\n    Parameters:\n    points (list of tuples or numpy array): Points representing the vertices of the triangle.\n    save_path (str): File path to save the plotted figure.\n    \"\"\"\n\n    # Ensure points is a numpy array\n    points = np.array(points)\n\n    # Check if the input is in the correct shape (3x2)\n    if points.shape != (3, 2):\n        raise ValueError(\"Input should be a list of 3 points, each as a pair of (x, y) coordinates.\")\n\n    # Extract the x and y coordinates\n    x_coords = points[:, 0]\n    y_coords = points[:, 1]\n\n    # Close the triangle by repeating the first point at the end\n    x_closed = np.append(x_coords, x_coords[0])\n    y_closed = np.append(y_coords, y_coords[0])\n\n    # Create the plot\n    plt.figure(figsize=(6, 6))\n\n    # Plot the triangle with open circles at each vertex\n    plt.plot(x_closed, y_closed, 'b-', marker='o', markerfacecolor='none',\n             markeredgecolor='r', markersize=10, label='Triangle')\n\n    # Set labels and title\n    plt.title(\"Triangle with Given Vertices\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n\n    # Set axis limits for better visualization\n    plt.xlim(min(x_closed) - 1, max(x_closed) + 1)\n    plt.ylim(min(y_closed) - 1, max(y_closed) + 1)\n\n    # Add grid for better visualization\n    plt.grid(True)\n\n\n    # Save the figure\n    plt.savefig(save_path, dpi=300)\n\n    # Show the plot\n   #  plt.show()\n\n# Test the function with the points (1, 2), (2, 1), and (2, 3)\ntest_points = [(1, 2), (2, 1), (2, 3)]\nplot_triangle(test_points, save_path='triangle_plot.png')\n\n\n\n\n\n\n\n\n\n\n\n\nFunction Definition:\n\nplot_triangle: This function takes in three points and an optional save_path parameter to specify where to save the plot.\nParameters:\n\npoints: A list of three tuples representing the vertices of the triangle or a 3x2 numpy array.\nsave_path: The file path where the plot image will be saved (default is 'triangle_plot.png').\n\n\nInput Validation:\n\nThe function first converts the input points to a numpy array and checks if it has the correct shape (3, 2). If not, it raises a ValueError.\n\nPlotting:\n\nClosing the Triangle: To draw a complete triangle, the first point is appended to the end of the x_coords and y_coords arrays.\nPlotting Lines and Markers:\n\nThe triangle is plotted with blue lines ('b-') connecting the points.\nSmall open red circles (marker='o', markerfacecolor='none', markeredgecolor='r') are placed at each vertex.\n\nLabels and Grid: The plot includes titles, axis labels, and a grid for better visualization.\n\nSaving and Displaying the Plot:\n\nThe plot is saved as a .png file with a resolution of 300 DPI.\nThe plot window is then displayed using plt.show().\n\n\n\n\n\nAfter running the function with the test points (1, 2), (2, 1), and (2, 3), the resulting triangle will be saved as triangle_plot.png and displayed as shown below:\n\n\n\nTriangle Plot"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w03/c0-p2.html#question",
    "href": "mathematics/numerical-analysis/homework/w03/c0-p2.html#question",
    "title": "Exercise 3.1.1c (C3-P1)",
    "section": "",
    "text": "Create a Python function that takes as input three points (six scalars, three pairs, or perhaps a 6-element numpy array–choose a method that makes sense to you) and uses the matplotlib package to create a figure window and then render a triangle with small open circles at each of the points and straight lines between each pair of circles. Include code to save your figure to a .png or .jpg file. Validate your function with the points (1, 2), (2, 1), and (2, 3).\n\n\nCreate function and figure\n# Define the function\ndef plot_triangle(points, save_path='triangle_plot.png'):\n    \"\"\"\n    Takes an input of three points (a list of 3 tuples or a 3x2 numpy array)\n    and plots a triangle with small open circles at each of the points.\n    The triangle is rendered with lines connecting each point.\n\n    Parameters:\n    points (list of tuples or numpy array): Points representing the vertices of the triangle.\n    save_path (str): File path to save the plotted figure.\n    \"\"\"\n\n    # Ensure points is a numpy array\n    points = np.array(points)\n\n    # Check if the input is in the correct shape (3x2)\n    if points.shape != (3, 2):\n        raise ValueError(\"Input should be a list of 3 points, each as a pair of (x, y) coordinates.\")\n\n    # Extract the x and y coordinates\n    x_coords = points[:, 0]\n    y_coords = points[:, 1]\n\n    # Close the triangle by repeating the first point at the end\n    x_closed = np.append(x_coords, x_coords[0])\n    y_closed = np.append(y_coords, y_coords[0])\n\n    # Create the plot\n    plt.figure(figsize=(6, 6))\n\n    # Plot the triangle with open circles at each vertex\n    plt.plot(x_closed, y_closed, 'b-', marker='o', markerfacecolor='none',\n             markeredgecolor='r', markersize=10, label='Triangle')\n\n    # Set labels and title\n    plt.title(\"Triangle with Given Vertices\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n\n    # Set axis limits for better visualization\n    plt.xlim(min(x_closed) - 1, max(x_closed) + 1)\n    plt.ylim(min(y_closed) - 1, max(y_closed) + 1)\n\n    # Add grid for better visualization\n    plt.grid(True)\n\n\n    # Save the figure\n    plt.savefig(save_path, dpi=300)\n\n    # Show the plot\n   #  plt.show()\n\n# Test the function with the points (1, 2), (2, 1), and (2, 3)\ntest_points = [(1, 2), (2, 1), (2, 3)]\nplot_triangle(test_points, save_path='triangle_plot.png')\n\n\n\n\n\n\n\n\n\n\n\n\nFunction Definition:\n\nplot_triangle: This function takes in three points and an optional save_path parameter to specify where to save the plot.\nParameters:\n\npoints: A list of three tuples representing the vertices of the triangle or a 3x2 numpy array.\nsave_path: The file path where the plot image will be saved (default is 'triangle_plot.png').\n\n\nInput Validation:\n\nThe function first converts the input points to a numpy array and checks if it has the correct shape (3, 2). If not, it raises a ValueError.\n\nPlotting:\n\nClosing the Triangle: To draw a complete triangle, the first point is appended to the end of the x_coords and y_coords arrays.\nPlotting Lines and Markers:\n\nThe triangle is plotted with blue lines ('b-') connecting the points.\nSmall open red circles (marker='o', markerfacecolor='none', markeredgecolor='r') are placed at each vertex.\n\nLabels and Grid: The plot includes titles, axis labels, and a grid for better visualization.\n\nSaving and Displaying the Plot:\n\nThe plot is saved as a .png file with a resolution of 300 DPI.\nThe plot window is then displayed using plt.show().\n\n\n\n\n\nAfter running the function with the test points (1, 2), (2, 1), and (2, 3), the resulting triangle will be saved as triangle_plot.png and displayed as shown below:\n\n\n\nTriangle Plot"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w02/exercise3-1-2c.html",
    "href": "mathematics/numerical-analysis/homework/w02/exercise3-1-2c.html",
    "title": "Exercise 3.1.1c (C3-P1)",
    "section": "",
    "text": "Use Newton’s Divided Differences to find a polynomial that passes through the points (0, -2), (2, 1), and (4, 4).\n\n\nThe points are:\n\n(x_1, y_1) = (0, -2)\n(x_2, y_2) = (2, 1)\n(x_3, y_3) = (4, 4)\n\nWe will first construct the divided differences table and use it to construct the Newton interpolating polynomial.\n\n\n\n\n\n\nx\nf[x]\nf[x_1, x_2]\nf[x_1, x_2, x_3]\n\n\n\n\n0\n-2\n\n\n\n\n2\n1\n1.5\n\n\n\n4\n4\n1.5\n0\n\n\n\n\n\n\n\nZeroth order divided differences:\n f[x_1] = y_1 = -2, \\quad f[x_2] = y_2 = 1, \\quad f[x_3] = y_3 = 4 \nFirst order divided differences:\n f[x_1, x_2] = \\frac{f[x_2] - f[x_1]}{x_2 - x_1} = \\frac{1 - (-2)}{2 - 0} = \\frac{3}{2} = 1.5 \n f[x_2, x_3] = \\frac{f[x_3] - f[x_2]}{x_3 - x_2} = \\frac{4 - 1}{4 - 2} = \\frac{3}{2} = 1.5 \nSecond order divided difference:\n f[x_1, x_2, x_3] = \\frac{f[x_2, x_3] - f[x_1, x_2]}{x_3 - x_1} = \\frac{1.5 - 1.5}{4 - 0} = 0 \n\n\n\n\nThe Newton polynomial is given by:\n\nP(x) = f[x_1] + f[x_1, x_2](x - x_1) + f[x_1, x_2, x_3](x - x_1)(x - x_2)\n\nSubstitute the values:\n\nP(x) = -2 + 1.5(x - 0) + 0(x - 0)(x - 2)\n\nSimplify:\n\nP(x) = -2 + 1.5x\n\nSo the final polynomial is:\n\nP(x) = 1.5x - 2\n\nThis is the Newton interpolating polynomial for the points (0, -2), (2, 1), and (4, 4).\n\n\nCreate graph with resulting polynomial\n# Define the Newton interpolating polynomial\ndef newton_polynomial(x):\n    return 1.5 * x - 2\n\n# Create an array of x values from -1 to 5 for the graph\nx_values = np.linspace(-1, 5, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = newton_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = 1.5x - 2\", color=\"blue\")\n\n# Plot the given points (0,-2), (2,1), (4,4)\ndata_points_x = [0, 2, 4]\ndata_points_y = [-2, 1, 4]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Newton's Divided Differences Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 6, 1))\nplt.yticks(np.arange(-4, 5, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w02/exercise3-1-2c.html#question",
    "href": "mathematics/numerical-analysis/homework/w02/exercise3-1-2c.html#question",
    "title": "Exercise 3.1.1c (C3-P1)",
    "section": "",
    "text": "Use Newton’s Divided Differences to find a polynomial that passes through the points (0, -2), (2, 1), and (4, 4).\n\n\nThe points are:\n\n(x_1, y_1) = (0, -2)\n(x_2, y_2) = (2, 1)\n(x_3, y_3) = (4, 4)\n\nWe will first construct the divided differences table and use it to construct the Newton interpolating polynomial.\n\n\n\n\n\n\nx\nf[x]\nf[x_1, x_2]\nf[x_1, x_2, x_3]\n\n\n\n\n0\n-2\n\n\n\n\n2\n1\n1.5\n\n\n\n4\n4\n1.5\n0\n\n\n\n\n\n\n\nZeroth order divided differences:\n f[x_1] = y_1 = -2, \\quad f[x_2] = y_2 = 1, \\quad f[x_3] = y_3 = 4 \nFirst order divided differences:\n f[x_1, x_2] = \\frac{f[x_2] - f[x_1]}{x_2 - x_1} = \\frac{1 - (-2)}{2 - 0} = \\frac{3}{2} = 1.5 \n f[x_2, x_3] = \\frac{f[x_3] - f[x_2]}{x_3 - x_2} = \\frac{4 - 1}{4 - 2} = \\frac{3}{2} = 1.5 \nSecond order divided difference:\n f[x_1, x_2, x_3] = \\frac{f[x_2, x_3] - f[x_1, x_2]}{x_3 - x_1} = \\frac{1.5 - 1.5}{4 - 0} = 0 \n\n\n\n\nThe Newton polynomial is given by:\n\nP(x) = f[x_1] + f[x_1, x_2](x - x_1) + f[x_1, x_2, x_3](x - x_1)(x - x_2)\n\nSubstitute the values:\n\nP(x) = -2 + 1.5(x - 0) + 0(x - 0)(x - 2)\n\nSimplify:\n\nP(x) = -2 + 1.5x\n\nSo the final polynomial is:\n\nP(x) = 1.5x - 2\n\nThis is the Newton interpolating polynomial for the points (0, -2), (2, 1), and (4, 4).\n\n\nCreate graph with resulting polynomial\n# Define the Newton interpolating polynomial\ndef newton_polynomial(x):\n    return 1.5 * x - 2\n\n# Create an array of x values from -1 to 5 for the graph\nx_values = np.linspace(-1, 5, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = newton_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = 1.5x - 2\", color=\"blue\")\n\n# Plot the given points (0,-2), (2,1), (4,4)\ndata_points_x = [0, 2, 4]\ndata_points_y = [-2, 1, 4]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Newton's Divided Differences Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 6, 1))\nplt.yticks(np.arange(-4, 5, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w02/exercise3-1-1c.html",
    "href": "mathematics/numerical-analysis/homework/w02/exercise3-1-1c.html",
    "title": "Exercise 3.1.1c (C3-P1)",
    "section": "",
    "text": "Use Lagrange interpolation to find a polynomial that passes through the points (0, -2), (2, 1), (4, 4).\nThe Lagrange interpolation polynomial for three points (x_1, y_1), (x_2, y_2), and (x_3, y_3) is given by the formula:\nP(x) = y_1 \\frac{(x - x_2)(x - x_3)}{(x_1 - x_2)(x_1 - x_3)} + y_2 \\frac{(x - x_1)(x - x_3)}{(x_2 - x_1)(x_2 - x_3)} + y_3 \\frac{(x - x_1)(x - x_2)}{(x_3 - x_1)(x_3 - x_2)}\nThe points are:\n\n(x_1, y_1) = (0, -2)\n(x_2, y_2) = (2, 1)\n(x_3, y_3) = (4, 4)\n\n\n\n\nFirst term (corresponding to (x_1, y_1) = (0, -2)):\n\n\n-2 \\cdot \\frac{(x - 2)(x - 4)}{(0 - 2)(0 - 4)} = -2 \\cdot \\frac{(x - 2)(x - 4)}{(-2)(-4)} = -2 \\cdot \\frac{(x - 2)(x - 4)}{8} = \\frac{-1}{4}(x - 2)(x - 4)\n\n\nSecond term (corresponding to (x_2, y_2) = (2, 1))\n\n\n1 \\cdot \\frac{(x - 0)(x - 4)}{(2 - 0)(2 - 4)} = 1 \\cdot \\frac{x(x - 4)}{(2)(-2)} = \\frac{x(x - 4)}{-4}\n\n\nThird term (corresponding to (x_3, y_3) = (4, 4)):\n\n\n4 \\cdot \\frac{(x - 0)(x - 2)}{(4 - 0)(4 - 2)} = 4 \\cdot \\frac{x(x - 2)}{(4)(2)} = 4 \\cdot \\frac{x(x - 2)}{8} = \\frac{x(x - 2)}{2}\n\n\n\n\n\nP(x) = \\frac{-1}{4}(x - 2)(x - 4) + \\frac{x(x - 4)}{-4} + \\frac{x(x - 2)}{2}\n\n\n\n\nFirst term:\n\n\\frac{-1}{4}(x - 2)(x - 4) = \\frac{-x^2 + 6x - 8}{4}\n\nSecond term:\n\n\\frac{-x(x - 4)}{4} = \\frac{-x^2 + 4x}{4}\n\nThird term:\n\n\\frac{x(x - 2)}{2} = \\frac{x^2 - 2x}{2}\n\nCombine the terms:\n\nP(x) = \\frac{-x^2 + 6x - 8}{4} + \\frac{-x^2 + 4x}{4} + \\frac{x^2 - 2x}{2}\n\nTo combine, first rewrite everything with a denominator of 4:\n\nP(x) = \\frac{-x^2 + 6x - 8 - x^2 + 4x}{4} + \\frac{x^2 - 2x}{2}\n\nConvert the second term to have a denominator of 4:\n\nP(x) = \\frac{-x^2 + 6x - 8 - x^2 + 4x}{4} + \\frac{2x^2 - 4x}{4}\n\nNow simplify:\n\nP(x) = \\frac{-2x^2 + 10x - 8}{4} + \\frac{2x^2 - 4x}{4}\n\n\nP(x) = \\frac{6x - 8}{4} = \\frac{3x - 4}{2}\n\nFinal polynomial:\n\nP(x) = \\frac{3x - 4}{2}\n\nThis is the interpolating polynomial that passes through the points (0, -2), (2, 1), and (4, 4).\n\n\nCreate graph with resulting polynomial\n# Define the Lagrange interpolating polynomial\ndef lagrange_polynomial(x):\n    return (3 * x - 4) / 2\n\n# Create an array of x values from -1 to 5 for the graph\nx_values = np.linspace(-1, 5, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = lagrange_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = (3x - 4) / 2\", color=\"blue\")\n\n# Plot the given points (0,-2), (2,1), (4,4)\ndata_points_x = [0, 2, 4]\ndata_points_y = [-2, 1, 4]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Lagrange Interpolating Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 6, 1))\nplt.yticks(np.arange(-4, 5, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w02/exercise3-1-1c.html#question",
    "href": "mathematics/numerical-analysis/homework/w02/exercise3-1-1c.html#question",
    "title": "Exercise 3.1.1c (C3-P1)",
    "section": "",
    "text": "Use Lagrange interpolation to find a polynomial that passes through the points (0, -2), (2, 1), (4, 4).\nThe Lagrange interpolation polynomial for three points (x_1, y_1), (x_2, y_2), and (x_3, y_3) is given by the formula:\nP(x) = y_1 \\frac{(x - x_2)(x - x_3)}{(x_1 - x_2)(x_1 - x_3)} + y_2 \\frac{(x - x_1)(x - x_3)}{(x_2 - x_1)(x_2 - x_3)} + y_3 \\frac{(x - x_1)(x - x_2)}{(x_3 - x_1)(x_3 - x_2)}\nThe points are:\n\n(x_1, y_1) = (0, -2)\n(x_2, y_2) = (2, 1)\n(x_3, y_3) = (4, 4)\n\n\n\n\nFirst term (corresponding to (x_1, y_1) = (0, -2)):\n\n\n-2 \\cdot \\frac{(x - 2)(x - 4)}{(0 - 2)(0 - 4)} = -2 \\cdot \\frac{(x - 2)(x - 4)}{(-2)(-4)} = -2 \\cdot \\frac{(x - 2)(x - 4)}{8} = \\frac{-1}{4}(x - 2)(x - 4)\n\n\nSecond term (corresponding to (x_2, y_2) = (2, 1))\n\n\n1 \\cdot \\frac{(x - 0)(x - 4)}{(2 - 0)(2 - 4)} = 1 \\cdot \\frac{x(x - 4)}{(2)(-2)} = \\frac{x(x - 4)}{-4}\n\n\nThird term (corresponding to (x_3, y_3) = (4, 4)):\n\n\n4 \\cdot \\frac{(x - 0)(x - 2)}{(4 - 0)(4 - 2)} = 4 \\cdot \\frac{x(x - 2)}{(4)(2)} = 4 \\cdot \\frac{x(x - 2)}{8} = \\frac{x(x - 2)}{2}\n\n\n\n\n\nP(x) = \\frac{-1}{4}(x - 2)(x - 4) + \\frac{x(x - 4)}{-4} + \\frac{x(x - 2)}{2}\n\n\n\n\nFirst term:\n\n\\frac{-1}{4}(x - 2)(x - 4) = \\frac{-x^2 + 6x - 8}{4}\n\nSecond term:\n\n\\frac{-x(x - 4)}{4} = \\frac{-x^2 + 4x}{4}\n\nThird term:\n\n\\frac{x(x - 2)}{2} = \\frac{x^2 - 2x}{2}\n\nCombine the terms:\n\nP(x) = \\frac{-x^2 + 6x - 8}{4} + \\frac{-x^2 + 4x}{4} + \\frac{x^2 - 2x}{2}\n\nTo combine, first rewrite everything with a denominator of 4:\n\nP(x) = \\frac{-x^2 + 6x - 8 - x^2 + 4x}{4} + \\frac{x^2 - 2x}{2}\n\nConvert the second term to have a denominator of 4:\n\nP(x) = \\frac{-x^2 + 6x - 8 - x^2 + 4x}{4} + \\frac{2x^2 - 4x}{4}\n\nNow simplify:\n\nP(x) = \\frac{-2x^2 + 10x - 8}{4} + \\frac{2x^2 - 4x}{4}\n\n\nP(x) = \\frac{6x - 8}{4} = \\frac{3x - 4}{2}\n\nFinal polynomial:\n\nP(x) = \\frac{3x - 4}{2}\n\nThis is the interpolating polynomial that passes through the points (0, -2), (2, 1), and (4, 4).\n\n\nCreate graph with resulting polynomial\n# Define the Lagrange interpolating polynomial\ndef lagrange_polynomial(x):\n    return (3 * x - 4) / 2\n\n# Create an array of x values from -1 to 5 for the graph\nx_values = np.linspace(-1, 5, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = lagrange_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = (3x - 4) / 2\", color=\"blue\")\n\n# Plot the given points (0,-2), (2,1), (4,4)\ndata_points_x = [0, 2, 4]\ndata_points_y = [-2, 1, 4]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Lagrange Interpolating Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 6, 1))\nplt.yticks(np.arange(-4, 5, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/math411-suggested-homework.html",
    "href": "mathematics/numerical-analysis/homework/math411-suggested-homework.html",
    "title": "MATH411 Suggested Homework - Fall 2024",
    "section": "",
    "text": "Week 1\n\n(C0-P1) Read/work through the Getting Started with Python notebook (see Modules &gt; Homework &gt; Getting_Started_with_Python.ipynb). Then complete the following exercises in a separate notebook or .py file:\n\nWhat is the difference between the outputs generated by the following two lines of code?\n\nnp.array([i for i in range(10)])\nnp.linspace(0, 9, 10, endpoint=True)\n\nUse both np.linspace() and np.arange() to create an array containing floating point numbers starting at 1.0, ending at 4.0, equally spaced with separation 0.2. In other words, the array should contain 1.0, 1.2, 1.4, …, 3.8, 4.0.\nCreate an array consisting of the floats 1.0, 2.0, 3.0, 4.0, and 5.0. Create a second array containing the square root of each of these numbers. Then, use a for loop to compute the sum of the squared differences between the two arrays:\n\n\n\\sum_{i=1}^n \\left(x_i - \\sqrt{x_i}\\right)^2\n\nExtra Challenge: Can you do this without a loop?\n\nStarting with x = 1, use a while loop to divide by 2 until x &lt; 10^{-4}. Display (print) the list 1.0, 0.5, 0.25, …, and report the number of divisions by 2 needed such that the (k-1)th division produces x &gt; 10^{-4} and the kth division produces x &lt; 10^{-4}.\nWrite code to create a function to compute f(x) = e^{-x} \\cos x, where x is a vector (array) of one or more numbers. Then evaluate f(x) at the points 0, 0.1, 0.2, …, 1.0.\nWrite code to plot the function h(x) = e^{x} \\cos^2 x - 2 on the interval -0.5 to 5.5 and visually estimate the roots of h(x) on that interval.\n\n(C1-P1) Exercise 1.1.4ab\n(C1-P2) Exercise 1.2.2\n(C1-P3) Computer Problem 1.2.2ab. For each equation, find an initial point x_0 and a function g(x) such that the fixed-point iteration x_{k+1} = g(x_k) converges to x, where g(x) = x. If this is not possible, explain why.\n(C1-P4) Exercise 1.2.14\n(C1-P5) Exercise 1.4.1\n(C1-P6) Exercise 1.4.3\n\n\nWeek 2\n\n(C1-P7) Exercise 1.4.6\n(C1-P8) Exercise 1.4.8\n(C1-P9) Computer Problem 1.4.7\n(C1-P10) Exercise 1.5.1\n(C1-P11) Use Python to compare results obtained using the Bisection Method, Newton’s Method, and the Secant Method to solve the equation \\ln x + x^2 = 3. Note that Python code for these methods is available on I-Learn.\n\nSolve the problem using each of the three methods. Report starting values and the number of iterations required to obtain 6 correct decimal places of accuracy. Hint: a graph of the function may help with starting values.\nOn the same axes, plot \\log(\\epsilon_{i+1}) vs. \\log(\\epsilon_i) for the three methods. Explain your plot. How is it related to the rate or order of convergence? Use the errors to determine if your results are consistent with theory. How would you compute the error if you didn’t have an exact value for the root?\n\n(C3-P1) Exercise 3.1.1ac\n(C3-P2) Exercise 3.1.2ac\n(C3-P3) Exercise 3.1.6\n\n\nWeek 3\n\n(C0-P2) Create a Python function that takes as input three points (six scalars, three pairs, or perhaps a 6-element numpy array—choose a method that makes sense to you) and uses the matplotlib package to create a figure window and then render a triangle with small open circles at each of the points and straight lines between each pair of circles. Include code to save your figure to a .png or .jpg file. Validate your function with the points (1, 2), (2, 1), and (2, 3).\n(C3-P4) Exercise 3.2.2\n(C3-P5) Exercise 3.2.5\n(C3-P6) Exercise 3.2.6 Note: the two additional points in the next-to-last sentence should be (x_7, y_7) = (0.1, f(0.1)) and (x_8, y_8) = (0.5, f(0.5)).\n(C3-P7) Computer Problem 3.1.3. To demonstrate that your function works, interpolate \\sin(x) on the interval [-\\pi, \\pi] using nodes -\\pi, -\\frac{\\pi}{2}, 0, \\frac{\\pi}{2}, \\pi. Plot your interpolating polynomial. Plot \\sin(x) on the same graph, and use the numpy functions polyfit and polyval to plot an interpolating polynomial from Python on the same graph. Use a legend to make clear which curve is yours and which one came from Python. Hint: the code on p. 146 should help. Python versions of newtdd and nest (p. 3) are available in Canvas.\n(C3-P8) Exercise 3.3.2ac\n(C3-P9) Exercise 3.3.2ac\n(C3-P10) Exercise 3.3.3\n\n\nWeek 4\n\n(C5-P1) Exercise 5.2.1ab\n(C5-P2) Exercise 5.2.2ab\n(C5-P3) Exercise 5.2.3ab\n(C5-P4) Exercise 5.2.10\n(C5-P5) Exercise 5.2.12\n(C5-P6) Computer Problem 5.2.1ac\n(C5-P7) Computer Problem 5.2.2de\n(C5-P8) Computer Problem 5.2.9bf\n\n\nWeek 5\n\n(C5-P9) Exercise 5.5.1ab\n(C5-P10) Exercise 5.5.4cd\n(C5-P11) Computer Problem 5.4.1acd\n(C5-P12) Computer Problem 5.4.2\n(C5-P13) Computer Problem 5.4.3acd\n(C5-P14) Exercise 5.5.5cd\n(C5-P15) Exercise 5.5.7\n\n\nWeek 6\n\n(C2-P1) Exercise 2.1.2ac\n(C2-P2) Computer Problem 2.1.2ac\n(C2-P3) Exercise 2.2.1ab\n(C2-P4) Exercise 2.2.2ab\n\n\nWeek 7\n\n(C2-P5) Exercise 2.2.4\n(C2-P6) Computer Problem 2.2.1ab\n(C2-P7) Exercise 2.4.1ab\n(C2-P8) Exercise 2.4.2ab\n(C2-P9) Exercise 2.4.4a\n(C2-P10) Exercise 2.4.6\n\n\nWeek 8\n\n(C2-P11) Exercise 2.5.2ab\n(C2-P12) Computer Problem 2.5.2 (solve using both Jacobi and Gauss-Seidel, compare results)\n(C4-P1) Exercise 4.1.2\n(C4-P2) Computer Problem 4.1.5 (also use a quadratic fit and compare)\n(C4-P3) Exercise 4.3.2\n\n\nWeek 9\n\n(C4-P4) Exercise 4.3.4 (use the matrix from Exercise 4.3.1d)\n(C4-P5) Exercise 4.3.7 (you can use your QR factorizations from Exercise 4.3.2)\n(C4-P6) Computer Problem 4.3.4 Additional instructions: Write a classical Gram-Schmidt code only. Use the matrices in Exercise 4.3.2 to check your code. If you use the code I provided, you must comment it (explain what every line does).\n\n\nWeek 10\n\n(C4-P7) Exercise 4.4.2\n(C4-P8) Exercise 4.4.3\n(C4-P9) Computer Problem 4.4.2 (find a preconditioned GMRES Python code and use it)\n\n\nWeek 11\n\nNone\nExam 2 in class on Monday Week 11\nAttempt the first several Week 12 problems before Monday Week 12\n\n\nWeek 12\n\n(C10-P1) Exercise 10.1.1ad (also, find the inverse DFT of your result, compare to the original vector)\n(C10-P2) Exercise 10.1.8\n(C10-P3) Exercise 10.2.1ab\n(C10-P4) Exercise 10.2.3ab\n(C10-P5) Exercise 10.2.3 (plot data and function to show your interpolating function does interpolate the data)\n(C10-P6) Computer Problem 10.2.4\n(C10-P7) Exercise 10.3.2ab\n\n\nWeek 13\n\n(C10-P8) Computer Problem 10.3.2cd\n(C10-P9) Exercise 10.3.5 (Complete the \\sum_{j=0}^{n-1} \\cos \\frac{2 \\pi j k}{n} \\cos \\frac{2 \\pi j l}{n} result only)\n(C10-P10) Exercise 10.1.6"
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/root-of-unity.html",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/root-of-unity.html",
    "title": "Roots of Unity: From Real Numbers to the Complex Plane",
    "section": "",
    "text": "Roots of unity are solutions to the equation:\n\nz^n = 1\n\nwhere z is a number (real or complex) and n is a positive integer. These solutions represent the numbers that, when multiplied by themselves n times, equal 1."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/root-of-unity.html#overview",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/root-of-unity.html#overview",
    "title": "Roots of Unity: From Real Numbers to the Complex Plane",
    "section": "",
    "text": "Roots of unity are solutions to the equation:\n\nz^n = 1\n\nwhere z is a number (real or complex) and n is a positive integer. These solutions represent the numbers that, when multiplied by themselves n times, equal 1."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/root-of-unity.html#roots-of-unity-in-real-numbers",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/root-of-unity.html#roots-of-unity-in-real-numbers",
    "title": "Roots of Unity: From Real Numbers to the Complex Plane",
    "section": "Roots of Unity in Real Numbers",
    "text": "Roots of Unity in Real Numbers\n\nWhat Are They?\nIn the real numbers, the equation z^n = 1 has at most two solutions:\n\nz = 1 (the trivial solution).\nz = -1, but only when n is even.\n\n\n\nPrimitive Roots in Real Numbers\nPrimitive roots of unity are roots that generate all roots of unity through their powers. In the real numbers:\n\nz = 1: Trivial root, but it cannot generate anything else. Not primitive.\nz = -1: Primitive for even n, as it alternates between 1 and -1.\n\nThus, in the real numbers:\n\nFor n = 2, the primitive root is -1.\nFor n &gt; 2, there are no new primitive roots in the reals.\n\nThis limitation motivates expanding the idea to the complex plane."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/root-of-unity.html#extending-roots-of-unity-to-the-complex-plane",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/root-of-unity.html#extending-roots-of-unity-to-the-complex-plane",
    "title": "Roots of Unity: From Real Numbers to the Complex Plane",
    "section": "Extending Roots of Unity to the Complex Plane",
    "text": "Extending Roots of Unity to the Complex Plane\n\nComplex Roots of Unity\nTo find all solutions to z^n = 1 in the complex plane, we use Euler’s formula:\n\nz = e^{i \\theta} = \\cos(\\theta) + i \\sin(\\theta)\n\nwhere \\theta is the angle (in radians) of z on the unit circle in the complex plane. The general solution to z^n = 1 is:\n\nz_k = e^{i \\frac{2\\pi k}{n}}, \\quad k = 0, 1, \\dots, n-1\n\nThese solutions are the n-th roots of unity. They are equally spaced points on the unit circle, with angles:\n\n\\theta_k = \\frac{2\\pi k}{n}\n\n\n\nExample: 4th Roots of Unity (n = 4)\nFor z^4 = 1, the roots are:\n\nz_0 = e^{i \\frac{2\\pi \\cdot 0}{4}} = e^{i 0} = 1\nz_1 = e^{i \\frac{2\\pi \\cdot 1}{4}} = e^{i \\frac{\\pi}{2}} = i\nz_2 = e^{i \\frac{2\\pi \\cdot 2}{4}} = e^{i \\pi} = -1\nz_3 = e^{i \\frac{2\\pi \\cdot 3}{4}} = e^{i \\frac{3\\pi}{2}} = -i\n\nThese roots are \\{1, i, -1, -i\\}, representing four equally spaced points on the unit circle."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/root-of-unity.html#primitive-roots-of-unity",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/root-of-unity.html#primitive-roots-of-unity",
    "title": "Roots of Unity: From Real Numbers to the Complex Plane",
    "section": "Primitive Roots of Unity",
    "text": "Primitive Roots of Unity\n\nWhat Makes a Root Primitive?\nA root of unity z is primitive if it generates all n-th roots of unity through its successive powers.\nFormally:\n\nz is primitive if z^k \\neq 1 for 1 \\leq k &lt; n, and z^n = 1\n\n\n\nExample: Primitive Roots for n = 4\nThe 4th roots of unity are \\{1, i, -1, -i\\}. Let’s test each root:\n\nz = 1: 1^k = 1 for all k. Not primitive.\nz = i:\n\ni^1 = i\ni^2 = -1\ni^3 = -i\ni^4 = 1 (cycles through all roots) Primitive\n\nz = -1:\n\n(-1)^1 = -1,\n(-1)^2 = 1 (only alternates between -1 and 1). Not primitive.\n\nz = -i:\n\n(-i)^1 = -i\n(-i)^2 = -1\n(-i)^3 = i\n(-i)^4 = 1 Primitive\n\n\nThus, the primitive roots for n = 4 are i and -i."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/root-of-unity.html#visualizing-roots-of-unity",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/root-of-unity.html#visualizing-roots-of-unity",
    "title": "Roots of Unity: From Real Numbers to the Complex Plane",
    "section": "Visualizing Roots of Unity",
    "text": "Visualizing Roots of Unity\nRoots of unity lie on the unit circle in the complex plane, evenly spaced at angles \\frac{2\\pi}{n}. For n = 4:\n\nz_0 = 1 at 0^\\circ\nz_1 = i at 90^\\circ\nz_2 = -1 at 180^\\circ\nz_3 = -i at 270^\\circ\n\nPrimitive roots, like i, act as the “generator,” visiting each point exactly once when raised to successive powers."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/root-of-unity.html#applications-of-roots-of-unity",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/root-of-unity.html#applications-of-roots-of-unity",
    "title": "Roots of Unity: From Real Numbers to the Complex Plane",
    "section": "Applications of Roots of Unity",
    "text": "Applications of Roots of Unity\n\nIn Algebra:\nRoots of unity are solutions to the polynomial z^n - 1 = 0, foundational in field theory and Galois theory.\nIn Fourier Analysis:\nThe Discrete Fourier Transform (DFT) uses primitive roots to decompose signals into frequency components.\nIn Number Theory:\nRoots of unity appear in modular arithmetic, cyclotomic fields, and cryptography."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/root-of-unity.html#summary",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/root-of-unity.html#summary",
    "title": "Roots of Unity: From Real Numbers to the Complex Plane",
    "section": "Summary",
    "text": "Summary\n\nReal Numbers: Only 1 and -1 are roots of unity, and -1 is primitive for n = 2.\nComplex Plane: Extending to complex numbers introduces all n-th roots of unity, equally spaced on the unit circle.\nPrimitive Roots: These roots generate all other roots through successive powers, playing a central role in algebra, signal processing, and number theory."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/fourier-transform.html",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/fourier-transform.html",
    "title": "Fourier Transform",
    "section": "",
    "text": "The Fourier Transform is a mathematical operation that transforms a function of time g(t) into a function of frequency f. It provides a powerful tool for analyzing the frequency components of signals and is widely used in physics, engineering, and signal processing."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/fourier-transform.html#overview",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/fourier-transform.html#overview",
    "title": "Fourier Transform",
    "section": "",
    "text": "The Fourier Transform is a mathematical operation that transforms a function of time g(t) into a function of frequency f. It provides a powerful tool for analyzing the frequency components of signals and is widely used in physics, engineering, and signal processing."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/fourier-transform.html#definition",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/fourier-transform.html#definition",
    "title": "Fourier Transform",
    "section": "Definition",
    "text": "Definition\nThe Fourier Transform of a function g(t) is given by:\n\n\\hat{g}(f) = \\int_{-\\infty}^{+\\infty} g(t) e^{-2\\pi i f t} \\, dt\n\nHere:\n\ng(t): The function in the time domain.\nf: The frequency variable (in Hz).\n\\hat{g}(f): The Fourier Transform, representing the frequency domain representation of g(t).\ne^{-2\\pi i f t}: A complex exponential that encodes frequency components.\n\nThe Inverse Fourier Transform allows reconstruction of g(t) from its Fourier Transform \\hat{g}(f):\n\ng(t) = \\int_{-\\infty}^{+\\infty} \\hat{g}(f) e^{2\\pi i f t} \\, df"
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/fourier-transform.html#properties",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/fourier-transform.html#properties",
    "title": "Fourier Transform",
    "section": "Properties",
    "text": "Properties\n\nLinearity:\n\n\\mathcal{F}(a g_1 + b g_2)(f) = a \\hat{g}_1(f) + b \\hat{g}_2(f)\n\nTime Shift: If g(t) \\to \\hat{g}(f), then g(t - t_0) \\to \\hat{g}(f)e^{-2\\pi i f t_0}.\nFrequency Shift: If g(t) \\to \\hat{g}(f), then e^{2\\pi i f_0 t} g(t) \\to \\hat{g}(f - f_0).\nScaling: If g(at) \\to \\frac{1}{|a|} \\hat{g}\\left(\\frac{f}{a}\\right).\nParseval’s Theorem: The total energy of the signal in the time and frequency domains is the same:\n\n\\int_{-\\infty}^{+\\infty} |g(t)|^2 \\, dt = \\int_{-\\infty}^{+\\infty} |\\hat{g}(f)|^2 \\, df\n\nConvolution: Convolution in the time domain corresponds to multiplication in the frequency domain:\n\n\\mathcal{F}(g_1 \\ast g_2)(f) = \\hat{g}_1(f) \\cdot \\hat{g}_2(f)\n\nwhere (g*1 \\ast g_2)(t) = \\int*{-\\infty}^{+\\infty} g_1(\\tau) g_2(t - \\tau) \\, d\\tau."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/fourier-transform.html#applications",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/fourier-transform.html#applications",
    "title": "Fourier Transform",
    "section": "Applications",
    "text": "Applications\n\nSignal Processing:\n\nFrequency analysis, filtering, and noise reduction.\n\nPhysics:\n\nAnalyzing waveforms, optics, and quantum mechanics.\n\nEngineering:\n\nCommunication systems and control systems.\n\nImage Processing:\n\nCompression, edge detection, and restoration.\n\n\n\nExample: Fourier Transform of a Sine Wave\nConsider the signal g(t) = \\sin(2\\pi f_0 t). Using Euler’s formula, \\sin(2\\pi f_0 t) = \\frac{e^{2\\pi i f_0 t} - e^{-2\\pi i f_0 t}}{2i}, we find:\n\n\\hat{g}(f) = \\frac{1}{2i} [\\delta(f - f_0) - \\delta(f + f_0)]\n\nThe Fourier Transform shows that the sine wave consists of two frequency components at \\pm f_0."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/fourier-transform.html#discrete-fourier-transform-dft",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/fourier-transform.html#discrete-fourier-transform-dft",
    "title": "Fourier Transform",
    "section": "Discrete Fourier Transform (DFT)",
    "text": "Discrete Fourier Transform (DFT)\nFor digital signals, the Fourier Transform is approximated using the Discrete Fourier Transform (DFT). For a sequence of N samples \\{g_n\\}:\n\n\\hat{g}_k = \\sum_{n=0}^{N-1} g_n e^{-2\\pi i k n / N}, \\quad k = 0, 1, \\dots, N-1\n\nThe Inverse DFT (IDFT) is:\n\ng_n = \\frac{1}{N} \\sum_{k=0}^{N-1} \\hat{g}_k e^{2\\pi i k n / N}\n\nThe Fast Fourier Transform (FFT) is an efficient algorithm to compute the DFT."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/fourier-transform.html#conclusion",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/fourier-transform.html#conclusion",
    "title": "Fourier Transform",
    "section": "Conclusion",
    "text": "Conclusion\nThe Fourier Transform bridges the time and frequency domains, enabling analysis and manipulation of signals. Its versatility and foundational properties make it an essential tool in science and engineering."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/discrete-fourier-transform.html",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/discrete-fourier-transform.html",
    "title": "Understanding the Discrete Fourier Transform (DFT)",
    "section": "",
    "text": "Imagine you’re listening to music. You hear a melody, but the melody is made up of individual notes (frequencies). The Discrete Fourier Transform (DFT) works similarly—it takes a “melody” of data in the time domain (a sequence of numbers) and breaks it into its “notes” (frequency components). This makes it easier to analyze patterns in signals like sound, images, or other data.\nThe DFT transforms a sequence of n-dimensional samples x into a sequence y that reveals how much of each frequency is present in the original data."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/discrete-fourier-transform.html#what-is-the-discrete-fourier-transform-dft",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/discrete-fourier-transform.html#what-is-the-discrete-fourier-transform-dft",
    "title": "Understanding the Discrete Fourier Transform (DFT)",
    "section": "",
    "text": "Imagine you’re listening to music. You hear a melody, but the melody is made up of individual notes (frequencies). The Discrete Fourier Transform (DFT) works similarly—it takes a “melody” of data in the time domain (a sequence of numbers) and breaks it into its “notes” (frequency components). This makes it easier to analyze patterns in signals like sound, images, or other data.\nThe DFT transforms a sequence of n-dimensional samples x into a sequence y that reveals how much of each frequency is present in the original data."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/discrete-fourier-transform.html#mathematical-definition",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/discrete-fourier-transform.html#mathematical-definition",
    "title": "Understanding the Discrete Fourier Transform (DFT)",
    "section": "Mathematical Definition",
    "text": "Mathematical Definition\nGiven an input vector x = [x_0, x_1, \\dots, x_{n-1}]^T, the DFT produces a vector y = [y_0, y_1, \\dots, y_{n-1}]^T, where:\n\ny_k = \\frac{1}{\\sqrt{n}} \\sum_{j=0}^{n-1} x_j \\omega^{jk}, \\quad \\omega = e^{-i \\frac{2\\pi}{n}}, \\quad k = 0, 1, \\dots, n-1"
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/discrete-fourier-transform.html#what-is-omega-and-why-does-omega1--i",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/discrete-fourier-transform.html#what-is-omega-and-why-does-omega1--i",
    "title": "Understanding the Discrete Fourier Transform (DFT)",
    "section": "What is \\omega and Why Does \\omega^1 = -i?",
    "text": "What is \\omega and Why Does \\omega^1 = -i?\n\nStep 1: Define \\omega\nThe root of unity \\omega is defined as:\n\n\\omega = e^{-i \\frac{2\\pi}{n}}\n\nFor n = 4, this becomes:\n\n\\omega = e^{-i \\frac{2\\pi}{4}} = e^{-i \\frac{\\pi}{2}}\n\n\n\nStep 2: Expand e^{-i \\frac{\\pi}{2}} Using Euler’s Formula\nEuler’s formula states:\n\ne^{i\\theta} = \\cos(\\theta) + i \\sin(\\theta)\n\nUsing this for \\omega = e^{-i \\frac{\\pi}{2}}, we get:\n\ne^{-i \\frac{\\pi}{2}} = \\cos\\left(-\\frac{\\pi}{2}\\right) + i \\sin\\left(-\\frac{\\pi}{2}\\right)\n\nFrom the unit circle:\n\n\\cos\\left(-\\frac{\\pi}{2}\\right) = 0,\n\\sin\\left(-\\frac{\\pi}{2}\\right) = -1.\n\nSubstitute these values:\n\ne^{-i \\frac{\\pi}{2}} = 0 - i = -i\n\nThus:\n\n\\omega^1 = -i\n\n\n\nStep 3: Verify Powers of \\omega\nThe powers of \\omega for n = 4 are as follows:\n\n\\omega^0 = e^{-i \\cdot 0} = 1\n\\omega^1 = e^{-i \\frac{\\pi}{2}} = -i\n\\omega^2 = e^{-i \\pi} = -1\n\\omega^3 = e^{-i \\frac{3\\pi}{2}} = i\n\\omega^4 = e^{-i 2\\pi} = 1 (cyclic repetition)\n\nThese powers correspond to four equally spaced points on the unit circle in the complex plane."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/discrete-fourier-transform.html#building-the-fourier-matrix",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/discrete-fourier-transform.html#building-the-fourier-matrix",
    "title": "Understanding the Discrete Fourier Transform (DFT)",
    "section": "Building the Fourier Matrix",
    "text": "Building the Fourier Matrix\nThe Fourier matrix F_n is defined as:\n\nF_n(k, j) = \\frac{1}{\\sqrt{n}} \\omega^{kj}, \\quad k, j = 0, 1, \\dots, n-1\n\nFor n = 4:\n\nCompute \\frac{1}{\\sqrt{4}} = \\frac{1}{2}.\nUse the powers of \\omega to fill the matrix:\n\n\nF_4 = \\frac{1}{2}\n\\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & -i & -1 & i \\\\\n1 & -1 & 1 & -1 \\\\\n1 & i & -1 & -i\n\\end{bmatrix}\n\nEach row corresponds to a frequency k, and each column corresponds to a time sample j."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/discrete-fourier-transform.html#conclusion",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/discrete-fourier-transform.html#conclusion",
    "title": "Understanding the Discrete Fourier Transform (DFT)",
    "section": "Conclusion",
    "text": "Conclusion\nThe key to understanding the Fourier matrix and DFT lies in the properties of \\omega, the primitive root of unity. The matrix formulation provides a systematic way to compute the DFT, while the powers of \\omega determine the contributions of different frequencies in the signal. By breaking the time-domain signal into its frequency components, the DFT reveals hidden patterns and structures in the data."
  },
  {
    "objectID": "mathematics/index.html",
    "href": "mathematics/index.html",
    "title": "MATHEMATICS",
    "section": "",
    "text": "Linear Algebra\nDiscrete Mathematics\nCalculus\nDifferential Equations\nNumerical Analysis"
  },
  {
    "objectID": "mathematics/differential-equations/tests-for-series.html",
    "href": "mathematics/differential-equations/tests-for-series.html",
    "title": "Summary of Tests for Series",
    "section": "",
    "text": "Test\n\n\nSeries\n\n\nConverges\n\n\nDiverges\n\n\nComment\n\n\n\n\n\n\nnth-Term\n\n\n\\sum_{n=1}^{\\infty} a_n\n\n\n\\lim_{n\\to\\infty} a_n \\neq 0\n\n\nThis test cannot be used to show convergence.\n\n\n\n\n\n\nGeometric Series\n\n\n\\sum_{n=0}^{\\infty} ar^n\n\n\nr &lt; 1\n\n\nr \\geq 1\n\n\nSum: S = \\frac{a}{1 - r}\n\n\n\n\nTelescoping Series\n\n\n\\sum_{n=1}^{\\infty} (b_n - b_{n+1})\n\n\n\\lim_{n\\to\\infty} b_n = L\n\n\nSum: S = b_1 - L\n\n\n\n\n\n\np-Series\n\n\n\\sum_{n=1}^{\\infty} \\frac{1}{n^p}\n\n\np &gt; 1\n\n\np \\leq 1\n\n\n\n\n\n\nAlternating Series\n\n\n\\sum_{n=1}^{\\infty} (-1)^{n-1}a_n\n\n\n0 &lt; a_{n+1} \\leq a_n and \\lim_{n\\to\\infty} a_n = 0\n\n\nRemainder: R_N \\leq a_{N+1}\n\n\n\n\n\n\nIntegral\n\n\n\\sum_{n=1}^{\\infty} a_n, \\quad a_n = f(n) \\geq 0\n\n\n\\int f(x) \\, dx converges\n\n\n\\int f(x) \\, dx diverges\n\n\nRemainder: 0 &lt; R_N &lt; \\int_{N}^{\\infty} f(x) \\, dx\n\n\n\n\nRoot\n\n\n\\sum_{n=1}^{\\infty} a_n\n\n\n\\lim_{n\\to\\infty} \\sqrt[n]{a_n} &lt; 1\n\n\n\\lim_{n\\to\\infty} \\sqrt[n]{a_n} &gt; 1\n\n\nTest is inconclusive if \\lim_{n\\to\\infty} \\sqrt[n]{a_n} = 1\n\n\n\n\nRatio\n\n\n\\sum_{n=1}^{\\infty} a_n\n\n\n\\lim_{n\\to\\infty} \\frac{a_{n+1}}{a_n} &lt; 1\n\n\n\\lim_{n\\to\\infty} \\frac{a_{n+1}}{a_n} \\geq 1\n\n\nTest is inconclusive if \\lim_{n\\to\\infty} \\frac{a_{n+1}}{a_n} = 1\n\n\n\n\nDirect Comparison  (a_n, b_n &gt; 0)\n\n\n\\sum_{n=1}^{\\infty} a_n\n\n\n0 &lt; a_n \\leq b_n and \\sum_{n=1}^{\\infty} b_n converges\n\n\n0 &lt; b_n \\leq a_n and \\sum_{n=1}^{\\infty} b_n diverges\n\n\n\n\n\n\nLimit Comparison  (a_n, b_n &gt; 0)\n\n\n\\sum_{n=1}^{\\infty} a_n\n\n\n\\lim_{n\\to\\infty} \\frac{a_n}{b_n} = L &gt; 0 and \\sum_{n=1}^{\\infty} b_n converges\n\n\n\\lim_{n\\to\\infty} \\frac{a_n}{b_n} = L &gt; 0 and \\sum_{n=1}^{\\infty} b_n diverges"
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-sine-function.html",
    "href": "mathematics/differential-equations/power-series/power-series-sine-function.html",
    "title": "Power Series Expansion for Sine Function",
    "section": "",
    "text": "The power series expansion for the sine function is another important result in calculus and analysis. Derived from the Taylor series at x = 0, it provides an exact representation of \\sin(x) as an infinite sum, allowing for precise approximations of its values."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-sine-function.html#overview",
    "href": "mathematics/differential-equations/power-series/power-series-sine-function.html#overview",
    "title": "Power Series Expansion for Sine Function",
    "section": "",
    "text": "The power series expansion for the sine function is another important result in calculus and analysis. Derived from the Taylor series at x = 0, it provides an exact representation of \\sin(x) as an infinite sum, allowing for precise approximations of its values."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-sine-function.html#power-series-for-sine",
    "href": "mathematics/differential-equations/power-series/power-series-sine-function.html#power-series-for-sine",
    "title": "Power Series Expansion for Sine Function",
    "section": "Power Series for Sine",
    "text": "Power Series for Sine\nThe sine function can be expressed as:\n\n\\sin(x) = \\sum_{n=0}^\\infty \\frac{(-1)^n}{(2n+1)!} x^{2n+1}\n\nThis series converges to the true value of \\sin(x) for all x. The first four terms of the series are:\n\n\\sin(x) \\approx x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\frac{x^7}{7!}\n\nwhich simplifies to:\n\n\\sin(x) \\approx x - \\frac{x^3}{6} + \\frac{x^5}{120} - \\frac{x^7}{5040}"
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-sine-function.html#explanation",
    "href": "mathematics/differential-equations/power-series/power-series-sine-function.html#explanation",
    "title": "Power Series Expansion for Sine Function",
    "section": "Explanation",
    "text": "Explanation\n\n1. Taylor Series\nThe power series for \\sin(x) comes from the Taylor series expansion about x = 0:\n\nf(x) = f(0) + f'(0)x + \\frac{f''(0)}{2!}x^2 + \\frac{f^{(3)}(0)}{3!}x^3 + \\cdots\n\nFor \\sin(x), the derivatives follow a cyclic pattern:\n\n\\sin(x)\n\\cos(x)\n-\\sin(x)\n-\\cos(x)\n\nEvaluating these derivatives at x = 0:\n\n\\sin(0) = 0\n\\cos(0) = 1\n-\\sin(0) = 0\n-\\cos(0) = -1\n\nOnly the odd derivatives contribute to the series.\n\n\n2. General Term\nThe general term of the series is:\n\n\\frac{(-1)^n}{(2n+1)!} x^{2n+1}\n\nwhere n is a non-negative integer. This results in a series with only odd powers of x, alternating in sign."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-sine-function.html#examples",
    "href": "mathematics/differential-equations/power-series/power-series-sine-function.html#examples",
    "title": "Power Series Expansion for Sine Function",
    "section": "Examples",
    "text": "Examples\n\nSmall x:\nFor x = 0.1, using the first four terms:\n\n\\sin(0.1) \\approx 0.1 - \\frac{(0.1)^3}{6} + \\frac{(0.1)^5}{120} - \\frac{(0.1)^7}{5040}\n\nThis gives a close approximation to the true value of \\sin(0.1).\nError Reduction:\nAdding more terms improves the approximation by reducing the truncation error."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-sine-function.html#summary",
    "href": "mathematics/differential-equations/power-series/power-series-sine-function.html#summary",
    "title": "Power Series Expansion for Sine Function",
    "section": "Summary",
    "text": "Summary\nThe power series expansion for \\sin(x) is a cornerstone in mathematics:\n\nExact Representation: The infinite series converges to \\sin(x) for all real x.\nApproximation: Truncated series provide practical approximations.\nApplications: Used in numerical analysis, physics, and solving differential equations.\n\nThe first four terms, x - \\frac{x^3}{6} + \\frac{x^5}{120} - \\frac{x^7}{5040}, often provide sufficient accuracy for small x. This series highlights the utility and elegance of representing functions as infinite sums."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-cosine-function.html",
    "href": "mathematics/differential-equations/power-series/power-series-cosine-function.html",
    "title": "Power Series Expansion for Cosine Function",
    "section": "",
    "text": "The power series expansion is a fundamental tool for representing functions as infinite sums. For the cosine function, this expansion provides an exact representation and a practical way to approximate \\cos(x) for any real value of x. The series is derived using the Taylor series centered at x = 0."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-cosine-function.html#overview",
    "href": "mathematics/differential-equations/power-series/power-series-cosine-function.html#overview",
    "title": "Power Series Expansion for Cosine Function",
    "section": "",
    "text": "The power series expansion is a fundamental tool for representing functions as infinite sums. For the cosine function, this expansion provides an exact representation and a practical way to approximate \\cos(x) for any real value of x. The series is derived using the Taylor series centered at x = 0."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-cosine-function.html#power-series-for-cosine",
    "href": "mathematics/differential-equations/power-series/power-series-cosine-function.html#power-series-for-cosine",
    "title": "Power Series Expansion for Cosine Function",
    "section": "Power Series for Cosine",
    "text": "Power Series for Cosine\nThe cosine function can be expressed as:\n\n\\cos(x) = \\sum_{n=0}^\\infty \\frac{(-1)^n}{(2n)!} x^{2n}\n\nThis series converges to the true value of \\cos(x) for all x. The first four terms of the series are:\n\n\\cos(x) \\approx 1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\frac{x^6}{6!}\n\nwhich simplifies to:\n\n\\cos(x) \\approx 1 - \\frac{x^2}{2} + \\frac{x^4}{24} - \\frac{x^6}{720}"
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-cosine-function.html#explanation",
    "href": "mathematics/differential-equations/power-series/power-series-cosine-function.html#explanation",
    "title": "Power Series Expansion for Cosine Function",
    "section": "Explanation",
    "text": "Explanation\n\n1. Taylor Series\nThe power series for \\cos(x) is a special case of the Taylor series expansion about x = 0:\n\nf(x) = f(0) + f'(0)x + \\frac{f''(0)}{2!}x^2 + \\frac{f^{(3)}(0)}{3!}x^3 + \\cdots\n\nFor \\cos(x), the derivatives follow a cyclic pattern:\n\n\\cos(x)\n-\\sin(x)\n-\\cos(x)\n\\sin(x)\n\nEvaluating these derivatives at x = 0:\n\n\\cos(0) = 1\n-\\sin(0) = 0\n-\\cos(0) = -1\n\\sin(0) = 0\n\nOnly the even derivatives contribute to the series.\n\n\n2. General Term\nThe general term of the series is:\n\n\\frac{(-1)^n}{(2n)!} x^{2n}\n\nwhere n is a non-negative integer. This results in a series with only even powers of x, alternating in sign."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-cosine-function.html#examples",
    "href": "mathematics/differential-equations/power-series/power-series-cosine-function.html#examples",
    "title": "Power Series Expansion for Cosine Function",
    "section": "Examples",
    "text": "Examples\n\nSmall x:\nFor x = 0.1, using the first four terms:\n\n\\cos(0.1) \\approx 1 - \\frac{(0.1)^2}{2} + \\frac{(0.1)^4}{24} - \\frac{(0.1)^6}{720}\n\nThis gives a close approximation to the true value of \\cos(0.1).\nError Reduction:\nAdding more terms improves the approximation by reducing the truncation error."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-cosine-function.html#summary",
    "href": "mathematics/differential-equations/power-series/power-series-cosine-function.html#summary",
    "title": "Power Series Expansion for Cosine Function",
    "section": "Summary",
    "text": "Summary\nThe power series expansion for \\cos(x) is an essential concept in calculus:\n\nExact Representation: The series converges to \\cos(x) for all real x.\nApproximation: Truncated series provide practical approximations.\nApplications: Used in numerical analysis, signal processing, and physics.\n\nThe first four terms, 1 - \\frac{x^2}{2} + \\frac{x^4}{24} - \\frac{x^6}{720}, often provide sufficient accuracy for small x. This compact yet precise representation highlights the elegance and utility of the power series."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/index.html",
    "href": "mathematics/differential-equations/power-series/index.html",
    "title": "POWER SERIES",
    "section": "",
    "text": "Quick Reference: Important Power Series Expansions\nPower Series Expansion for Cosine Function\nPower Series Expansion for Sine Function\nPower Series Expansion for e^x\nPower Series Expansion for \\frac{1}{1-x}"
  },
  {
    "objectID": "mathematics/differential-equations/nonhomogeneous-systems-variation-of-parameters.html",
    "href": "mathematics/differential-equations/nonhomogeneous-systems-variation-of-parameters.html",
    "title": "Nonhomogeneous Systems: Variation of Parameters",
    "section": "",
    "text": "When solving nonhomogeneous systems of differential equations, we use variation of parameters to find a particular solution to the system. This approach is especially useful when dealing with systems where a simple guess for the particular solution is not possible due to the nature of the nonhomogeneous term."
  },
  {
    "objectID": "mathematics/differential-equations/nonhomogeneous-systems-variation-of-parameters.html#overview",
    "href": "mathematics/differential-equations/nonhomogeneous-systems-variation-of-parameters.html#overview",
    "title": "Nonhomogeneous Systems: Variation of Parameters",
    "section": "",
    "text": "When solving nonhomogeneous systems of differential equations, we use variation of parameters to find a particular solution to the system. This approach is especially useful when dealing with systems where a simple guess for the particular solution is not possible due to the nature of the nonhomogeneous term."
  },
  {
    "objectID": "mathematics/differential-equations/nonhomogeneous-systems-variation-of-parameters.html#variation-of-parameters",
    "href": "mathematics/differential-equations/nonhomogeneous-systems-variation-of-parameters.html#variation-of-parameters",
    "title": "Nonhomogeneous Systems: Variation of Parameters",
    "section": "Variation of Parameters",
    "text": "Variation of Parameters\nGiven a matrix A of size n \\times n with constant entries, let \\Phi(t) be the fundamental solution matrix of the homogeneous system:\n\n\\mathbf{x}' = A \\mathbf{x}\n\nwhere \\mathbf{b}(t) is a continuous vector function. A particular solution \\mathbf{x}_p to the nonhomogeneous system:\n\n\\mathbf{x}' = A \\mathbf{x} + \\mathbf{b}(t)\n\nis given by:\n\n\\mathbf{x}_p(t) = \\Phi(t) \\int \\Phi^{-1}(t) \\mathbf{b}(t) \\, dt\n\nThis result is an application of the method of variation of parameters for nonhomogeneous systems of differential equations."
  },
  {
    "objectID": "mathematics/differential-equations/nonhomogeneous-systems-variation-of-parameters.html#explanation",
    "href": "mathematics/differential-equations/nonhomogeneous-systems-variation-of-parameters.html#explanation",
    "title": "Nonhomogeneous Systems: Variation of Parameters",
    "section": "Explanation",
    "text": "Explanation\n\nFundamental Solution Matrix \\Phi(t): A matrix whose columns are linearly independent solutions to the homogeneous system \\mathbf{x}' = A \\mathbf{x}.\nNonhomogeneous Term \\mathbf{b}(t): This is the vector function that adds a nonhomogeneous component to the system.\nParticular Solution \\mathbf{x}_p(t): This is the specific solution to the nonhomogeneous system, calculated using the formula above."
  },
  {
    "objectID": "mathematics/differential-equations/nonhomogeneous-systems-variation-of-parameters.html#formula-components",
    "href": "mathematics/differential-equations/nonhomogeneous-systems-variation-of-parameters.html#formula-components",
    "title": "Nonhomogeneous Systems: Variation of Parameters",
    "section": "Formula Components",
    "text": "Formula Components\n\n\\Phi(t): Fundamental solution matrix of the homogeneous system.\n\\Phi^{-1}(t): Inverse of the fundamental solution matrix.\n\\mathbf{b}(t): Nonhomogeneous term.\nIntegral \\int \\Phi^{-1}(t) \\mathbf{b}(t) \\, dt: Computes the effect of the nonhomogeneous term over time, transformed by the inverse of \\Phi(t).\n\nThis approach allows us to find a particular solution \\mathbf{x}_p(t) by integrating the transformed nonhomogeneous term. The total solution to the system is the sum of the general solution to the homogeneous system and this particular solution."
  },
  {
    "objectID": "mathematics/differential-equations/nonhomogeneous-systems-variation-of-parameters.html#summary",
    "href": "mathematics/differential-equations/nonhomogeneous-systems-variation-of-parameters.html#summary",
    "title": "Nonhomogeneous Systems: Variation of Parameters",
    "section": "Summary",
    "text": "Summary\nThe method of variation of parameters provides a systematic way to find a particular solution to nonhomogeneous systems of differential equations. By incorporating the fundamental solution matrix \\Phi(t), it enables us to account for the effects of the nonhomogeneous term \\mathbf{b}(t) and obtain a complete solution to the system."
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transforms.html",
    "href": "mathematics/differential-equations/laplace-transforms.html",
    "title": "Laplace Transforms",
    "section": "",
    "text": "The Laplace Transform is a powerful mathematical tool used to simplify problems involving differential equations, especially in engineering and physics. It converts a function of time f(t) (defined for t \\geq 0) into a function of a complex variable s. This transformation makes it easier to analyze systems in the frequency domain and solve problems involving linear differential equations.\nIn simple terms:\n\nThe Laplace Transform takes a function from the time domain and transforms it into a new function in the s-domain.\nIt simplifies operations like differentiation and integration into algebraic manipulations."
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transforms.html#overview",
    "href": "mathematics/differential-equations/laplace-transforms.html#overview",
    "title": "Laplace Transforms",
    "section": "",
    "text": "The Laplace Transform is a powerful mathematical tool used to simplify problems involving differential equations, especially in engineering and physics. It converts a function of time f(t) (defined for t \\geq 0) into a function of a complex variable s. This transformation makes it easier to analyze systems in the frequency domain and solve problems involving linear differential equations.\nIn simple terms:\n\nThe Laplace Transform takes a function from the time domain and transforms it into a new function in the s-domain.\nIt simplifies operations like differentiation and integration into algebraic manipulations."
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transforms.html#definition",
    "href": "mathematics/differential-equations/laplace-transforms.html#definition",
    "title": "Laplace Transforms",
    "section": "Definition",
    "text": "Definition\nThe Laplace Transform of a function f(t) is defined as:\n\n\\mathcal{L}\\{f(t)\\}(s) = F(s) = \\int_0^\\infty e^{-st} f(t) \\, dt\n\nwhere:\n\nt: Time variable (t \\geq 0).\ns: Complex frequency variable.\ne^{-st}: Exponential decay term.\n\n\nKey Requirements:\n\nf(t) must be piecewise continuous on [0, \\infty).\nf(t) must be of exponential order (i.e., |f(t)| \\leq M e^{\\alpha t} for some M, \\alpha &gt; 0)."
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transforms.html#example-1-transform-of-ft-1",
    "href": "mathematics/differential-equations/laplace-transforms.html#example-1-transform-of-ft-1",
    "title": "Laplace Transforms",
    "section": "Example 1: Transform of f(t) = 1",
    "text": "Example 1: Transform of f(t) = 1\nThe Laplace Transform of the constant function f(t) = 1 is:\n\n\\mathcal{L}\\{1\\}(s) = \\int_0^\\infty e^{-st} \\cdot 1 \\, dt\n\n\nSolution:\n\n\\mathcal{L}\\{1\\}(s) = \\int_0^\\infty e^{-st} \\, dt = \\left[ \\frac{-1}{s} e^{-st} \\right]_0^\\infty\n\nAt t = \\infty, e^{-st} \\to 0 (for s &gt; 0), and at t = 0, e^{-st} = 1. Therefore:\n\n\\mathcal{L}\\{1\\}(s) = \\frac{1}{s}, \\quad s &gt; 0"
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transforms.html#example-2-transform-of-ft-eat",
    "href": "mathematics/differential-equations/laplace-transforms.html#example-2-transform-of-ft-eat",
    "title": "Laplace Transforms",
    "section": "Example 2: Transform of f(t) = e^{at}",
    "text": "Example 2: Transform of f(t) = e^{at}\nThe Laplace Transform of f(t) = e^{at} is:\n\n\\mathcal{L}\\{e^{at}\\}(s) = \\int_0^\\infty e^{-st} e^{at} \\, dt = \\int_0^\\infty e^{-(s-a)t} \\, dt\n\n\nSolution:\nUsing the same steps as before:\n\n\\mathcal{L}\\{e^{at}\\}(s) = \\left[ \\frac{-1}{s-a} e^{-(s-a)t} \\right]_0^\\infty.\n\nAt t = \\infty, e^{-(s-a)t} \\to 0 if s &gt; a, and at t = 0, e^{-(s-a)t} = 1. Therefore:\n\n\\mathcal{L}\\{e^{at}\\}(s) = \\frac{1}{s - a}, \\quad s &gt; a"
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transforms.html#applications-of-the-laplace-transform",
    "href": "mathematics/differential-equations/laplace-transforms.html#applications-of-the-laplace-transform",
    "title": "Laplace Transforms",
    "section": "Applications of the Laplace Transform",
    "text": "Applications of the Laplace Transform\n\nSolving Differential Equations: The Laplace Transform converts differential equations into algebraic equations, which are easier to solve. Once solved, the inverse Laplace Transform is used to return to the time domain.\nControl Systems: It is widely used in engineering to analyze the stability and performance of systems in the frequency domain.\nSignal Processing: The Laplace Transform is used to study signals and their frequency components.\nCircuit Analysis: It simplifies the analysis of electrical circuits with resistors, capacitors, and inductors by converting time-domain functions to the s-domain."
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transforms.html#summary",
    "href": "mathematics/differential-equations/laplace-transforms.html#summary",
    "title": "Laplace Transforms",
    "section": "Summary",
    "text": "Summary\nThe Laplace Transform is a fundamental tool for solving problems in engineering and physics, particularly those involving differential equations. Its key features are:\n\nTransforming functions from the time domain to the s-domain.\nSimplifying operations like differentiation and integration.\nMaking it possible to analyze systems in the frequency domain.\n\n\nCommon Transforms Recap:\n\n\n\nFunction f(t)\nLaplace Transform F(s)\nConditions\n\n\n\n\n1\n\\frac{1}{s}\ns &gt; 0\n\n\nt^n\n\\frac{n!}{s^{n+1}}\ns &gt; 0\n\n\ne^{at}\n\\frac{1}{s - a}\ns &gt; a\n\n\n\\sin(bt)\n\\frac{b}{s^2 + b^2}\ns &gt; 0\n\n\n\\cos(bt)\n\\frac{s}{s^2 + b^2}\ns &gt; 0\n\n\ne^{at} \\sin(bt)\n\\frac{b}{(s-a)^2 + b^2}\ns &gt; a\n\n\ne^{at} \\cos(bt)\n\\frac{s-a}{(s-a)^2 + b^2}\ns &gt; a"
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transform-properties/index.html",
    "href": "mathematics/differential-equations/laplace-transform-properties/index.html",
    "title": "PROPERTIES OF THE LAPLACE TRANSFORM",
    "section": "",
    "text": "Multiplication by t Property (Theorem 5.3.2)\nFirst Derivative Property (Theorem 5.3.4)\nFirst Shifting Property (Theorem 5.3.6)"
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transform-properties/first-derivative-property.html",
    "href": "mathematics/differential-equations/laplace-transform-properties/first-derivative-property.html",
    "title": "Laplace Transform - First Derivative Property",
    "section": "",
    "text": "The First Derivative Property of the Laplace Transform relates the Laplace transform of a derivative f'(t) to the transform of f(t) and its initial value. The property states:\n\n\\mathcal{L}[f'(t)] = s\\mathcal{L}[f(t)] - f(0)\n\nThis theorem is widely used in solving differential equations with given initial conditions."
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transform-properties/first-derivative-property.html#overview",
    "href": "mathematics/differential-equations/laplace-transform-properties/first-derivative-property.html#overview",
    "title": "Laplace Transform - First Derivative Property",
    "section": "",
    "text": "The First Derivative Property of the Laplace Transform relates the Laplace transform of a derivative f'(t) to the transform of f(t) and its initial value. The property states:\n\n\\mathcal{L}[f'(t)] = s\\mathcal{L}[f(t)] - f(0)\n\nThis theorem is widely used in solving differential equations with given initial conditions."
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transform-properties/first-derivative-property.html#proof",
    "href": "mathematics/differential-equations/laplace-transform-properties/first-derivative-property.html#proof",
    "title": "Laplace Transform - First Derivative Property",
    "section": "Proof",
    "text": "Proof\n\nWrite the Laplace Transform of the Derivative\nBy definition, the Laplace transform of f'(t) is:\n\n\\mathcal{L}[f'(t)] = \\int_0^\\infty f'(t) e^{-st} \\, dt\n\n\n\nApply Integration by Parts\nThe integration by parts formula is given by:\n\n\\int u \\, dv = uv - \\int v \\, du\n\nLet:\n\nu = e^{-st} and dv = f'(t) dt,\nThen du = -s e^{-st} dt and v = f(t).\n\nUsing this formula:\n\n\\int_0^\\infty f'(t) e^{-st} \\, dt = \\left[ f(t) e^{-st} \\right]_0^\\infty - \\int_0^\\infty f(t)(-s e^{-st}) \\, dt\n\n\n\nEvaluate the Boundary Terms\n\nAt t = \\infty, f(t) e^{-st} \\to 0, assuming f(t) grows no faster than an exponential function.\nAt t = 0, f(t) e^{-s(0)} = f(0).\n\nThus, the boundary term becomes:\n\n\\left[ f(t) e^{-st} \\right]_0^\\infty = -f(0)\n\n\n\nSimplify the Remaining Integral\nThe remaining integral is:\n\n\\int_0^\\infty s f(t) e^{-st} \\, dt = s \\int_0^\\infty f(t) e^{-st} \\, dt\n\nBy definition, this is:\n\ns \\mathcal{L}[f(t)]\n\n\n\nCombine Results\nSubstituting the results back into the equation:\n\n\\mathcal{L}[f'(t)] = -f(0) + s \\mathcal{L}[f(t)]\n\nRearranging:\n\n\\mathcal{L}[f'(t)] = s\\mathcal{L}[f(t)] - f(0)\n\n\n\nConclusion\nThe proof demonstrates that:\n\n\\mathcal{L}[f'(t)] = s\\mathcal{L}[f(t)] - f(0)"
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transform-properties/first-derivative-property.html#corollary-laplace-transform-of-gt-ft",
    "href": "mathematics/differential-equations/laplace-transform-properties/first-derivative-property.html#corollary-laplace-transform-of-gt-ft",
    "title": "Laplace Transform - First Derivative Property",
    "section": "Corollary: Laplace Transform of g'(t) = f''(t)",
    "text": "Corollary: Laplace Transform of g'(t) = f''(t)\nTo extend the property to g'(t) = f''(t):\n\nDefine g(t) = f'(t). Then the Laplace transform of g'(t) = f''(t) is:\n\n\\mathcal{L}[f''(t)] = \\mathcal{L}[g'(t)]\n\nUsing the First Derivative Property for g(t):\n\n\\mathcal{L}[g'(t)] = s\\mathcal{L}[g(t)] - g(0)\n\nSubstituting g(t) = f'(t) and g(0) = f'(0):\n\n\\mathcal{L}[f''(t)] = s\\mathcal{L}[f'(t)] - f'(0)\n\nSubstitute the First Derivative Property for f'(t):\n\n\\mathcal{L}[f'(t)] = s\\mathcal{L}[f(t)] - f(0)\n\nSubstituting into the equation:\n\n\\mathcal{L}[f''(t)] = s \\big(s\\mathcal{L}[f(t)] - f(0)\\big) - f'(0)\n\nSimplify the expression:\n\n\\mathcal{L}[f''(t)] = s^2\\mathcal{L}[f(t)] - sf(0) - f'(0)\n\n\n\nExample Application\nFor f(t) = t^2, with f'(t) = 2t, f''(t) = 2, and f(0) = 0, f'(0) = 0:\n\nCompute \\mathcal{L}[f(t)] = \\frac{2}{s^3}.\nUsing the corollary:\n\n\\mathcal{L}[f''(t)] = s^2 \\cdot \\frac{2}{s^3} - s \\cdot 0 - 0\n\nSimplify:\n\n\\mathcal{L}[f''(t)] = \\frac{2s^2}{s^3} = \\frac{2}{s}"
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transform-properties/first-derivative-property.html#summary",
    "href": "mathematics/differential-equations/laplace-transform-properties/first-derivative-property.html#summary",
    "title": "Laplace Transform - First Derivative Property",
    "section": "Summary",
    "text": "Summary\nThe First Derivative Property establishes a relationship between the Laplace transform of f'(t) and the transform of f(t). The corollary extends this result to include the second derivative, f''(t) = g'(t), where g(t) = f'(t). These properties are essential for solving second-order differential equations with specified initial conditions."
  },
  {
    "objectID": "mathematics/differential-equations/generalized-eigenvectors-DE.html",
    "href": "mathematics/differential-equations/generalized-eigenvectors-DE.html",
    "title": "Generalized Eigenvectors in Differential Equations",
    "section": "",
    "text": "When solving systems of differential equations, we use eigenvectors to understand how the system evolves over time. Sometimes, however, we encounter a repeated eigenvalue but don’t have enough unique eigenvectors to fully describe the solution. This is where generalized eigenvectors come into play."
  },
  {
    "objectID": "mathematics/differential-equations/generalized-eigenvectors-DE.html#overview",
    "href": "mathematics/differential-equations/generalized-eigenvectors-DE.html#overview",
    "title": "Generalized Eigenvectors in Differential Equations",
    "section": "",
    "text": "When solving systems of differential equations, we use eigenvectors to understand how the system evolves over time. Sometimes, however, we encounter a repeated eigenvalue but don’t have enough unique eigenvectors to fully describe the solution. This is where generalized eigenvectors come into play."
  },
  {
    "objectID": "mathematics/differential-equations/generalized-eigenvectors-DE.html#why-generalized-eigenvectors-are-needed",
    "href": "mathematics/differential-equations/generalized-eigenvectors-DE.html#why-generalized-eigenvectors-are-needed",
    "title": "Generalized Eigenvectors in Differential Equations",
    "section": "Why Generalized Eigenvectors Are Needed",
    "text": "Why Generalized Eigenvectors Are Needed\n\nEigenvectors and Eigenvalues Recap:\n\nFor a matrix A, an eigenvalue \\lambda represents a rate of growth or decay in the system.\nAn eigenvector \\mathbf{v} associated with \\lambda is a direction in which the system evolves according to e^{\\lambda t} \\mathbf{v}.\n\nThe Problem with Repeated Eigenvalues:\n\nWhen an eigenvalue is repeated (e.g., \\lambda = 4 with multiplicity 2), we typically need two linearly independent eigenvectors to capture all directions in the solution space.\nIf there’s only one eigenvector for a repeated eigenvalue, we can’t span the entire space. This is where we turn to generalized eigenvectors.\n\nWhat a Generalized Eigenvector Does:\n\nA generalized eigenvector \\mathbf{v}_g helps fill in the missing direction when there’s only one regular eigenvector.\nTogether, the eigenvector \\mathbf{v} and generalized eigenvector \\mathbf{v}_g create a complete solution by introducing an extra component to the motion, such as t e^{\\lambda t} \\mathbf{v}_g.\nThis additional term allows the solution to “expand” over time, rather than being confined to a single line along \\mathbf{v}."
  },
  {
    "objectID": "mathematics/differential-equations/generalized-eigenvectors-DE.html#how-generalized-eigenvectors-affect-the-solution",
    "href": "mathematics/differential-equations/generalized-eigenvectors-DE.html#how-generalized-eigenvectors-affect-the-solution",
    "title": "Generalized Eigenvectors in Differential Equations",
    "section": "How Generalized Eigenvectors Affect the Solution",
    "text": "How Generalized Eigenvectors Affect the Solution\nWith a regular eigenvector \\mathbf{v} and generalized eigenvector \\mathbf{v}_g:\n\nThe solution takes the form:\n\n\\mathbf{x}(t) = c_1 e^{\\lambda t} \\mathbf{v} + c_2 t e^{\\lambda t} \\mathbf{v}_g\n\nHere:\n\ne^{\\lambda t} \\mathbf{v}: Moves along the direction of \\mathbf{v}.\nt e^{\\lambda t} \\mathbf{v}_g: Adds a “twist” or “shearing” motion that broadens the path, allowing the solution to explore the full space over time."
  },
  {
    "objectID": "mathematics/differential-equations/generalized-eigenvectors-DE.html#visual-intuition",
    "href": "mathematics/differential-equations/generalized-eigenvectors-DE.html#visual-intuition",
    "title": "Generalized Eigenvectors in Differential Equations",
    "section": "Visual Intuition",
    "text": "Visual Intuition\n\nEigenvector Only:\n\nImagine the solution tracing a single line in the direction of \\mathbf{v}, say along the x-axis. Without more directions, the solution can’t expand in other directions.\n\nWith Generalized Eigenvector:\n\nThe term t e^{\\lambda t} \\mathbf{v}_g introduces growth along a new direction, allowing the solution to move out of a single line. This gives a richer motion, often creating a “shearing” or “sweeping” effect in the plane."
  },
  {
    "objectID": "mathematics/differential-equations/generalized-eigenvectors-DE.html#summary",
    "href": "mathematics/differential-equations/generalized-eigenvectors-DE.html#summary",
    "title": "Generalized Eigenvectors in Differential Equations",
    "section": "Summary",
    "text": "Summary\nGeneralized eigenvectors allow us to fully capture the behavior of systems with repeated eigenvalues. They provide additional directions for the solution to evolve, ensuring it spans all necessary dimensions and doesn’t collapse onto a single line."
  },
  {
    "objectID": "mathematics/calculus/multivariable-calculus/vector-calculus/index.html",
    "href": "mathematics/calculus/multivariable-calculus/vector-calculus/index.html",
    "title": "VECTOR CALCULUS",
    "section": "",
    "text": "Vector Fields\nSurface Integrals\nCurl and Divergence of a Vector Field"
  },
  {
    "objectID": "mathematics/calculus/multivariable-calculus/partial-derivatives/index.html",
    "href": "mathematics/calculus/multivariable-calculus/partial-derivatives/index.html",
    "title": "PARTIAL DERIVATIVES",
    "section": "",
    "text": "Partial Derivatives and Notation\nChain Rule for Multivariable Functions\nDirectional Derivatives and Gradients\nTangent Planes and Linear Approximations"
  },
  {
    "objectID": "mathematics/calculus/multivariable-calculus/optimization-in-multiple-valriables/index.html",
    "href": "mathematics/calculus/multivariable-calculus/optimization-in-multiple-valriables/index.html",
    "title": "OPTIMIZATION IN MULTIPLE VARIABLES",
    "section": "",
    "text": "Critical Points and Classification\nLagrange Multipliers"
  },
  {
    "objectID": "mathematics/calculus/multivariable-calculus/double-and-triple-integrals/index.html",
    "href": "mathematics/calculus/multivariable-calculus/double-and-triple-integrals/index.html",
    "title": "DOUBLE AND TRIPLE INTEGRALS",
    "section": "",
    "text": "Setting Up Double Integrals\nTriple Integrals in Rectangular, Cylindrical, and Spherical Coordinates\nApplications of Double Integrals\nApplications of Triple Integrals"
  },
  {
    "objectID": "computer-science/index.html",
    "href": "computer-science/index.html",
    "title": "COMPUTER SCIENCE",
    "section": "",
    "text": "DevOps\nData Science\nMachine Learning"
  },
  {
    "objectID": "computer-science/index.html#sections",
    "href": "computer-science/index.html#sections",
    "title": "COMPUTER SCIENCE",
    "section": "",
    "text": "DevOps\nData Science\nMachine Learning"
  },
  {
    "objectID": "computer-science/devops/version-control/index.html",
    "href": "computer-science/devops/version-control/index.html",
    "title": "VERSION CONTROL",
    "section": "",
    "text": "Branching Strategies\nGit Basics\nWorkflows"
  },
  {
    "objectID": "computer-science/devops/resources/index.html",
    "href": "computer-science/devops/resources/index.html",
    "title": "RESOURCES",
    "section": "",
    "text": "Books\nCommunities\nCourses\nTools and Cheat Sheets"
  },
  {
    "objectID": "computer-science/devops/monitoring-and-logging/index.html",
    "href": "computer-science/devops/monitoring-and-logging/index.html",
    "title": "MONITORING AND LOGGING",
    "section": "",
    "text": "Log Management\nMetrics and Alerts\nPrometheus and Grafana"
  },
  {
    "objectID": "computer-science/devops/introduction-to-devops/index.html",
    "href": "computer-science/devops/introduction-to-devops/index.html",
    "title": "INTORDUCTION TO DEV OPS",
    "section": "",
    "text": "Culture and Benefits\nHistory\nWhat is Devops?"
  },
  {
    "objectID": "computer-science/devops/infrastructure-as-code/index.html",
    "href": "computer-science/devops/infrastructure-as-code/index.html",
    "title": "INFRASTRUCTURE AS CODE",
    "section": "",
    "text": "Best Practices\nCloudformation\nTerraform Basics"
  },
  {
    "objectID": "computer-science/devops/containerization/index.html",
    "href": "computer-science/devops/containerization/index.html",
    "title": "CONTAINERIZATION",
    "section": "",
    "text": "Docker Basics\nKubernetes Basics\nOrchestration"
  },
  {
    "objectID": "computer-science/devops/ci-cd/index.html",
    "href": "computer-science/devops/ci-cd/index.html",
    "title": "CI-CD",
    "section": "",
    "text": "Continuous Deployment\nContinuous Integration\nTools"
  },
  {
    "objectID": "computer-science/devops/advanced-topics/index.html",
    "href": "computer-science/devops/advanced-topics/index.html",
    "title": "ADVANCED TOPICS",
    "section": "",
    "text": "Chaos Engineering\nGitops\nHybrid and Multi-cloud\nsre"
  },
  {
    "objectID": "computer-science/devops/cloud-platforms/index.html",
    "href": "computer-science/devops/cloud-platforms/index.html",
    "title": "CLOUD PLATFORMS",
    "section": "",
    "text": "AWS\nAzure\nGoogle Cloud\nMulti-cloud Strategies"
  },
  {
    "objectID": "computer-science/devops/configuration-management/index.html",
    "href": "computer-science/devops/configuration-management/index.html",
    "title": "Index",
    "section": "",
    "text": "Index\nThis folder contains the following files:\n\nansible\nautomation\npuppet-and-chef"
  },
  {
    "objectID": "computer-science/devops/index.html",
    "href": "computer-science/devops/index.html",
    "title": "DEV OPS",
    "section": "",
    "text": "Introduction to DevOps\nVersion Control\nCI/CD\nInfrastructure as Code\nContainerization\nMonitoring and Logging\nConfiguration Management\nCloud Platforms\nMicroservices and Architecture\nSecurity in DevOps\nAdvanced Topics\nReal-World Applications\nResources"
  },
  {
    "objectID": "computer-science/devops/index.html#sections",
    "href": "computer-science/devops/index.html#sections",
    "title": "DEV OPS",
    "section": "",
    "text": "Introduction to DevOps\nVersion Control\nCI/CD\nInfrastructure as Code\nContainerization\nMonitoring and Logging\nConfiguration Management\nCloud Platforms\nMicroservices and Architecture\nSecurity in DevOps\nAdvanced Topics\nReal-World Applications\nResources"
  },
  {
    "objectID": "computer-science/devops/microservices-and-architecture/index.html",
    "href": "computer-science/devops/microservices-and-architecture/index.html",
    "title": "MICROSERVICES AND ARCHITECTURE",
    "section": "",
    "text": "Challenges\nMicroservices Basics\nService Mesh"
  },
  {
    "objectID": "computer-science/devops/real-world-applications/index.html",
    "href": "computer-science/devops/real-world-applications/index.html",
    "title": "REAL WORLD APPLICATIONS",
    "section": "",
    "text": "Case Studies\nStartups vs Enterprises"
  },
  {
    "objectID": "computer-science/devops/security-in-devops/index.html",
    "href": "computer-science/devops/security-in-devops/index.html",
    "title": "SECUIRTY IN DEVOPS",
    "section": "",
    "text": "Devsecops Basics\nSecurity in CICD\nTools and Practices"
  },
  {
    "objectID": "mathematics/calculus/multivariable-calculus/introduction-to-multivariable-functions/index.html",
    "href": "mathematics/calculus/multivariable-calculus/introduction-to-multivariable-functions/index.html",
    "title": "INTRODUCTION TO MULTIVARIABLE CALCULUS",
    "section": "",
    "text": "Functions of Two or More Variables\nGraphs and Level Curves\nLimits and Continuity in Higher Dimensions"
  },
  {
    "objectID": "mathematics/calculus/multivariable-calculus/vector-calculus/line-integrals/index.html",
    "href": "mathematics/calculus/multivariable-calculus/vector-calculus/line-integrals/index.html",
    "title": "LINE INTEGRALS",
    "section": "",
    "text": "Scalar Line Integrals\nVector Line Integrals"
  },
  {
    "objectID": "mathematics/calculus/multivariable-calculus/vector-calculus/vector-calculus-theorems/index.html",
    "href": "mathematics/calculus/multivariable-calculus/vector-calculus/vector-calculus-theorems/index.html",
    "title": "VECTOR CALCULUS THEOREMS",
    "section": "",
    "text": "Green’s Theorem\nStokes’ Theorem\nDivergence Theorem (Gauss’ Theorem)"
  },
  {
    "objectID": "mathematics/differential-equations/exponential-order.html",
    "href": "mathematics/differential-equations/exponential-order.html",
    "title": "Exponential Order: Definition and Examples",
    "section": "",
    "text": "A function f(t) is said to be of exponential order if its growth is controlled by an exponential function e^{\\alpha t} as t becomes large. This concept is crucial for ensuring the applicability of certain mathematical operations, such as the Laplace Transform, and for understanding the long-term behavior of functions.\nIn simpler terms:\n\nExponential Order: f(t) does not grow too quickly.\n“Too Fast”: The growth of f(t) is slower than or comparable to e^{\\alpha t} for some fixed \\alpha."
  },
  {
    "objectID": "mathematics/differential-equations/exponential-order.html#overview",
    "href": "mathematics/differential-equations/exponential-order.html#overview",
    "title": "Exponential Order: Definition and Examples",
    "section": "",
    "text": "A function f(t) is said to be of exponential order if its growth is controlled by an exponential function e^{\\alpha t} as t becomes large. This concept is crucial for ensuring the applicability of certain mathematical operations, such as the Laplace Transform, and for understanding the long-term behavior of functions.\nIn simpler terms:\n\nExponential Order: f(t) does not grow too quickly.\n“Too Fast”: The growth of f(t) is slower than or comparable to e^{\\alpha t} for some fixed \\alpha."
  },
  {
    "objectID": "mathematics/differential-equations/exponential-order.html#definition",
    "href": "mathematics/differential-equations/exponential-order.html#definition",
    "title": "Exponential Order: Definition and Examples",
    "section": "Definition",
    "text": "Definition\nFormally, a function f(t) is of exponential order \\alpha if there exist constants M &gt; 0 and t_0 \\geq 0 such that:\n\n|f(t)| \\leq M e^{\\alpha t}, \\quad \\text{for all } t \\geq t_0\n\n\nKey Components:\n\nBounded Growth: For t \\geq t_0, the magnitude of f(t) does not exceed M e^{\\alpha t}.\nConstants:\n\nM: A positive constant that bounds f(t).\n\\alpha: The exponential growth rate that controls the bound.\nt_0: The threshold after which the bound holds.\n\n\nThis definition ensures that f(t) does not grow faster than e^{\\alpha t} for sufficiently large t."
  },
  {
    "objectID": "mathematics/differential-equations/exponential-order.html#understanding-too-fast-growth",
    "href": "mathematics/differential-equations/exponential-order.html#understanding-too-fast-growth",
    "title": "Exponential Order: Definition and Examples",
    "section": "Understanding “Too Fast” Growth",
    "text": "Understanding “Too Fast” Growth\nWhen we describe a function as growing “too fast” to be of exponential order, we mean that it outpaces the growth of any exponential function e^{\\alpha t}, regardless of how large \\alpha is chosen. For instance, functions like e^{t^2} grow so rapidly that no exponential bound e^{\\alpha t} can contain them for all large t.\n\nIntuition:\n\nExponential Functions: e^{\\alpha t} grow rapidly, but many functions (e.g., polynomials) grow slower or at a similar rate for large t.\nSuper-Exponential Functions: Functions like e^{t^2} grow faster than any e^{\\alpha t} because the exponent itself grows faster (e.g., t^2 vs. \\alpha t).\n\nIf a function grows faster than e^{\\alpha t}, it cannot be bounded by any exponential function, and thus, it is not of exponential order.\n\n\nExample: Comparing e^{t^2} and e^{\\alpha t}\nConsider f(t) = e^{t^2} and g(t) = e^{\\alpha t} with \\alpha = 2.\n\nAt t = 1:\n\ne^{t^2} = e^1 \\approx 2.718, \\quad e^{\\alpha t} = e^2 \\approx 7.389\n\nHere, e^{t^2} &lt; e^{\\alpha t}.\nAt t = 2:\n\ne^{t^2} = e^4 \\approx 54.598, \\quad e^{\\alpha t} = e^4 \\approx 54.598\n\nNow, e^{t^2} = e^{\\alpha t}.\nAt t = 3: \ne^{t^2} = e^9 \\approx 8103, \\quad e^{\\alpha t} = e^6 \\approx 403\n Here, e^{t^2} &gt; e^{\\alpha t}.\n\nAs t \\to \\infty, e^{t^2} grows much faster than e^{\\alpha t}, no matter how large \\alpha is. This demonstrates that f(t) = e^{t^2} is not of exponential order."
  },
  {
    "objectID": "mathematics/differential-equations/exponential-order.html#example-1-polynomial-function-ft-t2",
    "href": "mathematics/differential-equations/exponential-order.html#example-1-polynomial-function-ft-t2",
    "title": "Exponential Order: Definition and Examples",
    "section": "Example 1: Polynomial Function f(t) = t^2",
    "text": "Example 1: Polynomial Function f(t) = t^2\nLet’s determine if f(t) = t^2 is of exponential order.\n\nStep 1: Analyze the Growth of f(t)\nThe function f(t) = t^2 grows quadratically as t \\to \\infty. In contrast, an exponential function like e^{\\alpha t} grows much faster for large t. This suggests that t^2 might be bounded by e^{\\alpha t} for some \\alpha &gt; 0.\n\n\nStep 2: Choose \\alpha and M\n\nSelect \\alpha = 1: A reasonable starting point for the exponential growth rate.\nDetermine M: We aim to find M such that t^2 \\leq M e^t for all t \\geq t_0.\n\n\n\nStep 3: Verify the Bound\n\nAt t = 3:\n\nt^2 = 9, \\quad e^t \\approx 20.09\n\nClearly, t^2 \\leq e^t.\nAs t increases: e^t grows significantly faster than t^2, ensuring that t^2 \\leq M e^t for some M &gt; 0.\n\nConclusion: For t \\geq t_0 = 3, we can choose M = 1 and \\alpha = 1 to satisfy:\n\n|f(t)| \\leq M e^{\\alpha t}\n\nTherefore, f(t) = t^2 is of exponential order with \\alpha = 1."
  },
  {
    "objectID": "mathematics/differential-equations/exponential-order.html#example-2-exponential-function-ft-et2",
    "href": "mathematics/differential-equations/exponential-order.html#example-2-exponential-function-ft-et2",
    "title": "Exponential Order: Definition and Examples",
    "section": "Example 2: Exponential Function f(t) = e^{t^2}",
    "text": "Example 2: Exponential Function f(t) = e^{t^2}\nNow, let’s examine whether f(t) = e^{t^2} is of exponential order.\n\nStep 1: Analyze the Growth of f(t)\nThe function f(t) = e^{t^2} grows much faster than any exponential function e^{\\alpha t} as t \\to \\infty.\n\n\nStep 2: Attempt to Establish the Bound\nAssume, for contradiction, that e^{t^2} \\leq M e^{\\alpha t}. Taking the natural logarithm of both sides:\n\nt^2 \\leq \\ln(M) + \\alpha t\n\nFor large t, the left side (t^2) grows faster than the right side (\\ln(M) + \\alpha t), making the inequality impossible to satisfy.\n\n\nConclusion\nNo constants M &gt; 0 and \\alpha &gt; 0 can ensure |f(t)| \\leq M e^{\\alpha t} for all sufficiently large t. Therefore, f(t) = e^{t^2} is not of exponential order."
  },
  {
    "objectID": "mathematics/differential-equations/exponential-order.html#why-use-exponential-bounds",
    "href": "mathematics/differential-equations/exponential-order.html#why-use-exponential-bounds",
    "title": "Exponential Order: Definition and Examples",
    "section": "Why Use Exponential Bounds?",
    "text": "Why Use Exponential Bounds?\nExponential functions e^{\\alpha t} serve as a benchmark for measuring the rate of growth of other functions:\n\nControlled Growth: If a function grows slower than or at the same rate as e^{\\alpha t}, its growth is considered manageable, making it suitable for operations like the Laplace Transform.\nUncontrolled Growth: If a function grows faster than e^{\\alpha t}, its growth becomes unmanageable, rendering it unsuitable for certain mathematical techniques.\n\n\nSummary:\n\nFunctions Growing “Too Fast”: These outpace exponential functions e^{\\alpha t} and are not of exponential order.\nFunctions of Exponential Order: These grow slower than or similarly to e^{\\alpha t}, ensuring their growth is under control.\n\nUnderstanding whether a function is of exponential order is essential for determining the applicability of various mathematical tools and for analyzing the function’s long-term behavior."
  },
  {
    "objectID": "mathematics/differential-equations/index.html",
    "href": "mathematics/differential-equations/index.html",
    "title": "DIFFERENTIAL EQUATIONS",
    "section": "",
    "text": "Generalized Eigenvectors in Differential Equations\nStability of the Origin in 2x2 Systems of Differential Equations\nNonhomogeneous Systems: Undetermined Coefficients\nNonhomogeneous Systems: Variation of Parameters\nExponential Order\nLaplace Transforms\nProperties of the Laplace Transform\nSummary of Tests for Series\nPower Series"
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transform-properties/first-shifting-property.html",
    "href": "mathematics/differential-equations/laplace-transform-properties/first-shifting-property.html",
    "title": "Laplace Transform - First Shifting Property",
    "section": "",
    "text": "The First Shifting Property of the Laplace Transform relates the Laplace transform of the product e^{at} f(t) to the transform of f(t). The property states:\n\n\\mathcal{L}[e^{at} f(t)] = F(s - a)\n\nwhere F(s) = \\mathcal{L}[f(t)] and a is any real number. This property is useful for analyzing systems involving exponential growth or decay."
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transform-properties/first-shifting-property.html#overview",
    "href": "mathematics/differential-equations/laplace-transform-properties/first-shifting-property.html#overview",
    "title": "Laplace Transform - First Shifting Property",
    "section": "",
    "text": "The First Shifting Property of the Laplace Transform relates the Laplace transform of the product e^{at} f(t) to the transform of f(t). The property states:\n\n\\mathcal{L}[e^{at} f(t)] = F(s - a)\n\nwhere F(s) = \\mathcal{L}[f(t)] and a is any real number. This property is useful for analyzing systems involving exponential growth or decay."
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transform-properties/first-shifting-property.html#statement-of-the-theorem",
    "href": "mathematics/differential-equations/laplace-transform-properties/first-shifting-property.html#statement-of-the-theorem",
    "title": "Laplace Transform - First Shifting Property",
    "section": "Statement of the Theorem",
    "text": "Statement of the Theorem\nLet f(t) be an acceptable function such that \\mathcal{L}[f(t)] = F(s). Then for any real value a:\n\n\\mathcal{L}[e^{at} f(t)] = F(s - a)"
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transform-properties/first-shifting-property.html#proof",
    "href": "mathematics/differential-equations/laplace-transform-properties/first-shifting-property.html#proof",
    "title": "Laplace Transform - First Shifting Property",
    "section": "Proof",
    "text": "Proof\n\nWrite the Laplace Transform of e^{at} f(t)\nBy definition of the Laplace transform:\n\n\\mathcal{L}[e^{at} f(t)] = \\int_0^\\infty e^{at} f(t) e^{-st} \\, dt\n\n\n\nCombine Exponentials\nCombine the exponential terms e^{at} and e^{-st}:\n\ne^{at} e^{-st} = e^{-(s - a)t}\n\nSubstituting into the integral:\n\n\\mathcal{L}[e^{at} f(t)] = \\int_0^\\infty f(t) e^{-(s - a)t} \\, dt\n\n\n\nRecognize the Laplace Transform\nThe integral \\int_0^\\infty f(t) e^{-(s - a)t} \\, dt is the definition of the Laplace transform of f(t), evaluated at s - a. Therefore:\n\n\\mathcal{L}[e^{at} f(t)] = F(s - a)\n\n\n\nConclusion\nThis demonstrates that:\n\n\\mathcal{L}[e^{at} f(t)] = F(s - a)"
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transform-properties/first-shifting-property.html#example",
    "href": "mathematics/differential-equations/laplace-transform-properties/first-shifting-property.html#example",
    "title": "Laplace Transform - First Shifting Property",
    "section": "Example",
    "text": "Example\nTo illustrate, let f(t) = t, for which \\mathcal{L}[f(t)] = \\frac{1}{s^2}. Using the First Shifting Property:\n\nCompute \\mathcal{L}[e^{3t} t]:\n\nSubstitute a = 3 and F(s) = \\frac{1}{s^2}.\n\n\n\\mathcal{L}[e^{3t} t] = F(s - 3) = \\frac{1}{(s - 3)^2}\n\nResult:\n\nThe Laplace transform of e^{3t} t is \\frac{1}{(s - 3)^2}."
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transform-properties/first-shifting-property.html#summary",
    "href": "mathematics/differential-equations/laplace-transform-properties/first-shifting-property.html#summary",
    "title": "Laplace Transform - First Shifting Property",
    "section": "Summary",
    "text": "Summary\nThe First Shifting Property of the Laplace Transform provides a direct relationship between the transform of e^{at} f(t) and the transform of f(t). By shifting s in F(s) by a, this property simplifies the analysis of functions involving exponential factors."
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transform-properties/multiplication-by-t-property.html",
    "href": "mathematics/differential-equations/laplace-transform-properties/multiplication-by-t-property.html",
    "title": "Laplace Transform - Multiplication by t Property",
    "section": "",
    "text": "The Multiplication by t Property of the Laplace Transform relates the Laplace transform of tf(t) to the derivative of the transform F(s) = \\mathcal{L}[f(t)]. The property states:\n\n\\mathcal{L}[tf(t)] = -\\frac{d}{ds}F(s)\n\nThis property is useful for analyzing systems in the Laplace domain that involve multiplication by t."
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transform-properties/multiplication-by-t-property.html#overview",
    "href": "mathematics/differential-equations/laplace-transform-properties/multiplication-by-t-property.html#overview",
    "title": "Laplace Transform - Multiplication by t Property",
    "section": "",
    "text": "The Multiplication by t Property of the Laplace Transform relates the Laplace transform of tf(t) to the derivative of the transform F(s) = \\mathcal{L}[f(t)]. The property states:\n\n\\mathcal{L}[tf(t)] = -\\frac{d}{ds}F(s)\n\nThis property is useful for analyzing systems in the Laplace domain that involve multiplication by t."
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transform-properties/multiplication-by-t-property.html#leibnizs-rule",
    "href": "mathematics/differential-equations/laplace-transform-properties/multiplication-by-t-property.html#leibnizs-rule",
    "title": "Laplace Transform - Multiplication by t Property",
    "section": "Leibniz’s Rule",
    "text": "Leibniz’s Rule\nLeibniz’s rule allows differentiation under the integral sign. For a function K(s, t), under reasonable hypotheses, Leibniz’s rule states:\n\n\\frac{d}{ds} \\int_{t=a}^{t=b} K(s, t) \\, dt = \\int_{t=a}^{t=b} \\frac{\\partial}{\\partial s} [K(s, t)] \\, dt\n\nLeibniz’s rule is essential for justifying the steps in the proof."
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transform-properties/multiplication-by-t-property.html#proof",
    "href": "mathematics/differential-equations/laplace-transform-properties/multiplication-by-t-property.html#proof",
    "title": "Laplace Transform - Multiplication by t Property",
    "section": "Proof",
    "text": "Proof\n\nWrite the Laplace Transform\nThe Laplace transform of f(t) is defined as:\n\nF(s) = \\mathcal{L}[f(t)] = \\int_0^\\infty f(t) e^{-st} \\, dt\n\nThe goal is to compute \\mathcal{L}[tf(t)], given by:\n\n\\mathcal{L}[tf(t)] = \\int_0^\\infty t f(t) e^{-st} \\, dt\n\n\n\nApply Leibniz’s Rule\nLet K(s, t) = f(t) e^{-st}. Using Leibniz’s rule, the derivative of F(s) with respect to s is:\n\n\\frac{d}{ds} F(s) = \\frac{d}{ds} \\int_0^\\infty f(t) e^{-st} \\, dt = \\int_0^\\infty \\frac{\\partial}{\\partial s} \\big[f(t) e^{-st}\\big] \\, dt\n\n\n\nDifferentiate Inside the Integral\nThe partial derivative of f(t) e^{-st} with respect to s is:\n\n\\frac{\\partial}{\\partial s} \\big[f(t) e^{-st}\\big] = -t f(t) e^{-st}\n\nSubstituting this into the integral gives:\n\n\\frac{d}{ds} F(s) = \\int_0^\\infty -t f(t) e^{-st} \\, dt\n\n\n\nRecognize the Laplace Transform\nThe integral on the right-hand side is the definition of \\mathcal{L}[tf(t)]. Therefore:\n\n\\frac{d}{ds} F(s) = -\\mathcal{L}[tf(t)]\n\nRearranging:\n\n\\mathcal{L}[tf(t)] = -\\frac{d}{ds} F(s)\n\n\n\nConclusion\nThe proof demonstrates that:\n\n\\mathcal{L}[tf(t)] = -\\frac{d}{ds} \\mathcal{L}[f(t)]"
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transform-properties/multiplication-by-t-property.html#corollary-laplace-transform-of-tn-ft",
    "href": "mathematics/differential-equations/laplace-transform-properties/multiplication-by-t-property.html#corollary-laplace-transform-of-tn-ft",
    "title": "Laplace Transform - Multiplication by t Property",
    "section": "Corollary: Laplace Transform of t^n f(t)",
    "text": "Corollary: Laplace Transform of t^n f(t)\nBuilding on Theorem 5.3.2, the Laplace transform of t^n f(t) can be expressed as:\n\n\\mathcal{L}[t^n f(t)] = (-1)^n F^{(n)}(s)\n\nwhere F^{(n)}(s) is the n-th derivative of F(s). This property generalizes the Multiplication by t Property to higher powers of t.\n\nDemonstration\nTo demonstrate the corollary for n = 2, consider t^2 f(t):\n\nApply Theorem 5.3.2 for tf(t):\n\n\\mathcal{L}[tf(t)] = -\\frac{d}{ds}F(s)\n\nApply Theorem 5.3.2 again for t^2 f(t) = t \\cdot (tf(t)):\n\n\\mathcal{L}[t^2 f(t)] = -\\frac{d}{ds} \\big( \\mathcal{L}[tf(t)] \\big) = -\\frac{d}{ds} \\big( -\\frac{d}{ds} F(s) \\big)\n\nSimplify the expression:\n\n\\mathcal{L}[t^2 f(t)] = \\frac{d^2}{ds^2} F(s) = F^{(2)}(s)\n\nIncorporate the factor (-1)^2 for consistency with the corollary:\n\n\\mathcal{L}[t^2 f(t)] = F^{(2)}(s) = (-1)^2 F^{(2)}(s)\n\n\n\n\nGeneral Case\nThe process can be repeated for any positive integer n to show that:\n\n\\mathcal{L}[t^n f(t)] = (-1)^n F^{(n)}(s)"
  },
  {
    "objectID": "mathematics/differential-equations/laplace-transform-properties/multiplication-by-t-property.html#summary",
    "href": "mathematics/differential-equations/laplace-transform-properties/multiplication-by-t-property.html#summary",
    "title": "Laplace Transform - Multiplication by t Property",
    "section": "Summary",
    "text": "Summary\nThe Multiplication by t Property provides a foundation for analyzing functions scaled by t or higher powers of t. The corollary extends the result to t^n f(t), relating it to the n-th derivative of the Laplace transform of f(t). This result is critical in applications requiring repeated differentiation in the s-domain."
  },
  {
    "objectID": "mathematics/differential-equations/nonhomogeneous-systems-undetermined-coefficients.html",
    "href": "mathematics/differential-equations/nonhomogeneous-systems-undetermined-coefficients.html",
    "title": "Nonhomogeneous Systems: Undetermined Coefficients",
    "section": "",
    "text": "When solving nonhomogeneous systems of differential equations, one common method to find a particular solution is undetermined coefficients. This approach works well when the nonhomogeneous term is of a specific form, such as polynomials, exponentials, sines, or cosines."
  },
  {
    "objectID": "mathematics/differential-equations/nonhomogeneous-systems-undetermined-coefficients.html#overview",
    "href": "mathematics/differential-equations/nonhomogeneous-systems-undetermined-coefficients.html#overview",
    "title": "Nonhomogeneous Systems: Undetermined Coefficients",
    "section": "",
    "text": "When solving nonhomogeneous systems of differential equations, one common method to find a particular solution is undetermined coefficients. This approach works well when the nonhomogeneous term is of a specific form, such as polynomials, exponentials, sines, or cosines."
  },
  {
    "objectID": "mathematics/differential-equations/nonhomogeneous-systems-undetermined-coefficients.html#method-of-undetermined-coefficients",
    "href": "mathematics/differential-equations/nonhomogeneous-systems-undetermined-coefficients.html#method-of-undetermined-coefficients",
    "title": "Nonhomogeneous Systems: Undetermined Coefficients",
    "section": "Method of Undetermined Coefficients",
    "text": "Method of Undetermined Coefficients\nConsider a system of differential equations given by:\n\n\\mathbf{x}' = A \\mathbf{x} + \\mathbf{b}(t)\n\nwhere A is an n \\times n matrix with constant entries, and \\mathbf{b}(t) is a nonhomogeneous term with a form that allows us to apply the method of undetermined coefficients.\nThe method involves guessing a form for the particular solution \\mathbf{x}_p(t) based on the form of \\mathbf{b}(t) and then determining the coefficients by substituting \\mathbf{x}_p(t) back into the differential equation."
  },
  {
    "objectID": "mathematics/differential-equations/nonhomogeneous-systems-undetermined-coefficients.html#steps-for-applying-undetermined-coefficients",
    "href": "mathematics/differential-equations/nonhomogeneous-systems-undetermined-coefficients.html#steps-for-applying-undetermined-coefficients",
    "title": "Nonhomogeneous Systems: Undetermined Coefficients",
    "section": "Steps for Applying Undetermined Coefficients",
    "text": "Steps for Applying Undetermined Coefficients\n\nIdentify the Form of \\mathbf{b}(t): Determine the type of functions in the nonhomogeneous term \\mathbf{b}(t). Typical forms include:\n\nPolynomials: \\mathbf{b}(t) = t^n \\mathbf{c}\nExponentials: \\mathbf{b}(t) = e^{rt} \\mathbf{c}\nSines and Cosines: \\mathbf{b}(t) = \\sin(\\theta t) \\mathbf{c} or \\cos(\\theta t) \\mathbf{c}\n\nUse the Table to Choose a Guess for \\mathbf{x}_p(t): Based on the form of \\mathbf{b}(t), select a guess for \\mathbf{x}_p(t) using the table below. If the guess overlaps with solutions to the homogeneous system, multiply by t as needed to create a new linearly independent solution.\n\n\n\n\n\nForm of \\mathbf{b}(t)\n\n\nGuess for \\mathbf{x}_p(t)\n\n\nAdditional Guess (if overlaps with homogeneous solution)\n\n\n\n\n\\mathbf{b}(t) = \\mathbf{c}\n\n\n\\mathbf{x}_p = \\mathbf{a}\n\n\n\\mathbf{x}_p = t \\mathbf{a}\n\n\n\n\n\\mathbf{b}(t) = t \\mathbf{c}\n\n\n\\mathbf{x}_p = \\mathbf{a}_1 t + \\mathbf{a}_0\n\n\n\\mathbf{x}_p = t^2 \\mathbf{a}_1 + t \\mathbf{a}_0\n\n\n\n\n\\mathbf{b}(t) = t^n \\mathbf{c}\n\n\n\\mathbf{x}_p = \\mathbf{a}_n t^n + \\dots + \\mathbf{a}_0\n\n\n\\mathbf{x}_p = t^{n+1} \\mathbf{a}_n + \\dots + t \\mathbf{a}_0\n\n\n\n\n\\mathbf{b}(t) = e^{rt} \\mathbf{c}\n\n\n\\mathbf{x}_p = e^{rt} \\mathbf{a}\n\n\n\\mathbf{x}_p = t e^{rt} \\mathbf{a}\n\n\n\n\n\\mathbf{b}(t) = t^n e^{rt} \\mathbf{c}\n\n\n\\mathbf{x}_p = e^{rt} (\\mathbf{a}_n t^n + \\dots + \\mathbf{a}_0)\n\n\n\\mathbf{x}_p = t^{n+1} e^{rt} (\\mathbf{a}_n t^n + \\dots + \\mathbf{a}_0)\n\n\n\n\n\\mathbf{b}(t) = \\cos(\\theta t) \\mathbf{c}\n\n\n\\mathbf{x}_p = \\mathbf{a}_1 \\cos(\\theta t) + \\mathbf{a}_2 \\sin(\\theta t)\n\n\n\\mathbf{x}_p = t (\\mathbf{a}_1 \\cos(\\theta t) + \\mathbf{a}_2 \\sin(\\theta t))\n\n\n\n\n\\mathbf{b}(t) = \\sin(\\theta t) \\mathbf{c}\n\n\n\\mathbf{x}_p = \\mathbf{a}_1 \\cos(\\theta t) + \\mathbf{a}_2 \\sin(\\theta t)\n\n\n\\mathbf{x}_p = t (\\mathbf{a}_1 \\cos(\\theta t) + \\mathbf{a}_2 \\sin(\\theta t))\n\n\n\n\n\\mathbf{b}(t) = t^n \\cos(\\theta t) \\mathbf{c}\n\n\n\\mathbf{x}_p = ( \\mathbf{a}_n t^n + \\dots + \\mathbf{a}_0 ) \\cos(\\theta t)  + ( \\mathbf{a}_{n+1} t^n + \\dots + \\mathbf{a}_1 ) \\sin(\\theta t)\n\n\n\\mathbf{x}_p = t^{n+1} \\big[ ( \\mathbf{a}_n t^n + \\dots + \\mathbf{a}_0 ) \\cos(\\theta t)  + ( \\mathbf{a}_{n+1} t^n + \\dots + \\mathbf{a}_1 ) \\sin(\\theta t) \\big]\n\n\n\n\n\\mathbf{b}(t) = t^n \\sin(\\theta t) \\mathbf{c}\n\n\n\\mathbf{x}_p = ( \\mathbf{a}_n t^n + \\dots + \\mathbf{a}_0 ) \\cos(\\theta t)  + ( \\mathbf{a}_{n+1} t^n + \\dots + \\mathbf{a}_1 ) \\sin(\\theta t)\n\n\n\\mathbf{x}_p = t^{n+1} \\big[ ( \\mathbf{a}_n t^n + \\dots + \\mathbf{a}_0 ) \\cos(\\theta t)  + ( \\mathbf{a}_{n+1} t^n + \\dots + \\mathbf{a}_1 ) \\sin(\\theta t) \\big]\n\n\n\n\nSubstitute and Solve for Coefficients: Substitute \\mathbf{x}_p(t) into the differential equation and solve for the unknown coefficients to ensure that the equation is satisfied."
  },
  {
    "objectID": "mathematics/differential-equations/nonhomogeneous-systems-undetermined-coefficients.html#example",
    "href": "mathematics/differential-equations/nonhomogeneous-systems-undetermined-coefficients.html#example",
    "title": "Nonhomogeneous Systems: Undetermined Coefficients",
    "section": "Example",
    "text": "Example\nFor a system:\n\n\\mathbf{x}' = A \\mathbf{x} + e^{rt} \\mathbf{c}\n\nwe might guess:\n\n\\mathbf{x}_p(t) = e^{rt} \\mathbf{a}\n\nIf this guess overlaps with the homogeneous solution, we would use \\mathbf{x}_p(t) = t e^{rt} \\mathbf{a} instead. Substituting \\mathbf{x}_p(t) into the system, we determine \\mathbf{a} by equating terms."
  },
  {
    "objectID": "mathematics/differential-equations/nonhomogeneous-systems-undetermined-coefficients.html#summary",
    "href": "mathematics/differential-equations/nonhomogeneous-systems-undetermined-coefficients.html#summary",
    "title": "Nonhomogeneous Systems: Undetermined Coefficients",
    "section": "Summary",
    "text": "Summary\nThe method of undetermined coefficients provides a straightforward way to find a particular solution to nonhomogeneous systems when \\mathbf{b}(t) has certain forms. By using the table to make an educated guess for \\mathbf{x}_p(t) and solving for unknown coefficients, we can construct a particular solution to complete the system’s solution."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/important-power-series-expansions.html",
    "href": "mathematics/differential-equations/power-series/important-power-series-expansions.html",
    "title": "Quick Reference: Important Power Series Expansions",
    "section": "",
    "text": "This reference provides the most common and important power series expansions for rapid access. These series are fundamental in mathematics and appear in calculus, physics, and engineering applications.\n\n\n\ne^x = \\sum_{n=0}^\\infty \\frac{x^n}{n!}\n\nFirst Four Terms:\n\ne^x \\approx 1 + x + \\frac{x^2}{2} + \\frac{x^3}{6}\n\nRadius of Convergence:\nConverges for all x.\n\n\n\n\n\\sin(x) = \\sum_{n=0}^\\infty \\frac{(-1)^n}{(2n+1)!} x^{2n+1}\n\nFirst Four Terms:\n\n\\sin(x) \\approx x - \\frac{x^3}{6} + \\frac{x^5}{120} - \\frac{x^7}{5040}\n\nRadius of Convergence:\nConverges for all x.\n\n\n\n\n\\cos(x) = \\sum_{n=0}^\\infty \\frac{(-1)^n}{(2n)!} x^{2n}\n\nFirst Four Terms:\n\n\\cos(x) \\approx 1 - \\frac{x^2}{2} + \\frac{x^4}{24} - \\frac{x^6}{720}\n\nRadius of Convergence:\nConverges for all x.\n\n\n\n\n\\frac{1}{1-x} = \\sum_{n=0}^\\infty x^n, \\quad |x| &lt; 1\n\nFirst Four Terms:\n\n\\frac{1}{1-x} \\approx 1 + x + x^2 + x^3\n\nRadius of Convergence:\nConverges for |x| &lt; 1.\n\n\n\n\n\\ln(1+x) = \\sum_{n=1}^\\infty (-1)^{n+1} \\frac{x^n}{n}, \\quad |x| &lt; 1\n\nFirst Four Terms:\n\n\\ln(1+x) \\approx x - \\frac{x^2}{2} + \\frac{x^3}{3} - \\frac{x^4}{4}\n\nRadius of Convergence:\nConverges for |x| &lt; 1.\n\n\n\n\n\\arctan(x) = \\sum_{n=0}^\\infty (-1)^n \\frac{x^{2n+1}}{2n+1}, \\quad |x| \\leq 1\n\nFirst Four Terms:\n\n\\arctan(x) \\approx x - \\frac{x^3}{3} + \\frac{x^5}{5} - \\frac{x^7}{7}\n\nRadius of Convergence:\nConverges for |x| \\leq 1.\n\n\n\n\n(1+x)^k = \\sum_{n=0}^\\infty \\binom{k}{n} x^n, \\quad |x| &lt; 1\n\nWhere:\n\n\\binom{k}{n} = \\frac{k(k-1)(k-2)\\cdots(k-n+1)}{n!}\n\nFirst Four Terms:\n\n(1+x)^k \\approx 1 + kx + \\frac{k(k-1)}{2}x^2 + \\frac{k(k-1)(k-2)}{6}x^3\n\nRadius of Convergence:\nConverges for |x| &lt; 1 unless k is a non-negative integer, in which case it converges for all x.\n\n\n\nThis collection of power series expansions provides a quick reference for commonly used functions:\n\nUniversal: e^x, \\sin(x), \\cos(x) converge for all x.\nRestricted Radius: \\frac{1}{1-x}, \\ln(1+x), \\arctan(x), (1+x)^k converge only for |x| &lt; 1 or specific conditions.\n\nThese expansions are critical tools in analysis, numerical computation, and approximations."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/important-power-series-expansions.html#exponential-function-ex",
    "href": "mathematics/differential-equations/power-series/important-power-series-expansions.html#exponential-function-ex",
    "title": "Quick Reference: Important Power Series Expansions",
    "section": "",
    "text": "e^x = \\sum_{n=0}^\\infty \\frac{x^n}{n!}\n\nFirst Four Terms:\n\ne^x \\approx 1 + x + \\frac{x^2}{2} + \\frac{x^3}{6}\n\nRadius of Convergence:\nConverges for all x."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/important-power-series-expansions.html#sine-function-sinx",
    "href": "mathematics/differential-equations/power-series/important-power-series-expansions.html#sine-function-sinx",
    "title": "Quick Reference: Important Power Series Expansions",
    "section": "",
    "text": "\\sin(x) = \\sum_{n=0}^\\infty \\frac{(-1)^n}{(2n+1)!} x^{2n+1}\n\nFirst Four Terms:\n\n\\sin(x) \\approx x - \\frac{x^3}{6} + \\frac{x^5}{120} - \\frac{x^7}{5040}\n\nRadius of Convergence:\nConverges for all x."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/important-power-series-expansions.html#cosine-function-cosx",
    "href": "mathematics/differential-equations/power-series/important-power-series-expansions.html#cosine-function-cosx",
    "title": "Quick Reference: Important Power Series Expansions",
    "section": "",
    "text": "\\cos(x) = \\sum_{n=0}^\\infty \\frac{(-1)^n}{(2n)!} x^{2n}\n\nFirst Four Terms:\n\n\\cos(x) \\approx 1 - \\frac{x^2}{2} + \\frac{x^4}{24} - \\frac{x^6}{720}\n\nRadius of Convergence:\nConverges for all x."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/important-power-series-expansions.html#geometric-series-frac11-x",
    "href": "mathematics/differential-equations/power-series/important-power-series-expansions.html#geometric-series-frac11-x",
    "title": "Quick Reference: Important Power Series Expansions",
    "section": "",
    "text": "\\frac{1}{1-x} = \\sum_{n=0}^\\infty x^n, \\quad |x| &lt; 1\n\nFirst Four Terms:\n\n\\frac{1}{1-x} \\approx 1 + x + x^2 + x^3\n\nRadius of Convergence:\nConverges for |x| &lt; 1."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/important-power-series-expansions.html#natural-logarithm-ln1x",
    "href": "mathematics/differential-equations/power-series/important-power-series-expansions.html#natural-logarithm-ln1x",
    "title": "Quick Reference: Important Power Series Expansions",
    "section": "",
    "text": "\\ln(1+x) = \\sum_{n=1}^\\infty (-1)^{n+1} \\frac{x^n}{n}, \\quad |x| &lt; 1\n\nFirst Four Terms:\n\n\\ln(1+x) \\approx x - \\frac{x^2}{2} + \\frac{x^3}{3} - \\frac{x^4}{4}\n\nRadius of Convergence:\nConverges for |x| &lt; 1."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/important-power-series-expansions.html#arctangent-function-arctanx",
    "href": "mathematics/differential-equations/power-series/important-power-series-expansions.html#arctangent-function-arctanx",
    "title": "Quick Reference: Important Power Series Expansions",
    "section": "",
    "text": "\\arctan(x) = \\sum_{n=0}^\\infty (-1)^n \\frac{x^{2n+1}}{2n+1}, \\quad |x| \\leq 1\n\nFirst Four Terms:\n\n\\arctan(x) \\approx x - \\frac{x^3}{3} + \\frac{x^5}{5} - \\frac{x^7}{7}\n\nRadius of Convergence:\nConverges for |x| \\leq 1."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/important-power-series-expansions.html#binomial-series-1xk",
    "href": "mathematics/differential-equations/power-series/important-power-series-expansions.html#binomial-series-1xk",
    "title": "Quick Reference: Important Power Series Expansions",
    "section": "",
    "text": "(1+x)^k = \\sum_{n=0}^\\infty \\binom{k}{n} x^n, \\quad |x| &lt; 1\n\nWhere:\n\n\\binom{k}{n} = \\frac{k(k-1)(k-2)\\cdots(k-n+1)}{n!}\n\nFirst Four Terms:\n\n(1+x)^k \\approx 1 + kx + \\frac{k(k-1)}{2}x^2 + \\frac{k(k-1)(k-2)}{6}x^3\n\nRadius of Convergence:\nConverges for |x| &lt; 1 unless k is a non-negative integer, in which case it converges for all x."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/important-power-series-expansions.html#summary",
    "href": "mathematics/differential-equations/power-series/important-power-series-expansions.html#summary",
    "title": "Quick Reference: Important Power Series Expansions",
    "section": "",
    "text": "This collection of power series expansions provides a quick reference for commonly used functions:\n\nUniversal: e^x, \\sin(x), \\cos(x) converge for all x.\nRestricted Radius: \\frac{1}{1-x}, \\ln(1+x), \\arctan(x), (1+x)^k converge only for |x| &lt; 1 or specific conditions.\n\nThese expansions are critical tools in analysis, numerical computation, and approximations."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-1-over-1-minus-x.html",
    "href": "mathematics/differential-equations/power-series/power-series-1-over-1-minus-x.html",
    "title": "Power Series Expansion for \\frac{1}{1-x}",
    "section": "",
    "text": "The power series expansion for \\frac{1}{1-x} is a classic example of a geometric series. It provides a representation of the function as an infinite sum of powers of x, valid for |x| &lt; 1. This series is particularly useful in algebra and calculus."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-1-over-1-minus-x.html#overview",
    "href": "mathematics/differential-equations/power-series/power-series-1-over-1-minus-x.html#overview",
    "title": "Power Series Expansion for \\frac{1}{1-x}",
    "section": "",
    "text": "The power series expansion for \\frac{1}{1-x} is a classic example of a geometric series. It provides a representation of the function as an infinite sum of powers of x, valid for |x| &lt; 1. This series is particularly useful in algebra and calculus."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-1-over-1-minus-x.html#power-series-for-frac11-x",
    "href": "mathematics/differential-equations/power-series/power-series-1-over-1-minus-x.html#power-series-for-frac11-x",
    "title": "Power Series Expansion for \\frac{1}{1-x}",
    "section": "Power Series for \\frac{1}{1-x}",
    "text": "Power Series for \\frac{1}{1-x}\nThe function \\frac{1}{1-x} can be expressed as:\n\n\\frac{1}{1-x} = \\sum_{n=0}^\\infty x^n\n\nThis series converges for |x| &lt; 1. The first four terms of the series are:\n\n\\frac{1}{1-x} \\approx 1 + x + x^2 + x^3"
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-1-over-1-minus-x.html#explanation",
    "href": "mathematics/differential-equations/power-series/power-series-1-over-1-minus-x.html#explanation",
    "title": "Power Series Expansion for \\frac{1}{1-x}",
    "section": "Explanation",
    "text": "Explanation\n\n1. Geometric Series\nThe series arises from the infinite geometric series:\n\n\\sum_{n=0}^\\infty r^n = \\frac{1}{1-r}, \\quad \\text{for } |r| &lt; 1\n\nIn this case, substituting r = x, we get:\n\n\\frac{1}{1-x} = \\sum_{n=0}^\\infty x^n\n\n\n\n2. General Term\nThe general term of the series is:\n\nx^n\n\nwhere n is a non-negative integer. This leads to a simple and elegant representation.\n\n\n3. Radius of Convergence\nThe series converges only for |x| &lt; 1, as the geometric series formula is valid within this interval. For |x| \\geq 1, the series does not converge."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-1-over-1-minus-x.html#examples",
    "href": "mathematics/differential-equations/power-series/power-series-1-over-1-minus-x.html#examples",
    "title": "Power Series Expansion for \\frac{1}{1-x}",
    "section": "Examples",
    "text": "Examples\n\nSmall x:\nFor x = 0.1, using the first four terms:\n\n\\frac{1}{1-0.1} \\approx 1 + 0.1 + (0.1)^2 + (0.1)^3 = 1 + 0.1 + 0.01 + 0.001 = 1.111\n\nThis is close to the exact value \\frac{1}{0.9} = 1.111\\ldots.\nLarger x:\nFor x = 0.5, the first four terms give:\n\n\\frac{1}{1-0.5} \\approx 1 + 0.5 + (0.5)^2 + (0.5)^3 = 1 + 0.5 + 0.25 + 0.125 = 1.875\n\nAdding more terms increases the accuracy."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-1-over-1-minus-x.html#summary",
    "href": "mathematics/differential-equations/power-series/power-series-1-over-1-minus-x.html#summary",
    "title": "Power Series Expansion for \\frac{1}{1-x}",
    "section": "Summary",
    "text": "Summary\nThe power series expansion for \\frac{1}{1-x} is a simple yet powerful tool in analysis:\n\nExact Representation: The infinite series converges to \\frac{1}{1-x} for |x| &lt; 1.\nApproximation: Truncated series provide practical approximations for |x| &lt; 1.\nApplications: Used in algebra, calculus, and as the foundation for many other series expansions.\n\nThe first four terms, 1 + x + x^2 + x^3, offer a quick approximation for small x. This series demonstrates the elegance of geometric progressions in representing functions."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-exponential-function.html",
    "href": "mathematics/differential-equations/power-series/power-series-exponential-function.html",
    "title": "Power Series Expansion for e^x",
    "section": "",
    "text": "The power series expansion for the exponential function e^x is one of the most widely used series in mathematics, physics, and engineering. Derived from the Taylor series at x = 0, this expansion provides an exact representation of e^x as an infinite sum."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-exponential-function.html#overview",
    "href": "mathematics/differential-equations/power-series/power-series-exponential-function.html#overview",
    "title": "Power Series Expansion for e^x",
    "section": "",
    "text": "The power series expansion for the exponential function e^x is one of the most widely used series in mathematics, physics, and engineering. Derived from the Taylor series at x = 0, this expansion provides an exact representation of e^x as an infinite sum."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-exponential-function.html#power-series-for-ex",
    "href": "mathematics/differential-equations/power-series/power-series-exponential-function.html#power-series-for-ex",
    "title": "Power Series Expansion for e^x",
    "section": "Power Series for e^x",
    "text": "Power Series for e^x\nThe exponential function e^x can be expressed as:\n\ne^x = \\sum_{n=0}^\\infty \\frac{x^n}{n!}\n\nThis series converges for all real x. The first four terms of the series are:\n\ne^x \\approx 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!}\n\nwhich simplifies to:\n\ne^x \\approx 1 + x + \\frac{x^2}{2} + \\frac{x^3}{6}"
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-exponential-function.html#explanation",
    "href": "mathematics/differential-equations/power-series/power-series-exponential-function.html#explanation",
    "title": "Power Series Expansion for e^x",
    "section": "Explanation",
    "text": "Explanation\n\n1. Taylor Series\nThe power series for e^x is derived from the Taylor series centered at x = 0:\n\nf(x) = f(0) + f'(0)x + \\frac{f''(0)}{2!}x^2 + \\frac{f^{(3)}(0)}{3!}x^3 + \\cdots\n\nFor e^x, all derivatives are e^x, and at x = 0:\n\nf(0) = e^0 = 1, \\quad f'(0) = e^0 = 1, \\quad f''(0) = e^0 = 1, \\ldots-\n\nThus, the coefficients of the Taylor series are 1/n!.\n\n\n2. General Term\nThe general term of the series is:\n\n\\frac{x^n}{n!}\n\nwhere n is a non-negative integer.\n\n\nKey Property\nThe series converges absolutely and uniformly for all x, making it highly reliable for approximations."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-exponential-function.html#examples",
    "href": "mathematics/differential-equations/power-series/power-series-exponential-function.html#examples",
    "title": "Power Series Expansion for e^x",
    "section": "Examples",
    "text": "Examples\n\nSmall x:\nFor x = 0.1, using the first four terms:\n\ne^{0.1} \\approx 1 + 0.1 + \\frac{(0.1)^2}{2} + \\frac{(0.1)^3}{6}\n\nThis provides a close approximation to e^{0.1}.\nLarger x:\nFor x = 1, the first four terms give:\n\ne^1 \\approx 1 + 1 + \\frac{1^2}{2} + \\frac{1^3}{6} = 1 + 1 + 0.5 + 0.1667 \\approx 2.6667\n\nAdding more terms improves the accuracy."
  },
  {
    "objectID": "mathematics/differential-equations/power-series/power-series-exponential-function.html#summary",
    "href": "mathematics/differential-equations/power-series/power-series-exponential-function.html#summary",
    "title": "Power Series Expansion for e^x",
    "section": "Summary",
    "text": "Summary\nThe power series expansion for e^x is a cornerstone in mathematics and science:\n\nExact Representation: The infinite series converges to e^x for all real x.\nApproximation: Truncated series provide practical approximations.\nApplications: Used in numerical analysis, differential equations, and growth modeling.\n\nThe first four terms, 1 + x + \\frac{x^2}{2} + \\frac{x^3}{6}, are often sufficient for small x. This series illustrates the utility of infinite sums in representing functions accurately and elegantly."
  },
  {
    "objectID": "mathematics/differential-equations/stability-of-origin-2x2.html",
    "href": "mathematics/differential-equations/stability-of-origin-2x2.html",
    "title": "Stability of the Origin in 2x2 Systems of Differential Equations",
    "section": "",
    "text": "In a system of differential equations defined by \\mathbf{x}' = A \\mathbf{x}, understanding the stability of the origin (the point \\mathbf{x} = 0) is essential. The stability depends on the eigenvalues of the 2 \\times 2 matrix A. Different configurations of eigenvalues lead to distinct types of stability, affecting how trajectories in the phase plane behave near the origin."
  },
  {
    "objectID": "mathematics/differential-equations/stability-of-origin-2x2.html#overview",
    "href": "mathematics/differential-equations/stability-of-origin-2x2.html#overview",
    "title": "Stability of the Origin in 2x2 Systems of Differential Equations",
    "section": "",
    "text": "In a system of differential equations defined by \\mathbf{x}' = A \\mathbf{x}, understanding the stability of the origin (the point \\mathbf{x} = 0) is essential. The stability depends on the eigenvalues of the 2 \\times 2 matrix A. Different configurations of eigenvalues lead to distinct types of stability, affecting how trajectories in the phase plane behave near the origin."
  },
  {
    "objectID": "mathematics/differential-equations/stability-of-origin-2x2.html#types-of-stability",
    "href": "mathematics/differential-equations/stability-of-origin-2x2.html#types-of-stability",
    "title": "Stability of the Origin in 2x2 Systems of Differential Equations",
    "section": "Types of Stability",
    "text": "Types of Stability\n\nRepelling Node (Unstable)\n\nEigenvalues: 0 &lt; \\lambda_1 \\leq \\lambda_2 (both positive real numbers).\nDescription: The origin is unstable. Trajectories move away from the origin in all directions. This configuration is known as a repelling node.\n\n\n\nShow Code\n# Define the system of ODEs for a repelling node\ndef dx_dt(X, t):\n    x1, x2 = X\n    dx1 = 3 * x1 + 1 * x2\n    dx2 = 1 * x1 + 3 * x2\n    return [dx1, dx2]\n\n# Generate grid for direction field\nx1_vals = np.linspace(-5, 5, 35)\nx2_vals = np.linspace(-5, 5, 35)\nX1, X2 = np.meshgrid(x1_vals, x2_vals)\n\n# Calculate slopes for direction field\nU = 3 * X1 + 1 * X2\nV = 1 * X1 + 3 * X2\n\n# Normalize the direction field arrows\nspeed = np.sqrt(U**2 + V**2)\nepsilon = 1e-10  # Small value to prevent division by zero\nU_norm = U / (speed + epsilon)\nV_norm = V / (speed + epsilon)\n\n# Plot the direction field\nplt.figure(figsize=(8, 8))\nplt.quiver(X1, X2, U_norm, V_norm, angles=\"xy\", color=\"black\", scale=40, alpha=0.4, width=0.0035)\n\n# Straight-line solutions (eigenvectors)\nx = np.linspace(-5, 5, 200)\nplt.plot(x, x, 'r', linewidth=3, label=r'Unstable direction, $\\lambda=3$')\nplt.plot(x, -x, 'b', linewidth=3, label=r'Unstable direction, $\\lambda=4$')\n\n\n# Set plot limits and labels\nplt.xlim(-5, 5)\nplt.ylim(-5, 5)\nplt.xlabel(\"$x_1$\", fontsize=14)\nplt.ylabel(\"$x_2$\", fontsize=14)\nplt.title(\"Repelling Node (Unstable)\", fontsize=18)\n\n# Add grid and legend\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.legend(loc=\"best\", fontsize=12, framealpha=1.0, frameon=True)\n\n# Display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nSaddle (Unstable)\n\nEigenvalues: \\lambda_1 &lt; 0 &lt; \\lambda_2 (one positive and one negative real number).\nDescription: The origin is unstable, with trajectories approaching along one direction and moving away along another. This configuration creates a “saddle” shape in the phase plane, and is called a saddle point.\n\n\n\nShow Code\n# Define the system of ODEs for a saddle point\ndef dx_dt(X, t):\n    x1, x2 = X\n    dx1 = 1 * x1 + 2 * x2\n    dx2 = 2 * x1 - 1 * x2\n    return [dx1, dx2]\n\n# Plot the direction field\nplt.figure(figsize=(8, 8))\nplt.quiver(X1, X2, U_norm, V_norm, angles=\"xy\", color=\"black\", scale=40, alpha=0.4, width=0.0035)\n\n# Straight-line solutions\nplt.plot(x, x, 'r', linewidth=3, label=r'Unstable direction, $\\lambda=1$')\nplt.plot(x, -x, 'b', linewidth=3, label=r'Stable direction, $\\lambda=-1$')\n\n\n# Labels and title\nplt.xlim(-5, 5)\nplt.ylim(-5, 5)\nplt.xlabel(\"$x_1$\", fontsize=14)\nplt.ylabel(\"$x_2$\", fontsize=14)\nplt.title(\"Saddle Point (Unstable)\", fontsize=18)\n\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.legend(loc=\"best\", fontsize=12, framealpha=1.0, frameon=True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAttracting Node (Stable)\n\nEigenvalues: \\lambda_1 \\leq \\lambda_2 &lt; 0 (both negative real numbers).\nDescription: The origin is stable, and all trajectories are attracted toward it. This setup is called an attracting node, as trajectories converge to the origin from all directions.\n\n\n\nShow Code\n# Define the system of ODEs\ndef dx_dt(X, t):\n    x1, x2 = X\n    dx1 = -3 * x1 + 2 * x2\n    dx2 = 2 * x1 - 3 * x2\n    return [dx1, dx2]\n\n# Generate grid for direction field\nx1_vals = np.linspace(-5, 5, 35)\nx2_vals = np.linspace(-5, 5, 35)\nX1, X2 = np.meshgrid(x1_vals, x2_vals)\n\n# Calculate slopes for direction field\nU = -3 * X1 + 2 * X2\nV = 2 * X1 - 3 * X2\n\n# Normalize the direction field arrows\nspeed = np.sqrt(U**2 + V**2)\nepsilon = 1e-10  # Small value to prevent division by zero\nU_norm = U / (speed + epsilon)\nV_norm = V / (speed + epsilon)\n\n# Plot the direction field\nplt.figure(figsize=(8, 8))\nplt.quiver(X1, X2, U_norm, V_norm, angles=\"xy\", color=\"black\", scale=40, alpha=0.4, width=0.0035)\n\n# Plot the straight-line solutions (eigenvectors)\nx = np.linspace(-5, 5, 200)\nplt.plot(x, x, 'r', linewidth=3, label=r'Stable direction, $\\lambda=-1$')\nplt.plot(x, -x, 'b', linewidth=3, label=r'Stable direction, $\\lambda=-5$')\n\n\n# Set plot limits and labels\nplt.xlim(-5, 5)\nplt.ylim(-5, 5)\nplt.xlabel(\"$x_1$\", fontsize=14)\nplt.ylabel(\"$x_2$\", fontsize=14)\nplt.title(\"Attracting Node (Stable)\", fontsize=18)\n\n# Add grid and legend\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.legend(loc=\"best\", fontsize=12, framealpha=1.0, frameon=True)\n\n# Display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nSpiral Source (Unstable)\n\nEigenvalues: \\lambda = a \\pm bi with a &gt; 0 (complex eigenvalues with a positive real part).\nDescription: The origin is unstable. Trajectories spiral outward away from the origin, creating a spiral source.\n\n\n\nShow Code\n# Define the system of ODEs for a spiral source (unstable)\ndef dx_dt(X, t):\n    x1, x2 = X\n    dx1 = 1 * x1 - 2 * x2\n    dx2 = 2 * x1 + 1 * x2\n    return [dx1, dx2]\n\n# Generate grid for direction field\nx1_vals = np.linspace(-5, 5, 35)\nx2_vals = np.linspace(-5, 5, 35)\nX1, X2 = np.meshgrid(x1_vals, x2_vals)\n\n# Calculate slopes for direction field\nU = 1 * X1 - 2 * X2\nV = 2 * X1 + 1 * X2\n\n# Normalize the direction field arrows\nspeed = np.sqrt(U**2 + V**2)\nepsilon = 1e-10  # Small value to prevent division by zero\nU_norm = U / (speed + epsilon)\nV_norm = V / (speed + epsilon)\n\n# Plot the direction field\nplt.figure(figsize=(8, 8))\nplt.quiver(X1, X2, U_norm, V_norm, angles=\"xy\", color=\"black\", scale=40, alpha=0.4, width=0.0035)\n\n# Additional trajectories to show the outward spiral behavior\ninitial_conditions = [\n    [0, .1],\n    [0, -.1],\n    [.1, 0],\n    [-.1, 0],\n]\n\nt_values = np.linspace(0, 5, 500)\nfor ic in initial_conditions:\n    sol = odeint(dx_dt, ic, t_values)\n    plt.plot(sol[:, 0], sol[:, 1], 'green', linestyle='-', linewidth=2)\n\n# Set plot limits and labels\nplt.xlim(-5, 5)\nplt.ylim(-5, 5)\nplt.xlabel(\"$x_1$\", fontsize=14)\nplt.ylabel(\"$x_2$\", fontsize=14)\nplt.title(\"Spiral Source (Unstable)\", fontsize=18)\n\n# Add grid\nplt.grid(True, linestyle='--', alpha=0.5)\n\n# Display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCenter (Stable)\n\nEigenvalues: \\lambda = a \\pm bi with a = 0 (purely imaginary eigenvalues).\nDescription: The origin neither attracts nor repels trajectories. Instead, they form closed orbits around the origin, resulting in a behavior called a center. This indicates neutral stability.\n\n\n\nShow Code\n# Define the system of ODEs for a center (neutral stability)\ndef dx_dt(X, t):\n    x1, x2 = X\n    dx1 = -1 * x2\n    dx2 = 1 * x1\n    return [dx1, dx2]\n\n# Generate grid for direction field\nx1_vals = np.linspace(-5, 5, 35)\nx2_vals = np.linspace(-5, 5, 35)\nX1, X2 = np.meshgrid(x1_vals, x2_vals)\n\n# Calculate slopes for direction field\nU = -1 * X2\nV = 1 * X1\n\n# Normalize the direction field arrows\nspeed = np.sqrt(U**2 + V**2)\nepsilon = 1e-10  # Small value to prevent division by zero\nU_norm = U / (speed + epsilon)\nV_norm = V / (speed + epsilon)\n\n# Plot the direction field\nplt.figure(figsize=(8, 8))\nplt.quiver(X1, X2, U_norm, V_norm, angles=\"xy\", color=\"black\", scale=40, alpha=0.4, width=0.0035)\n\n# Additional trajectories to show the circular motion\ninitial_conditions = [\n    [5, 0],\n    [0, 3],\n    [1, 0]\n\n]\n\nt_values = np.linspace(0, 20, 500)\nfor ic in initial_conditions:\n    sol = odeint(dx_dt, ic, t_values)\n    plt.plot(sol[:, 0], sol[:, 1], 'green', linestyle='-', linewidth=2)\n\n# Set plot limits and labels\nplt.xlim(-5, 5)\nplt.ylim(-5, 5)\nplt.xlabel(\"$x_1$\", fontsize=14)\nplt.ylabel(\"$x_2$\", fontsize=14)\nplt.title(\"Center (Neutral Stability)\", fontsize=18)\n\n# Add grid\nplt.grid(True, linestyle='--', alpha=0.5)\n\n# Display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nSpiral Sink (Stable)\n\nEigenvalues: \\lambda = a \\pm bi with a &lt; 0 (complex eigenvalues with a negative real part).\nDescription: The origin is stable, and trajectories spiral inward toward it, forming a spiral sink.\n\n\n\nShow Code\n# Define the system of ODEs for a spiral sink (stable)\ndef dx_dt(X, t):\n    x1, x2 = X\n    dx1 = -1 * x1 - 2 * x2\n    dx2 = 2 * x1 - 1 * x2\n    return [dx1, dx2]\n\n# Generate grid for direction field\nx1_vals = np.linspace(-5, 5, 35)\nx2_vals = np.linspace(-5, 5, 35)\nX1, X2 = np.meshgrid(x1_vals, x2_vals)\n\n# Calculate slopes for direction field\nU = -1 * X1 - 2 * X2\nV = 2 * X1 - 1 * X2\n\n# Normalize the direction field arrows\nspeed = np.sqrt(U**2 + V**2)\nepsilon = 1e-10  # Small value to prevent division by zero\nU_norm = U / (speed + epsilon)\nV_norm = V / (speed + epsilon)\n\n# Plot the direction field\nplt.figure(figsize=(8, 8))\nplt.quiver(X1, X2, U_norm, V_norm, angles=\"xy\", color=\"black\", scale=40, alpha=0.4, width=0.0035)\n\n# Additional trajectories to show the spiral inward behavior\ninitial_conditions = [\n    [5, 0],\n    [0, 5],\n    [-5, 0],\n    [0, -5],\n]\n\nt_values = np.linspace(0, 10, 500)\nfor ic in initial_conditions:\n    sol = odeint(dx_dt, ic, t_values)\n    plt.plot(sol[:, 0], sol[:, 1], 'green', linestyle='-', linewidth=2)\n\n# Set plot limits and labels\nplt.xlim(-5, 5)\nplt.ylim(-5, 5)\nplt.xlabel(\"$x_1$\", fontsize=14)\nplt.ylabel(\"$x_2$\", fontsize=14)\nplt.title(\"Spiral Sink (Stable)\", fontsize=18)\n\n# Add grid and legend\nplt.grid(True, linestyle='--', alpha=0.5)\n\n\n# Display the plot\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "mathematics/differential-equations/stability-of-origin-2x2.html#summary",
    "href": "mathematics/differential-equations/stability-of-origin-2x2.html#summary",
    "title": "Stability of the Origin in 2x2 Systems of Differential Equations",
    "section": "Summary",
    "text": "Summary\nThe stability of the origin in a 2 \\times 2 system of differential equations depends on the real and imaginary parts of the eigenvalues of matrix A:\n\nReal positive eigenvalues lead to a repelling node (unstable).\nMixed positive and negative real eigenvalues create a saddle point (unstable).\nReal negative eigenvalues result in an attracting node (stable).\nComplex eigenvalues with a positive real part indicate a spiral source (unstable).\nPurely imaginary eigenvalues create a center (neutral stability).\nComplex eigenvalues with a negative real part lead to a spiral sink (stable).\n\nBy determining the eigenvalues, we can classify the type of stability at the origin and predict the behavior of trajectories in the phase plane."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/fast-fourier-transform.html",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/fast-fourier-transform.html",
    "title": "The Fast Fourier Transform (FFT)",
    "section": "",
    "text": "The Fast Fourier Transform (FFT) is an efficient algorithm to compute the Discrete Fourier Transform (DFT). By recursively breaking the DFT into smaller DFTs, the FFT reduces the computational complexity from O(n^2) to O(n \\log n), making it essential for signal processing, image analysis, and many other fields."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/fast-fourier-transform.html#overview",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/fast-fourier-transform.html#overview",
    "title": "The Fast Fourier Transform (FFT)",
    "section": "",
    "text": "The Fast Fourier Transform (FFT) is an efficient algorithm to compute the Discrete Fourier Transform (DFT). By recursively breaking the DFT into smaller DFTs, the FFT reduces the computational complexity from O(n^2) to O(n \\log n), making it essential for signal processing, image analysis, and many other fields."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/fast-fourier-transform.html#the-discrete-fourier-transform-dft",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/fast-fourier-transform.html#the-discrete-fourier-transform-dft",
    "title": "The Fast Fourier Transform (FFT)",
    "section": "The Discrete Fourier Transform (DFT)",
    "text": "The Discrete Fourier Transform (DFT)\nThe DFT transforms a sequence of n samples x_j in the time domain into its frequency components y_k:\n\ny_k = \\frac{1}{\\sqrt{n}} \\sum_{j=0}^{n-1} x_j \\omega^{jk}, \\quad \\omega = e^{-i \\frac{2\\pi}{n}}, \\quad k = 0, 1, \\dots, n-1\n\nHere:\n\nx_j: The input samples in the time domain.\ny_k: The output frequency components.\n\\omega = e^{-i \\frac{2\\pi}{n}}: The primitive n-th root of unity.\n\nThe Inverse DFT (IDFT) reconstructs the time-domain signal from its frequency components:\n\nx_j = \\frac{1}{\\sqrt{n}} \\sum_{k=0}^{n-1} y_k \\omega^{-jk}, \\quad j = 0, 1, \\dots, n-1\n\nThe FFT optimizes the computation of the DFT by exploiting symmetries in the roots of unity e^{-2\\pi i / n}."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/fast-fourier-transform.html#fft-algorithm",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/fast-fourier-transform.html#fft-algorithm",
    "title": "The Fast Fourier Transform (FFT)",
    "section": "FFT Algorithm",
    "text": "FFT Algorithm\n\nExpand the DFT\nConsider the DFT for n = 4. The transformation can be written as a matrix-vector product:\n\n\\begin{bmatrix}\nz_0 \\\\\nz_1 \\\\\nz_2 \\\\\nz_3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\omega^{0 \\cdot 0} & \\omega^{0 \\cdot 1} & \\omega^{0 \\cdot 2} & \\omega^{0 \\cdot 3} \\\\\n\\omega^{1 \\cdot 0} & \\omega^{1 \\cdot 1} & \\omega^{1 \\cdot 2} & \\omega^{1 \\cdot 3} \\\\\n\\omega^{2 \\cdot 0} & \\omega^{2 \\cdot 1} & \\omega^{2 \\cdot 2} & \\omega^{2 \\cdot 3} \\\\\n\\omega^{3 \\cdot 0} & \\omega^{3 \\cdot 1} & \\omega^{3 \\cdot 2} & \\omega^{3 \\cdot 3}\n\\end{bmatrix}\n\\begin{bmatrix}\nx_0 \\\\\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix}\n\nwhere \\omega = e^{-i \\frac{2\\pi}{4}} = e^{-i \\frac{\\pi}{2}}.\nCompute \\omega^k for k = 0, 1, 2, 3:\n\n\\omega^0 = e^{-i 0} = 1\n\\omega^1 = e^{-i \\frac{\\pi}{2}} = -i\n\\omega^2 = e^{-i \\pi} = -1\n\\omega^3 = e^{-i \\frac{3\\pi}{2}} = i\n\nExpanding the terms gives:\n\nFor z_0 ( k = 0 ):\n\n\\begin{align*}\nz_0 &= x_0 \\omega^{0 \\cdot 0} + x_1 \\omega^{0 \\cdot 1} + x_2 \\omega^{0 \\cdot 2} + x_3 \\omega^{0 \\cdot 3} \\\\\n    &= x_0 (1) + x_1 (1) + x_2 (1) + x_3 (1) \\\\\n    &= x_0 + x_1 + x_2 + x_3\n\\end{align*}\n\nFor z_1 ( k = 1 ):\n\n\\begin{align*}\nz_1 &= x_0 \\omega^{0 \\cdot 1} + x_1 \\omega^{1 \\cdot 1} + x_2 \\omega^{2 \\cdot 1} + x_3 \\omega^{3 \\cdot 1} \\\\\n    &= x_0 (1) + x_1 (-i) + x_2 (-1) + x_3 (i)\n\\end{align*}\n\nFor z_2 ( k = 2 ):\n\n\\begin{align*}\nz_2 &= x_0 \\omega^{0 \\cdot 2} + x_1 \\omega^{1 \\cdot 2} + x_2 \\omega^{2 \\cdot 2} + x_3 \\omega^{3 \\cdot 2} \\\\\n    &= x_0 (1) + x_1 (-1) + x_2 (1) + x_3 (-1)\n\\end{align*}\n\nFor z_3 ( k = 3 ):\n\n\\begin{align*}\nz_3 &= x_0 \\omega^{0 \\cdot 3} + x_1 \\omega^{1 \\cdot 3} + x_2 \\omega^{2 \\cdot 3} + x_3 \\omega^{3 \\cdot 3} \\\\\n    &= x_0 (1) + x_1 (i) + x_2 (-1) + x_3 (-i)\n\\end{align*}\n\n\n\n\nSeparate Even and Odd Terms\nGroup the terms into even-indexed (x_0, x_2) and odd-indexed (x_1, x_3) contributions:\n\nFor z_0:\n\nz_0 = (x_0 + x_2) + (x_1 + x_3)\n\nFor z_1:\n\nz_1 = \\left( x_0 + x_2 (-1) \\right) + \\left( x_1 (-i) + x_3 (i) \\right)\n\nFor z_2:\n\nz_2 = (x_0 + x_2) + (x_1 (-1) + x_3 (-1))\n\nFor z_3:\n\nz_3 = \\left( x_0 + x_2 (-1) \\right) + \\left( x_1 (i) + x_3 (-i) \\right)\n\n\n\n\nFactor Out Common Terms\nFor the odd terms, notice that:\n\nIn z_1 and z_3, the odd-indexed terms involve \\omega^k multiplied by x_1 and x_3.\nWe can factor out \\omega^k from these terms.\n\nLet’s define:\n\nu_k = x_0 + x_2 (-1)^k\nv_k = x_1 + x_3 (-1)^k\n\nCompute (-1)^k:\n\n(-1)^0 = 1\n(-1)^1 = -1\n(-1)^2 = 1\n(-1)^3 = -1\n\nCompute u_k:\n\nu_0 = x_0 + x_2 (1) = x_0 + x_2\nu_1 = x_0 + x_2 (-1) = x_0 - x_2\nu_2 = x_0 + x_2 (1) = x_0 + x_2\nu_3 = x_0 + x_2 (-1) = x_0 - x_2\n\nCompute v_k:\n\nv_0 = x_1 + x_3 (1) = x_1 + x_3\nv_1 = x_1 + x_3 (-1) = x_1 - x_3\nv_2 = x_1 + x_3 (1) = x_1 + x_3\nv_3 = x_1 + x_3 (-1) = x_1 - x_3\n\nNow, the DFT outputs can be expressed as:\n\nFor k = 0:\n\nz_0 = u_0 + v_0\n\nFor k = 1:\n\nz_1 = u_1 + \\omega^{k} v_1 = u_1 + (-i) v_1\n\nFor k = 2:\n\nz_2 = u_2 - v_2 = u_2 - v_2\n\nFor k = 3:\n\nz_3 = u_3 + \\omega^{k} v_3 = u_3 + i v_3\n\n\n\n\nRecognize Smaller DFTs\nWe can now see that we’ve broken down the original DFT into smaller DFTs of size 2:\n\nEven-Indexed DFT (U_k)\nCompute U_k using the even-indexed elements x_0 and x_2:\n\n\\begin{bmatrix}\nu_0 \\\\\nu_1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nx_0 + x_2 \\\\\nx_0 - x_2\n\\end{bmatrix}\n\n\n\nOdd-Indexed DFT (V_k)\nCompute V_k using the odd-indexed elements x_1 and x_3:\n\n\\begin{bmatrix}\nv_0 \\\\\nv_1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nx_1 + x_3 \\\\\nx_1 - x_3\n\\end{bmatrix}\n\n\n\n\nCombine the Results with Twiddle Factors\nNow, combine U_k and V_k using the twiddle factors \\omega^{k}:\n\nFor k = 0:\n\n\\begin{align*}\nz_0 &= u_0 + \\omega^{0} v_0 = u_0 + v_0 \\\\\nz_2 &= u_0 - \\omega^{0} v_0 = u_0 - v_0\n\\end{align*}\n\nSince \\omega^{0} = 1.\nFor k = 1:\n\n\\begin{align*}\nz_1 &= u_1 + \\omega^{1} v_1 = u_1 + (-i) v_1 \\\\\nz_3 &= u_1 - \\omega^{1} v_1 = u_1 - (-i) v_1 = u_1 + i v_1\n\\end{align*}\n\nSince \\omega^{1} = -i.\n\n\n\nSummary of Steps\n\nCompute u_0 and u_1:\n\nu_0 = x_0 + x_2\nu_1 = x_0 - x_2\n\nCompute v_0 and v_1:\n\nv_0 = x_1 + x_3\nv_1 = x_1 - x_3\n\nCompute z_k:\n\nz_0 = u_0 + v_0\nz_1 = u_1 + (-i) v_1\nz_2 = u_0 - v_0\nz_3 = u_1 + i v_1\n\n\n\n\nFinal FFT Algorithm for n = 4\nBy following these steps, we’ve efficiently computed the DFT using the FFT algorithm:\n\nStep 1: Divide the input sequence into even and odd indices.\nStep 2: Compute the smaller DFTs (U_k and V_k) of size 2.\nStep 3: Combine the results using the twiddle factors \\omega^{k}."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/fast-fourier-transform.html#fft-flow-diagram",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/fast-fourier-transform.html#fft-flow-diagram",
    "title": "The Fast Fourier Transform (FFT)",
    "section": "FFT Flow Diagram",
    "text": "FFT Flow Diagram\nThe following diagram illustrates how the FFT splits a DFT into smaller sub-DFTs and combines the results:\n\n\nExplanation of the Diagram\n\nInput Splitting:\n\nThe input sequence x[n] of length N is split into two sequences:\n\nThe even-indexed terms: x[0], x[2], \\dots, x[N-2].\nThe odd-indexed terms: x[1], x[3], \\dots, x[N-1].\n\n\nRecursive Computation:\n\nEach sequence is passed through an N/2-point DFT.\nThe top box computes the DFT of the even terms, yielding E[k].\nThe bottom box computes the DFT of the odd terms, yielding O[k].\n\nCombining the Results:\n\nThe outputs of the two N/2-point DFTs are combined using the twiddle factors W_N^k to compute the final N-point DFT:\n\nX[k] = E[k] + W_N^k O[k], for k = 0, 1, \\dots, N/2-1.\nX[k + N/2] = E[k] - W_N^k O[k], for k = 0, 1, \\dots, N/2-1.\n\n\nButterfly Connections:\n\nThe crossing lines (labeled with W_N^k) represent the butterfly structure, which combines the even and odd terms.\n\n\nThis recursive breakdown is what enables the FFT to compute the DFT efficiently."
  },
  {
    "objectID": "mathematics/numerical-analysis/fast-fourier-transform/index.html",
    "href": "mathematics/numerical-analysis/fast-fourier-transform/index.html",
    "title": "FAST FOURIER TRANSFORM",
    "section": "",
    "text": "The Fourier Transform\nRoots of Unity\nThe Discrete Fourier Transform (DFT)\nThe Fast Fourier Transform (FFT)"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/index.html",
    "href": "mathematics/numerical-analysis/homework/index.html",
    "title": "HOMEWORK",
    "section": "",
    "text": "Suggested Homework\n\nWEEK 01\n\n(C0-P1)\n(C1-P1) Exercise 1.1.4ab\n(C1-P2) Exercise 1.2.2\n(C1-P3)\n(C1-P4) Exercise 1.2.14\n(C1-P5) Exercise 1.4.1\n(C1-P6) Exercise 1.4.3\n\n\n\nWEEK 02\n\n(C1-P7) Exercise 1.4.6\n(C1-P8) Exercise 1.4.8\n(C1-P9) Computer Problem 1.4.7\n(C1-P10) Exercise 1.5.1\n(C1-P11)\n(C3-P1) Exercise 3.1.1a\n(C3-P1) Exercise 3.1.1c\n(C3-P2) Exercise 3.1.2a\n(C3-P2) Exercise 3.1.2c\n(C3-P3) Exercise 3.1.6\n\n\n\nWEEK 03\n\n(C0-P2)\n(C3-P4) Exercise 3.2.2\n(C3-P5) Exercise 3.2.5\n(C3-P6) Exercise 3.2.6\n(C3-P7) Computer Problem 3.1.3\n(C3-P8) Exercise 3.3.1ac\n(C3-P9) Exercise 3.3.2ac\n(C3-P10) Exercise 3.3.3\n\n\n\nWEEK 04\n\n(C5-P1) Exercise 5.2.1a\n(C5-P1) Exercise 5.2.1b\n(C5-P2) Exercise 5.2.2ab\n(C5-P3) Exercise 5.2.3ab\n(C5-P4) Exercise 5.2.10\n(C5-P5) Exercise 5.2.12\n(C5-P6) Computer Problem 5.2.1ac\n(C5-P7) Computer Problem 5.2.2de\n(C5-P8) Computer Problem 5.2.9bf\n\n\n\nWEEK 05\n\n(C5-P9) Exercise 5.5.1ab\n(C5-P10) Exercise 5.5.4cd\n(C5-P11) Computer Problem 5.4.1acd\n(C5-P12) Computer Problem 5.4.2\n(C5-P13) Computer Problem 5.4.3acd\n(C5-P14) Exercise 5.5.5cd\n(C5-P15) Exercise 5.5.7\n\n\n\nWEEK 06\n\n(C2-P1) Exercise 2.1.2a\n(C2-P1) Exercise 2.1.2c\n(C2-P2) Computer Problem 2.1.2ac\n(C2-P3) Exercise 2.2.1ab\n(C2-P4) Exercise 2.2.2ab\n\n\n\nWEEK 07\n\n(C2-P5) Exercise 2.2.4\n(C2-P6) Computer Problem 2.2.1ab\n(C2-P7) Exercise 2.4.1ab\n(C2-P8) Exercise 2.4.2ab\n(C2-P9) Exercise 2.4.4a\n(C2-P10) Exercise 2.4.6\n\n\n\nWEEK 08\n\n(C2-P11) Exercise 2.5.2ab\n(C2-P12) Computer Problem 2.5.2 (solve using both Jacobi and Gauss-Seidel, compare results)\n(C4-P1) Exercise 4.1.2a\n(C4-P1) Exercise 4.1.2b\n(C4-P2) Computer Problem 4.1.5 (also use a quadratic fit and compare)\n(C4-P3) Exercise 4.3.2\n\n\n\nWEEK 09\n\nExercise 4.3.1d\n(C4-P4) Exercise 4.3.4\n(C4-P5) Exercise 4.3.7 (you can use your QR factorizations from Exercise 4.3.2)\n(C4-P6) Computer Problem 4.3.4 Additional instructions: Write a classical Gram-Schmidt code only. Use the matrices in Exercise 4.3.2 to check your code. If you use the code I provided, you must comment it (explain what every line does).\n\n\n\nWEEK 10\n\n(C4-P7) Exercise 4.4.2\n(C4-P8) Exercise 4.4.3\n(C4-P9) Computer Problem 4.4.2 (find a preconditioned GMRES Python code and use it)\n\n\n\nWEEK 11\nNone\n\n\nWEEK 12\n\n(C10-P1) Exercise 10.1.1ad (also, find the inverse DFT of your result, compare to the original vector)\n(C10-P2) Exercise 10.1.8\n(C10-P3) Exercise 10.2.1a\n(C10-P3) Exercise 10.2.1b\n(C10-P4) Exercise 10.2.3a\n(C10-P4) Exercise 10.2.3b\n(C10-P5) Exercise 10.2.3a - Plot\n(C10-P5) Exercise 10.2.3b - Plot\n(C10-P6) Computer Problem 10.2.4\n(C10-P7) Exercise 10.3.2ab\n\n\n\nWEEK 13\n\n(C10-P8) Computer Problem 10.3.2cd\n(C10-P9) Exercise 10.3.5 (Complete the \\sum_{j=0}^{n-1} \\cos \\frac{2 \\pi j k}{n} \\cos \\frac{2 \\pi j l}{n} result only)\n(C10-P10) Exercise 10.1.6"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w02/exercise3-1-1a.html",
    "href": "mathematics/numerical-analysis/homework/w02/exercise3-1-1a.html",
    "title": "Exercise 3.1.1a (C3-P1)",
    "section": "",
    "text": "Use Lagrange interpolation to find a polynomial that passes through the points (0, 1), (2, 3), (3, 0).\nThe Lagrange interpolation polynomial for three points (x_1, y_1), (x_2, y_2), and (x_3, y_3) is given by the formula:\nP(x) = y_1 \\frac{(x - x_2)(x - x_3)}{(x_1 - x_2)(x_1 - x_3)} + y_2 \\frac{(x - x_1)(x - x_3)}{(x_2 - x_1)(x_2 - x_3)} + y_3 \\frac{(x - x_1)(x - x_2)}{(x_3 - x_1)(x_3 - x_2)}\nThe points are:\n\n(x_1, y_1) = (0, 1)\n(x_2, y_2) = (2, 3)\n(x_3, y_3) = (3, 0)\n\n\n\n\nFirst term (corresponding to (x_1, y_1) = (0, 1)):\n\n\n1 \\cdot \\frac{(x - 2)(x - 3)}{(0 - 2)(0 - 3)} = 1 \\cdot \\frac{(x - 2)(x - 3)}{(-2)(-3)} = \\frac{(x - 2)(x - 3)}{6}\n\n\nSecond term (corresponding to (x_2, y_2) = (2, 3))\n\n\n3 \\cdot \\frac{(x - 0)(x - 3)}{(2 - 0)(2 - 3)} = 3 \\cdot \\frac{(x)(x - 3)}{(2)(-1)} = -\\frac{3x(x - 3)}{2}\n\n\nThird term (corresponding to (x_3, y_3) = (3, 0)):\n\n\n0 \\cdot \\frac{(x - 0)(x - 2)}{(3 - 0)(3 - 2)} = 0\n\n\n\n\n\nP(x) = \\frac{(x - 2)(x - 3)}{6} - \\frac{3x(x - 3)}{2}\n\n\n\n\nFirst term:\n\n\\frac{(x - 2)(x - 3)}{6} = \\frac{x^2 - 5x + 6}{6}\n\nSecond term:\n\n-\\frac{3x(x - 3)}{2} = -\\frac{3(x^2 - 3x)}{2} = -\\frac{3x^2}{2} + \\frac{9x}{2}\n\nNow, combine these two terms:\n\nP(x) = \\frac{x^2 - 5x + 6}{6} - \\left(\\frac{3x^2}{2} - \\frac{9x}{2}\\right)\n\nTo combine, first rewrite everything with a denominator of 6:\n\nP(x) = \\frac{x^2 - 5x + 6}{6} - \\frac{9x^2 - 27x}{6}\n\nNow simplify:\n\nP(x) = \\frac{x^2 - 5x + 6 - 9x^2 + 27x}{6}\n\n\nP(x) = \\frac{-8x^2 + 22x + 6}{6}\n\nThis is the final polynomial:\n\nP(x) = \\frac{-4x^2 + 11x + 3}{3}\n\nThis is the interpolating polynomial that passes through the points (0, 1), (2, 3), and (3, 0).\n\n\nCreate graph with resulting polynomial\n# Define the Lagrange interpolating polynomial\ndef lagrange_polynomial(x):\n    return (-4 * x**2 + 11 * x + 3) / 3\n\n# Create an array of x values from -1 to 4 for the graph\nx_values = np.linspace(-1, 4, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = lagrange_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = (-4x^2 + 11x + 3) / 3\", color=\"blue\")\n\n# Plot the given points (0,1), (2,3), (3,0)\ndata_points_x = [0, 2, 3]\ndata_points_y = [1, 3, 0]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Lagrange Interpolating Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 5, 1))\nplt.yticks(np.arange(-6, 7, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w02/exercise3-1-1a.html#question",
    "href": "mathematics/numerical-analysis/homework/w02/exercise3-1-1a.html#question",
    "title": "Exercise 3.1.1a (C3-P1)",
    "section": "",
    "text": "Use Lagrange interpolation to find a polynomial that passes through the points (0, 1), (2, 3), (3, 0).\nThe Lagrange interpolation polynomial for three points (x_1, y_1), (x_2, y_2), and (x_3, y_3) is given by the formula:\nP(x) = y_1 \\frac{(x - x_2)(x - x_3)}{(x_1 - x_2)(x_1 - x_3)} + y_2 \\frac{(x - x_1)(x - x_3)}{(x_2 - x_1)(x_2 - x_3)} + y_3 \\frac{(x - x_1)(x - x_2)}{(x_3 - x_1)(x_3 - x_2)}\nThe points are:\n\n(x_1, y_1) = (0, 1)\n(x_2, y_2) = (2, 3)\n(x_3, y_3) = (3, 0)\n\n\n\n\nFirst term (corresponding to (x_1, y_1) = (0, 1)):\n\n\n1 \\cdot \\frac{(x - 2)(x - 3)}{(0 - 2)(0 - 3)} = 1 \\cdot \\frac{(x - 2)(x - 3)}{(-2)(-3)} = \\frac{(x - 2)(x - 3)}{6}\n\n\nSecond term (corresponding to (x_2, y_2) = (2, 3))\n\n\n3 \\cdot \\frac{(x - 0)(x - 3)}{(2 - 0)(2 - 3)} = 3 \\cdot \\frac{(x)(x - 3)}{(2)(-1)} = -\\frac{3x(x - 3)}{2}\n\n\nThird term (corresponding to (x_3, y_3) = (3, 0)):\n\n\n0 \\cdot \\frac{(x - 0)(x - 2)}{(3 - 0)(3 - 2)} = 0\n\n\n\n\n\nP(x) = \\frac{(x - 2)(x - 3)}{6} - \\frac{3x(x - 3)}{2}\n\n\n\n\nFirst term:\n\n\\frac{(x - 2)(x - 3)}{6} = \\frac{x^2 - 5x + 6}{6}\n\nSecond term:\n\n-\\frac{3x(x - 3)}{2} = -\\frac{3(x^2 - 3x)}{2} = -\\frac{3x^2}{2} + \\frac{9x}{2}\n\nNow, combine these two terms:\n\nP(x) = \\frac{x^2 - 5x + 6}{6} - \\left(\\frac{3x^2}{2} - \\frac{9x}{2}\\right)\n\nTo combine, first rewrite everything with a denominator of 6:\n\nP(x) = \\frac{x^2 - 5x + 6}{6} - \\frac{9x^2 - 27x}{6}\n\nNow simplify:\n\nP(x) = \\frac{x^2 - 5x + 6 - 9x^2 + 27x}{6}\n\n\nP(x) = \\frac{-8x^2 + 22x + 6}{6}\n\nThis is the final polynomial:\n\nP(x) = \\frac{-4x^2 + 11x + 3}{3}\n\nThis is the interpolating polynomial that passes through the points (0, 1), (2, 3), and (3, 0).\n\n\nCreate graph with resulting polynomial\n# Define the Lagrange interpolating polynomial\ndef lagrange_polynomial(x):\n    return (-4 * x**2 + 11 * x + 3) / 3\n\n# Create an array of x values from -1 to 4 for the graph\nx_values = np.linspace(-1, 4, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = lagrange_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = (-4x^2 + 11x + 3) / 3\", color=\"blue\")\n\n# Plot the given points (0,1), (2,3), (3,0)\ndata_points_x = [0, 2, 3]\ndata_points_y = [1, 3, 0]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Lagrange Interpolating Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 5, 1))\nplt.yticks(np.arange(-6, 7, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w02/exercise3-1-2a.html",
    "href": "mathematics/numerical-analysis/homework/w02/exercise3-1-2a.html",
    "title": "Exercise 3.1.2a (C3-P2)",
    "section": "",
    "text": "Use Newton’s Divided Differences to find a polynomial that passes through the points (0, 1), (2, 3), (3, 0).\n\n\nThe points are:\n\n(x_1, y_1) = (0, 1)\n(x_2, y_2) = (2, 3)\n(x_3, y_3) = (3, 0)\n\nWe will first construct the divided differences table and use it to construct the Newton interpolating polynomial.\n\n\n\n\n\n\nx\nf[x]\nf[x_1, x_2]\nf[x_1, x_2, x_3]\n\n\n\n\n0\n1\n\n\n\n\n2\n3\n1\n\n\n\n3\n0\n-3\n-\\frac{4}{3}\n\n\n\n\n\n\n\nZeroth order divided differences:\n f[x_1] = y_1 = 1, \\quad f[x_2] = y_2 = 3, \\quad f[x_3] = y_3 = 0 \nFirst order divided differences:\n f[x_1, x_2] = \\frac{f[x_2] - f[x_1]}{x_2 - x_1} = \\frac{3 - 1}{2 - 0} = \\frac{2}{2} = 1 \n f[x_2, x_3] = \\frac{f[x_3] - f[x_2]}{x_3 - x_2} = \\frac{0 - 3}{3 - 2} = \\frac{-3}{1} = -3 \nSecond order divided difference:\n f[x_1, x_2, x_3] = \\frac{f[x_2, x_3] - f[x_1, x_2]}{x_3 - x_1} = \\frac{-3 - 1}{3 - 0} = \\frac{-4}{3} \n\n\n\n\nThe Newton polynomial is given by:\n\nP(x) = f[x_1] + f[x_1, x_2](x - x_1) + f[x_1, x_2, x_3](x - x_1)(x - x_2)\n\nSubstitute the values:\n\nP(x) = 1 + 1(x - 0) + \\left(\\frac{-4}{3}\\right)(x - 0)(x - 2)\n\nSimplify:\n\nP(x) = 1 + x - \\frac{4}{3}(x(x - 2)) = 1 + x - \\frac{4}{3}(x^2 - 2x)\n\n\nP(x) = 1 + x - \\frac{4}{3}x^2 + \\frac{8}{3}x\n\nCombine like terms:\n\nP(x) = 1 + \\left(x + \\frac{8}{3}x\\right) - \\frac{4}{3}x^2 = 1 + \\frac{11x}{3} - \\frac{4x^2}{3}\n\nSo the final polynomial is:\n\nP(x) = \\frac{-4x^2 + 11x + 3}{3}\n\nThis is the Newton interpolating polynomial for the points (0, 1), (2, 3), and (3, 0).\n\n\nCreate graph with resulting polynomial\n# Define the Newton interpolating polynomial\ndef newton_polynomial(x):\n    return (-4 * x**2 + 11 * x + 3) / 3\n\n# Create an array of x values from -1 to 4 for the graph\nx_values = np.linspace(-1, 4, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = newton_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = (-4x^2 + 11x + 3) / 3\", color=\"blue\")\n\n# Plot the given points (0,1), (2,3), (3,0)\ndata_points_x = [0, 2, 3]\ndata_points_y = [1, 3, 0]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Newton's Divided Differences Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 5, 1))\nplt.yticks(np.arange(-6, 7, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w02/exercise3-1-2a.html#question",
    "href": "mathematics/numerical-analysis/homework/w02/exercise3-1-2a.html#question",
    "title": "Exercise 3.1.2a (C3-P2)",
    "section": "",
    "text": "Use Newton’s Divided Differences to find a polynomial that passes through the points (0, 1), (2, 3), (3, 0).\n\n\nThe points are:\n\n(x_1, y_1) = (0, 1)\n(x_2, y_2) = (2, 3)\n(x_3, y_3) = (3, 0)\n\nWe will first construct the divided differences table and use it to construct the Newton interpolating polynomial.\n\n\n\n\n\n\nx\nf[x]\nf[x_1, x_2]\nf[x_1, x_2, x_3]\n\n\n\n\n0\n1\n\n\n\n\n2\n3\n1\n\n\n\n3\n0\n-3\n-\\frac{4}{3}\n\n\n\n\n\n\n\nZeroth order divided differences:\n f[x_1] = y_1 = 1, \\quad f[x_2] = y_2 = 3, \\quad f[x_3] = y_3 = 0 \nFirst order divided differences:\n f[x_1, x_2] = \\frac{f[x_2] - f[x_1]}{x_2 - x_1} = \\frac{3 - 1}{2 - 0} = \\frac{2}{2} = 1 \n f[x_2, x_3] = \\frac{f[x_3] - f[x_2]}{x_3 - x_2} = \\frac{0 - 3}{3 - 2} = \\frac{-3}{1} = -3 \nSecond order divided difference:\n f[x_1, x_2, x_3] = \\frac{f[x_2, x_3] - f[x_1, x_2]}{x_3 - x_1} = \\frac{-3 - 1}{3 - 0} = \\frac{-4}{3} \n\n\n\n\nThe Newton polynomial is given by:\n\nP(x) = f[x_1] + f[x_1, x_2](x - x_1) + f[x_1, x_2, x_3](x - x_1)(x - x_2)\n\nSubstitute the values:\n\nP(x) = 1 + 1(x - 0) + \\left(\\frac{-4}{3}\\right)(x - 0)(x - 2)\n\nSimplify:\n\nP(x) = 1 + x - \\frac{4}{3}(x(x - 2)) = 1 + x - \\frac{4}{3}(x^2 - 2x)\n\n\nP(x) = 1 + x - \\frac{4}{3}x^2 + \\frac{8}{3}x\n\nCombine like terms:\n\nP(x) = 1 + \\left(x + \\frac{8}{3}x\\right) - \\frac{4}{3}x^2 = 1 + \\frac{11x}{3} - \\frac{4x^2}{3}\n\nSo the final polynomial is:\n\nP(x) = \\frac{-4x^2 + 11x + 3}{3}\n\nThis is the Newton interpolating polynomial for the points (0, 1), (2, 3), and (3, 0).\n\n\nCreate graph with resulting polynomial\n# Define the Newton interpolating polynomial\ndef newton_polynomial(x):\n    return (-4 * x**2 + 11 * x + 3) / 3\n\n# Create an array of x values from -1 to 4 for the graph\nx_values = np.linspace(-1, 4, 400)\n\n# Compute the corresponding y values using the polynomial function\ny_values = newton_polynomial(x_values)\n\n# Plot the polynomial curve\nplt.figure(figsize=(8, 6))\nplt.plot(x_values, y_values, label=\"P(x) = (-4x^2 + 11x + 3) / 3\", color=\"blue\")\n\n# Plot the given points (0,1), (2,3), (3,0)\ndata_points_x = [0, 2, 3]\ndata_points_y = [1, 3, 0]\nplt.scatter(data_points_x, data_points_y, color=\"red\", label=\"Data Points\", zorder=5)\n\n# Add labels, title, and legend\nplt.title(\"Newton's Divided Differences Polynomial\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(x)\")\n\n# Set x and y ticks to have increments of 1\nplt.xticks(np.arange(-1, 5, 1))\nplt.yticks(np.arange(-6, 7, 1))\n\nplt.axhline(0, color='black',linewidth=0.5)\nplt.axvline(0, color='black',linewidth=0.5)\nplt.grid(True)\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w02/exercise3-1-6.html",
    "href": "mathematics/numerical-analysis/homework/w02/exercise3-1-6.html",
    "title": "Exercise 3.1.6 (C3-P6)",
    "section": "",
    "text": "How do we construct a polynomial of degree exactly 5 that interpolates four points?\nGiven four points (1, 1), (2, 3), (3, 3), and (4, 4), the standard interpolation methods (such as Newton’s or Lagrange’s methods) would give us a unique polynomial of degree 3. However, constructing a polynomial of degree exactly 5 requires a different approach.\n\n\n\nTheorem 3.2 (Main Theorem of Polynomial Interpolation) guarantees that a unique polynomial of degree n-1 exists for n distinct points.\nTo create a polynomial of degree 5 when you only have 4 points, you need to add an extra term that still passes through all the original points but elevates the polynomial’s degree.\n\n\n\n\nWe will use Newton’s divided differences to first construct the degree 3 polynomial, and then extend it to a degree 5 polynomial.\n\n\n\nWe will use the given points (x_1, y_1) = (1, 1), (x_2, y_2) = (2, 3), (x_3, y_3) = (3, 3), and (x_4, y_4) = (4, 4) to compute the divided differences and build the polynomial.\n\n\n\n\n\n\n\n\n\n\nx\nf(x)\nFirst Difference f[x_i, x_{i+1}]\nSecond Difference f[x_i, x_{i+1}, x_{i+2}]\nThird Difference f[x_i, x_{i+1}, x_{i+2}, x_{i+3}]\n\n\n\n\n1\n1\n\n\n\n\n\n2\n3\n\\frac{3 - 1}{2 - 1} = 2\n\n\n\n\n3\n3\n\\frac{3 - 3}{3 - 2} = 0\n\\frac{0 - 2}{3 - 1} = -1\n\n\n\n4\n4\n\\frac{4 - 3}{4 - 3} = 1\n\\frac{1 - 0}{4 - 2} = \\frac{1}{2}\n\\frac{\\frac{1}{2} - (-1)}{4 - 1} = \\frac{3}{6} = \\frac{1}{2}\n\n\n\n\n\n\nThe Newton’s divided difference form of the interpolating polynomial is:\n\nP_3(x) = f[x_1] + f[x_1, x_2](x - x_1) + f[x_1, x_2, x_3](x - x_1)(x - x_2) + f[x_1, x_2, x_3, x_4](x - x_1)(x - x_2)(x - x_3)\n\nSubstituting the values from the divided difference table:\n\nP_3(x) = 1 + 2(x - 1) - (x - 1)(x - 2) + \\frac{1}{2}(x - 1)(x - 2)(x - 3)\n\n\n\n\nTo create a degree 5 polynomial, we add an extra term c(x - 1)(x - 2)(x - 3)(x - 4), which evaluates to zero at the given points:\n\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nSubstitute P_3(x):\n\nP_5(x) = 1 + 2(x - 1) - (x - 1)(x - 2) + \\frac{1}{2}(x - 1)(x - 2)(x - 3) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\n\n\n\nThe value of c does not affect the polynomial at the given points. The additional term evaluates to 0 at x = 1, 2, 3, 4, so no matter what c is, the polynomial will pass through the points (1, 1), (2, 3), (3, 3), (4, 4).\nHowever, changing c will affect the polynomial’s behavior outside the given points. For different values of c, the polynomial will look different outside the interpolation points.\n\n\n\nThus, the degree 5 polynomial is:\n\nP_5(x) = 1 + 2(x - 1) - (x - 1)(x - 2) + \\frac{1}{2}(x - 1)(x - 2)(x - 3) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nThis polynomial passes through the points (1, 1), (2, 3), (3, 3), (4, 4), and c is an arbitrary constant that influences how the polynomial behaves outside of those points."
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w02/exercise3-1-6.html#question",
    "href": "mathematics/numerical-analysis/homework/w02/exercise3-1-6.html#question",
    "title": "Exercise 3.1.6 (C3-P6)",
    "section": "",
    "text": "How do we construct a polynomial of degree exactly 5 that interpolates four points?\nGiven four points (1, 1), (2, 3), (3, 3), and (4, 4), the standard interpolation methods (such as Newton’s or Lagrange’s methods) would give us a unique polynomial of degree 3. However, constructing a polynomial of degree exactly 5 requires a different approach.\n\n\n\nTheorem 3.2 (Main Theorem of Polynomial Interpolation) guarantees that a unique polynomial of degree n-1 exists for n distinct points.\nTo create a polynomial of degree 5 when you only have 4 points, you need to add an extra term that still passes through all the original points but elevates the polynomial’s degree.\n\n\n\n\nWe will use Newton’s divided differences to first construct the degree 3 polynomial, and then extend it to a degree 5 polynomial.\n\n\n\nWe will use the given points (x_1, y_1) = (1, 1), (x_2, y_2) = (2, 3), (x_3, y_3) = (3, 3), and (x_4, y_4) = (4, 4) to compute the divided differences and build the polynomial.\n\n\n\n\n\n\n\n\n\n\nx\nf(x)\nFirst Difference f[x_i, x_{i+1}]\nSecond Difference f[x_i, x_{i+1}, x_{i+2}]\nThird Difference f[x_i, x_{i+1}, x_{i+2}, x_{i+3}]\n\n\n\n\n1\n1\n\n\n\n\n\n2\n3\n\\frac{3 - 1}{2 - 1} = 2\n\n\n\n\n3\n3\n\\frac{3 - 3}{3 - 2} = 0\n\\frac{0 - 2}{3 - 1} = -1\n\n\n\n4\n4\n\\frac{4 - 3}{4 - 3} = 1\n\\frac{1 - 0}{4 - 2} = \\frac{1}{2}\n\\frac{\\frac{1}{2} - (-1)}{4 - 1} = \\frac{3}{6} = \\frac{1}{2}\n\n\n\n\n\n\nThe Newton’s divided difference form of the interpolating polynomial is:\n\nP_3(x) = f[x_1] + f[x_1, x_2](x - x_1) + f[x_1, x_2, x_3](x - x_1)(x - x_2) + f[x_1, x_2, x_3, x_4](x - x_1)(x - x_2)(x - x_3)\n\nSubstituting the values from the divided difference table:\n\nP_3(x) = 1 + 2(x - 1) - (x - 1)(x - 2) + \\frac{1}{2}(x - 1)(x - 2)(x - 3)\n\n\n\n\nTo create a degree 5 polynomial, we add an extra term c(x - 1)(x - 2)(x - 3)(x - 4), which evaluates to zero at the given points:\n\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nSubstitute P_3(x):\n\nP_5(x) = 1 + 2(x - 1) - (x - 1)(x - 2) + \\frac{1}{2}(x - 1)(x - 2)(x - 3) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\n\n\n\nThe value of c does not affect the polynomial at the given points. The additional term evaluates to 0 at x = 1, 2, 3, 4, so no matter what c is, the polynomial will pass through the points (1, 1), (2, 3), (3, 3), (4, 4).\nHowever, changing c will affect the polynomial’s behavior outside the given points. For different values of c, the polynomial will look different outside the interpolation points.\n\n\n\nThus, the degree 5 polynomial is:\n\nP_5(x) = 1 + 2(x - 1) - (x - 1)(x - 2) + \\frac{1}{2}(x - 1)(x - 2)(x - 3) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nThis polynomial passes through the points (1, 1), (2, 3), (3, 3), (4, 4), and c is an arbitrary constant that influences how the polynomial behaves outside of those points."
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w03/exercise3-2-2.html",
    "href": "mathematics/numerical-analysis/homework/w03/exercise3-2-2.html",
    "title": "Exercise 3.2.2 (C3-P4)",
    "section": "",
    "text": "Given the data points (1, 0), (2, \\ln 2), and (4, \\ln 4), find the degree 2 interpolating polynomial.\n\nUse the result of (a) to approximate \\ln 3.\n\nUse Theorem 3.3 to give an error bound for the approximation in part (b).\n\nCompare the actual error to your error bound."
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w03/exercise3-2-2.html#problem",
    "href": "mathematics/numerical-analysis/homework/w03/exercise3-2-2.html#problem",
    "title": "Exercise 3.2.2 (C3-P4)",
    "section": "",
    "text": "Given the data points (1, 0), (2, \\ln 2), and (4, \\ln 4), find the degree 2 interpolating polynomial.\n\nUse the result of (a) to approximate \\ln 3.\n\nUse Theorem 3.3 to give an error bound for the approximation in part (b).\n\nCompare the actual error to your error bound."
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w03/exercise3-2-2.html#key-concepts",
    "href": "mathematics/numerical-analysis/homework/w03/exercise3-2-2.html#key-concepts",
    "title": "Exercise 3.2.2 (C3-P4)",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nError in Polynomial Interpolation:\nThe error between the actual function f(x) and the interpolated polynomial P_{n-1}(x) is given by the following bound:\n\n|f(x) - P_{n-1}(x)| \\leq \\left| \\frac{(x - x_0)(x - x_1) \\dots (x - x_n)}{n!} f^{(n)}(c) \\right|\n\nwhere:\n\nn is the number of interpolation points.\nx_0, x_1, \\dots, x_n are the known data points.\nf^{(n)}(c) is the n-th derivative of the actual function f(x), evaluated at some point c in the interval [x_0, x_n]."
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w03/exercise3-2-2.html#solution",
    "href": "mathematics/numerical-analysis/homework/w03/exercise3-2-2.html#solution",
    "title": "Exercise 3.2.2 (C3-P4)",
    "section": "Solution",
    "text": "Solution\n\n(a) Finding the Degree 2 Interpolating Polynomial\nGiven the points (1, 0), (2, \\ln 2), and (4, \\ln 4), we apply the Lagrange interpolation formula.\n\nLagrange Basis Polynomials:\n\nL_0(x):\n\nL_0(x) = \\frac{(x - 2)(x - 4)}{(1 - 2)(1 - 4)} = \\frac{(x - 2)(x - 4)}{3}\n\nL_1(x):\n\nL_1(x) = \\frac{(x - 1)(x - 4)}{(2 - 1)(2 - 4)} = -\\frac{(x - 1)(x - 4)}{2}\n\nL_2(x): \nL_2(x) = \\frac{(x - 1)(x - 2)}{(4 - 1)(4 - 2)} = \\frac{(x - 1)(x - 2)}{6}\n\n\n\n\nPolynomial Construction:\nNow, the interpolating polynomial becomes:\n\nP(x) = y_0 L_0(x) + y_1 L_1(x) + y_2 L_2(x)\n\nSince y_0 = 0, we have:\n\nP(x) = \\ln 2 \\cdot \\left(-\\frac{(x - 1)(x - 4)}{2}\\right) + \\ln 4 \\cdot \\frac{(x - 1)(x - 2)}{6}\n\nWe know that \\ln 4 = 2 \\ln 2, so the polynomial simplifies to:\n\nP(x) = \\ln 2 \\cdot \\left(-\\frac{(x - 1)(x - 4)}{2} + \\frac{(x - 1)(x - 2)}{3}\\right)\n\n\n\n\n(b) Approximation of \\ln 3\nTo approximate \\ln 3, substitute x = 3 into the polynomial:\n\nP(3) = \\ln 2 \\cdot \\left(-\\frac{(3 - 1)(3 - 4)}{2} + \\frac{(3 - 1)(3 - 2)}{3}\\right)\n\nSimplifying:\n\nP(3) = \\ln 2 \\cdot \\left(1 + \\frac{2}{3}\\right) = \\ln 2 \\cdot \\frac{5}{3}\n\nSince \\ln 2 \\approx 0.6931, we have:\n\nP(3) \\approx \\frac{5}{3} \\cdot 0.6931 \\approx 1.1552\n\nThus, the approximation for \\ln 3 is:\n\nP(3) \\approx 1.1552\n\n\n\n(c) Error Bound using Theorem 3.3\nThe error formula for degree 2 interpolation is:\n\n|f(x) - P(x)| \\leq \\left| \\frac{(x - x_0)(x - x_1)(x - x_2)}{3!} f^{(3)}(c) \\right|\n\nwhere f(x) = \\ln(x) and f^{(3)}(x) = \\frac{2}{x^3}. The maximum of f^{(3)}(x) occurs at x = 1, giving:\n\nf^{(3)}(1) = 2\n\nSubstituting into the error formula for x = 3:\n\nE(3) = \\frac{(3 - 1)(3 - 2)(3 - 4)}{6} \\cdot 2 = \\frac{(2)(1)(-1)}{6} \\cdot 2 = -\\frac{4}{6} = -\\frac{2}{3}\n\nTherefore, the error bound is approximately -\\frac{2}{3} \\approx -0.6667.\n\n\n(d) Comparison of Actual Error and Error Bound\nThe actual value of \\ln 3 is approximately:\n\n\\ln 3 \\approx 1.0986\n\nOur approximation was P(3) \\approx 1.1552. Therefore, the actual error is:\n\n\\text{Actual error} = |1.0986 - 1.1552| \\approx 0.0566\n\nThe error bound is approximately 0.6667, which is larger than the actual error. This confirms that the actual error is well within the error bound, as expected."
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w03/exercise3-2-6.html",
    "href": "mathematics/numerical-analysis/homework/w03/exercise3-2-6.html",
    "title": "Exercise 3.1.6 (C3-P6)",
    "section": "",
    "text": "How do we construct a polynomial of degree exactly 5 that interpolates four points?\nGiven four points (1, 1), (2, 3), (3, 3), and (4, 4), the standard interpolation methods (such as Newton’s or Lagrange’s methods) would give us a unique polynomial of degree 3. However, constructing a polynomial of degree exactly 5 requires a different approach.\n\n\n\nTheorem 3.2 (Main Theorem of Polynomial Interpolation) guarantees that a unique polynomial of degree n-1 exists for n distinct points.\nTo create a polynomial of degree 5 when you only have 4 points, you need to add an extra term that still passes through all the original points but elevates the polynomial’s degree."
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w03/exercise3-2-6.html#problem",
    "href": "mathematics/numerical-analysis/homework/w03/exercise3-2-6.html#problem",
    "title": "Exercise 3.1.6 (C3-P6)",
    "section": "",
    "text": "How do we construct a polynomial of degree exactly 5 that interpolates four points?\nGiven four points (1, 1), (2, 3), (3, 3), and (4, 4), the standard interpolation methods (such as Newton’s or Lagrange’s methods) would give us a unique polynomial of degree 3. However, constructing a polynomial of degree exactly 5 requires a different approach.\n\n\n\nTheorem 3.2 (Main Theorem of Polynomial Interpolation) guarantees that a unique polynomial of degree n-1 exists for n distinct points.\nTo create a polynomial of degree 5 when you only have 4 points, you need to add an extra term that still passes through all the original points but elevates the polynomial’s degree."
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w03/exercise3-2-6.html#solution",
    "href": "mathematics/numerical-analysis/homework/w03/exercise3-2-6.html#solution",
    "title": "Exercise 3.1.6 (C3-P6)",
    "section": "Solution:",
    "text": "Solution:\nTo create a degree 5 polynomial that still passes through all four points (1, 1), (2, 3), (3, 3), and (4, 4), follow these steps:\n\nFind the degree 3 polynomial P_3(x):\n\nUse either Newton’s divided differences or Lagrange interpolation to find the polynomial of degree 3 that passes through the given points. Let’s call this polynomial P_3(x).\n\nAdd a higher-degree term:\n\nTo turn this degree 3 polynomial into a degree 5 polynomial, we add a term that evaluates to zero at the given points:\n\n\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nThe new term (x - 1)(x - 2)(x - 3)(x - 4) is zero at x = 1, 2, 3, 4, so it won’t affect the interpolation at those points. By multiplying this term by a constant c, we ensure that the degree of the polynomial is elevated to 5, but the polynomial still interpolates the original points.\n\n\nFinal Degree 5 Polynomial\nThus, the degree 5 polynomial is given by:\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\nP_3(x) is the degree 3 polynomial found using standard interpolation methods.\nc is an arbitrary constant that determines the shape of the polynomial outside the interpolation points.\n\n\n\nExample\nLet’s assume the degree 3 polynomial P_3(x) through the points (1, 1), (2, 3), (3, 3), (4, 4) is:\nP_3(x) = 1 + 2(x - 1) + 3(x - 1)(x - 2)\nThen the degree 5 polynomial becomes:\nP_5(x) = P_3(x) + c(x - 1)(x - 2)(x - 3)(x - 4)\n\n\nDoes the Value of c Matter?\nNo, the value of c does not affect the interpolation at the given points. Since the additional term evaluates to 0 at x = 1, 2, 3, 4, the polynomial will still pass through the points, regardless of c.\nHowever, changing c affects the behavior of the polynomial outside the interpolation points. For different values of c, the polynomial will look different beyond the four points, but it will still pass through (1, 1), $(2, 3) $, $ (3, 3) $, and $ (4, 4)$."
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w04/exercise5-2-1a.html",
    "href": "mathematics/numerical-analysis/homework/w04/exercise5-2-1a.html",
    "title": "Exercise 5.2.1a (C5-P1)",
    "section": "",
    "text": "Apply the composite Trapezoid Rule with m = 1, 2, and 4 panels to approximate the integral. Compute the error by comparing with the exact value from calculus.\n\n\\int_0^1 x^2 \\, dx\n\n\n\n\n\n\nThe formula for the composite trapezoidal rule is:\n\nT_m = \\frac{h}{2} \\left( f(a) + 2 \\sum_{i=1}^{m-1} f(x_i) + f(b) \\right)\n\nwhere:\n\nh = \\frac{b - a}{m}\nx_i = a + i \\cdot h for i = 1, 2, \\ldots, m-1"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w04/exercise5-2-1a.html#problem",
    "href": "mathematics/numerical-analysis/homework/w04/exercise5-2-1a.html#problem",
    "title": "Exercise 5.2.1a (C5-P1)",
    "section": "",
    "text": "Apply the composite Trapezoid Rule with m = 1, 2, and 4 panels to approximate the integral. Compute the error by comparing with the exact value from calculus.\n\n\\int_0^1 x^2 \\, dx\n\n\n\n\n\n\nThe formula for the composite trapezoidal rule is:\n\nT_m = \\frac{h}{2} \\left( f(a) + 2 \\sum_{i=1}^{m-1} f(x_i) + f(b) \\right)\n\nwhere:\n\nh = \\frac{b - a}{m}\nx_i = a + i \\cdot h for i = 1, 2, \\ldots, m-1"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w04/exercise5-2-1a.html#solution",
    "href": "mathematics/numerical-analysis/homework/w04/exercise5-2-1a.html#solution",
    "title": "Exercise 5.2.1a (C5-P1)",
    "section": "Solution:",
    "text": "Solution:\n\n1. Exact Value of the Integral\nWe first compute the exact value of the integral:\n\n\\int_0^1 x^2 \\, dx = \\left[ \\frac{x^3}{3} \\right]_0^1 = \\frac{1^3}{3} - \\frac{0^3}{3} = \\frac{1}{3} \\approx 0.3333\n\n\n\n2. Approximations Using the Composite Trapezoidal Rule\n\nCase m = 1:\n\nh = \\frac{1 - 0}{1} = 1\nPoints: x_0 = 0, x_1 = 1\nApproximation:\n\n\nT_1 = \\frac{1}{2} \\left( f(0) + f(1) \\right) = \\frac{1}{2} \\left( 0^2 + 1^2 \\right) = \\frac{1}{2} \\cdot 1 = 0.5\n\n\nError:\n\n\n\\text{Error} = \\left| 0.3333 - 0.5 \\right| = 0.1667\n\n\n\nCase m = 2:\n\nh = \\frac{1 - 0}{2} = 0.5\nPoints: x_0 = 0, x_1 = 0.5, x_2 = 1\nApproximation:\n\n\nT_2 = \\frac{0.5}{2} \\left( f(0) + 2 \\cdot f(0.5) + f(1) \\right) = \\frac{0.5}{2} \\left( 0^2 + 2 \\cdot 0.5^2 + 1^2 \\right)\n\n\nT_2 = \\frac{0.5}{2} \\cdot (0 + 2 \\cdot 0.25 + 1) = \\frac{0.5}{2} \\cdot 1.5 = 0.375\n\n\nError:\n\n\n\\text{Error} = \\left| 0.3333 - 0.375 \\right| = 0.0417\n\n\n\nCase m = 4:\n\nh = \\frac{1 - 0}{4} = 0.25\nPoints: x_0 = 0, x_1 = 0.25, x_2 = 0.5, x_3 = 0.75, x_4 = 1\nApproximation:\n\n\nT_4 = \\frac{0.25}{2} \\left( f(0) + 2 \\cdot \\left( f(0.25) + f(0.5) + f(0.75) \\right) + f(1) \\right)\n\n\nT_4 = \\frac{0.25}{2} \\left( 0^2 + 2 \\cdot (0.25^2 + 0.5^2 + 0.75^2) + 1^2 \\right)\n\n\nT_4 = \\frac{0.25}{2} \\cdot (0 + 2 \\cdot (0.0625 + 0.25 + 0.5625) + 1) = \\frac{0.25}{2} \\cdot 2.75 = 0.34375\n\n\nError:\n\n\n\\text{Error} = \\left| 0.3333 - 0.34375 \\right| = 0.0104\n\n\n\n\n3. Summary of Results\n\n\n\nm\nApproximation\nError\n\n\n\n\n1\n0.5000\n0.1667\n\n\n2\n0.3750\n0.0417\n\n\n4\n0.3438\n0.0104\n\n\n\n\n\n4. Conclusion\nAs the number of panels m increases, the approximation becomes more accurate, and the error decreases. This demonstrates that the composite trapezoidal rule converges to the exact value as the number of panels increases."
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w06/exercise2-1-2a.html",
    "href": "mathematics/numerical-analysis/homework/w06/exercise2-1-2a.html",
    "title": "Exercise 2.1.2a (C2-P1)",
    "section": "",
    "text": "2.1.2a\n\n\n\nSolve the following system of equations using Gaussian elimination:\n\n\\begin{aligned}\n2x - 2y - z &= -2, \\\\\n4x + y - 2z &= 1, \\\\\n-2x + y - z &= -3.\n\\end{aligned}"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w06/exercise2-1-2a.html#problem",
    "href": "mathematics/numerical-analysis/homework/w06/exercise2-1-2a.html#problem",
    "title": "Exercise 2.1.2a (C2-P1)",
    "section": "",
    "text": "2.1.2a\n\n\n\nSolve the following system of equations using Gaussian elimination:\n\n\\begin{aligned}\n2x - 2y - z &= -2, \\\\\n4x + y - 2z &= 1, \\\\\n-2x + y - z &= -3.\n\\end{aligned}"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w06/exercise2-1-2a.html#solution",
    "href": "mathematics/numerical-analysis/homework/w06/exercise2-1-2a.html#solution",
    "title": "Exercise 2.1.2a (C2-P1)",
    "section": "Solution:",
    "text": "Solution:\n\nRepresenting the System in Augmented Matrix Form\nThe system can be written in augmented matrix form as:\n\n\\begin{bmatrix}\n2 & -2 & -1 & -2 \\\\\n4 & 1 & -2 & 1 \\\\\n-2 & 1 & -1 & -3\n\\end{bmatrix}.\n\n\n\nRow Operations for Gaussian Elimination\n\nNormalize Row 1\nDivide Row 1 by 2:\n\n\\begin{bmatrix}\n1 & -1 & -\\frac{1}{2} & -1 \\\\\n4 & 1 & -2 & 1 \\\\\n-2 & 1 & -1 & -3\n\\end{bmatrix}.\n\n\n\nEliminate the First Column in Rows 2 and 3\n\nR_2 \\to R_2 - 4 \\cdot R_1\nR_3 \\to R_3 + 2 \\cdot R_1\n\n\n\\begin{bmatrix}\n1 & -1 & -\\frac{1}{2} & -1 \\\\\n0 & 5 & -\\frac{6}{2} & 5 \\\\\n0 & -1 & -2 & -5\n\\end{bmatrix}.\n\n\n\nNormalize Row 2\nDivide Row 2 by 5:\n\n\\begin{bmatrix}\n1 & -1 & -\\frac{1}{2} & -1 \\\\\n0 & 1 & -\\frac{3}{5} & 1 \\\\\n0 & -1 & -2 & -5\n\\end{bmatrix}.\n\n\n\nEliminate the Second Column in Rows 1 and 3\n\nR_1 \\to R_1 + R_2\nR_3 \\to R_3 + R_2\n\n\n\\begin{bmatrix}\n1 & 0 & -\\frac{7}{10} & 0 \\\\\n0 & 1 & -\\frac{3}{5} & 1 \\\\\n0 & 0 & -\\frac{13}{5} & -4\n\\end{bmatrix}.\n\n\n\nNormalize Row 3\nDivide Row 3 by -\\frac{13}{5}:\n\n\\begin{bmatrix}\n1 & 0 & -\\frac{7}{10} & 0 \\\\\n0 & 1 & -\\frac{3}{5} & 1 \\\\\n0 & 0 & 1 & \\frac{20}{13}\n\\end{bmatrix}.\n\n\n\nBack Substitution\n\nUpdate R_2: R_2 \\to R_2 + \\frac{3}{5} \\cdot R_3\nUpdate R_1: R_1 \\to R_1 + \\frac{7}{10} \\cdot R_3\n\n\n\\begin{bmatrix}\n1 & 0 & 0 & \\frac{14}{13} \\\\\n0 & 1 & 0 & \\frac{19}{13} \\\\\n0 & 0 & 1 & \\frac{20}{13}\n\\end{bmatrix}.\n\n\n\n\n\n\n\nFinal Answer:\n\n\n\n\nx = \\frac{14}{13}, \\quad y = \\frac{19}{13}, \\quad z = \\frac{20}{13}."
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w07/exercise2-4-6.html",
    "href": "mathematics/numerical-analysis/homework/w07/exercise2-4-6.html",
    "title": "Exercise 2.4.6 (C2-P10)",
    "section": "",
    "text": "(a) Write down the 4 \\times 4 matrix P such that multiplying a matrix on the left by P causes the second and fourth rows of the matrix to be exchanged.\n(b) What is the effect of multiplying on the right by P? Demonstrate with an example."
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w07/exercise2-4-6.html#problem",
    "href": "mathematics/numerical-analysis/homework/w07/exercise2-4-6.html#problem",
    "title": "Exercise 2.4.6 (C2-P10)",
    "section": "",
    "text": "(a) Write down the 4 \\times 4 matrix P such that multiplying a matrix on the left by P causes the second and fourth rows of the matrix to be exchanged.\n(b) What is the effect of multiplying on the right by P? Demonstrate with an example."
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w07/exercise2-4-6.html#solution",
    "href": "mathematics/numerical-analysis/homework/w07/exercise2-4-6.html#solution",
    "title": "Exercise 2.4.6 (C2-P10)",
    "section": "Solution:",
    "text": "Solution:\n\n(a) Constructing the Permutation Matrix P\nTo create a permutation matrix P that exchanges the second and fourth rows when multiplied on the left, start with the 4 \\times 4 identity matrix I_4 and swap its second and fourth rows.\nIdentity Matrix I_4:\n\nI_4 = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n\\end{pmatrix}\n\nPermutation Matrix P (after swapping rows 2 and 4):\n\nP = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n\\end{pmatrix}\n\nWhen P multiplies any 4 \\times n matrix A on the left (PA), it exchanges the second and fourth rows of A.\n\n\n(b) Effect of Multiplying on the Right by P\nMultiplying a matrix on the right by P rearranges its columns (instead of rows) according to the pattern defined by P. In this case, it specifically swaps the matrix’s second and fourth columns.\nDemonstration with an Example:\nLet’s consider a 4 \\times 4 matrix A:\n\nA = \\begin{pmatrix}\na_{11} & a_{12} & a_{13} & a_{14} \\\\\na_{21} & a_{22} & a_{23} & a_{24} \\\\\na_{31} & a_{32} & a_{33} & a_{34} \\\\\na_{41} & a_{42} & a_{43} & a_{44} \\\\\n\\end{pmatrix}\n\nCompute AP:\n\nAP = A \\times P = A \\times \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n\\end{pmatrix}\n\nResulting Matrix AP:\n\nAP = \\begin{pmatrix}\na_{11} & a_{14} & a_{13} & a_{12} \\\\\na_{21} & a_{24} & a_{23} & a_{22} \\\\\na_{31} & a_{34} & a_{33} & a_{32} \\\\\na_{41} & a_{44} & a_{43} & a_{42} \\\\\n\\end{pmatrix}\n\nNumerical Example:\n\nA = \\begin{pmatrix}\n1 & 2 & 3 & 4 \\\\\n5 & 6 & 7 & 8 \\\\\n9 & 10 & 11 & 12 \\\\\n13 & 14 & 15 & 16 \\\\\n\\end{pmatrix}\n\nCompute AP:\n\nAP = A \\times \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 4 & 3 & 2 \\\\\n5 & 8 & 7 & 6 \\\\\n9 & 12 & 11 & 10 \\\\\n13 & 16 & 15 & 14 \\\\\n\\end{pmatrix}\n\nFinal Answer\n(a) The 4 \\times 4 permutation matrix P that exchanges the second and fourth rows when multiplied on the left is:\n\nP = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n\\end{pmatrix}\n\n(b) Multiplying on the right by P exchanges the second and fourth columns of a matrix. For example, for the matrix:\n\nA = \\begin{pmatrix}\n1 & 2 & 3 & 4 \\\\\n5 & 6 & 7 & 8 \\\\\n9 & 10 & 11 & 12 \\\\\n13 & 14 & 15 & 16 \\\\\n\\end{pmatrix}\n\nMultiplying on the right by P yields:\n\nAP = \\begin{pmatrix}\n1 & 4 & 3 & 2 \\\\\n5 & 8 & 7 & 6 \\\\\n9 & 12 & 11 & 10 \\\\\n13 & 16 & 15 & 14 \\\\\n\\end{pmatrix}\n\nwhich is A with its second and fourth columns exchanged."
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w09/exercise4-3-1d.html",
    "href": "mathematics/numerical-analysis/homework/w09/exercise4-3-1d.html",
    "title": "Exercise 4.3.1d",
    "section": "",
    "text": "4.3.1d\n\n\n\nApply classical Gram–Schmidt orthogonalization to find the full QR factorization of the matrix:\n\n\\mathbf{A} = \\begin{bmatrix} 4 & 8 & 1 \\\\ 0 & 2 & -2 \\\\ 3 & 6 & 7 \\end{bmatrix}"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w09/exercise4-3-1d.html#define-the-columns-of-matrix-mathbfa",
    "href": "mathematics/numerical-analysis/homework/w09/exercise4-3-1d.html#define-the-columns-of-matrix-mathbfa",
    "title": "Exercise 4.3.1d",
    "section": "Define the Columns of Matrix \\mathbf{A}",
    "text": "Define the Columns of Matrix \\mathbf{A}\nRepresent the columns of \\mathbf{A} as vectors:\n\n\\mathbf{a}_1 = \\begin{bmatrix} 4 \\\\ 0 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{a}_2 = \\begin{bmatrix} 8 \\\\ 2 \\\\ 6 \\end{bmatrix}, \\quad \\mathbf{a}_3 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 7 \\end{bmatrix}"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w09/exercise4-3-1d.html#finding-mathbfq_1",
    "href": "mathematics/numerical-analysis/homework/w09/exercise4-3-1d.html#finding-mathbfq_1",
    "title": "Exercise 4.3.1d",
    "section": "Finding \\mathbf{q}_1",
    "text": "Finding \\mathbf{q}_1\nTo find the first orthonormal vector \\mathbf{q}_1, normalize \\mathbf{a}_1.\n\nCalculate the norm of \\mathbf{a}_1:\n\n\\|\\mathbf{a}_1\\| = \\sqrt{4^2 + 0^2 + 3^2} = \\sqrt{16 + 9} = \\sqrt{25} = 5\n\nNormalize \\mathbf{a}_1:\n\n\\mathbf{q}_1 = \\frac{\\mathbf{a}_1}{\\|\\mathbf{a}_1\\|} = \\frac{1}{5} \\begin{bmatrix} 4 \\\\ 0 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} \\frac{4}{5} \\\\ 0 \\\\ \\frac{3}{5} \\end{bmatrix}"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w09/exercise4-3-1d.html#finding-mathbfq_2",
    "href": "mathematics/numerical-analysis/homework/w09/exercise4-3-1d.html#finding-mathbfq_2",
    "title": "Exercise 4.3.1d",
    "section": "Finding \\mathbf{q}_2",
    "text": "Finding \\mathbf{q}_2\nTo find \\mathbf{q}_2, project \\mathbf{a}_2 onto \\mathbf{q}_1 and then subtract this projection from \\mathbf{a}_2.\n\nCalculate the projection of \\mathbf{a}_2 onto \\mathbf{q}_1:\n\n\\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_2 = \\left( \\mathbf{a}_2 \\cdot \\mathbf{q}_1 \\right) \\mathbf{q}_1\n\nSince \\mathbf{q}_1 is a unit vector, \\mathbf{q}_1 \\cdot \\mathbf{q}_1 = 1.\nCompute \\mathbf{a}_2 \\cdot \\mathbf{q}_1:\n\n\\mathbf{a}_2 \\cdot \\mathbf{q}_1 = \\begin{bmatrix} 8 \\\\ 2 \\\\ 6 \\end{bmatrix} \\cdot \\begin{bmatrix} \\frac{4}{5} \\\\ 0 \\\\ \\frac{3}{5} \\end{bmatrix} = \\frac{32}{5} + 0 + \\frac{18}{5} = \\frac{50}{5} = 10\n\nCalculate \\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_2:\n\n\\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_2 = 10 \\begin{bmatrix} \\frac{4}{5} \\\\ 0 \\\\ \\frac{3}{5} \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ 0 \\\\ 6 \\end{bmatrix}\n\nCalculate \\mathbf{u}_2:\nSubtracting the projection from \\mathbf{a}_2 gives \\mathbf{u}_2:\n\n\\mathbf{u}_2 = \\mathbf{a}_2 - \\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_2 = \\begin{bmatrix} 8 \\\\ 2 \\\\ 6 \\end{bmatrix} - \\begin{bmatrix} 8 \\\\ 0 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 2 \\\\ 0 \\end{bmatrix}\n\nNormalize \\mathbf{u}_2 to obtain \\mathbf{q}_2:\nThe norm of \\mathbf{u}_2 is:\n\n\\|\\mathbf{u}_2\\| = \\sqrt{0^2 + 2^2 + 0^2} = 2\n\nThus,\n\n\\mathbf{q}_2 = \\frac{\\mathbf{u}_2}{\\|\\mathbf{u}_2\\|} = \\frac{1}{2} \\begin{bmatrix} 0 \\\\ 2 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w09/exercise4-3-1d.html#finding-mathbfq_3",
    "href": "mathematics/numerical-analysis/homework/w09/exercise4-3-1d.html#finding-mathbfq_3",
    "title": "Exercise 4.3.1d",
    "section": "Finding \\mathbf{q}_3",
    "text": "Finding \\mathbf{q}_3\nTo find \\mathbf{q}_3, project \\mathbf{a}_3 onto both \\mathbf{q}_1 and \\mathbf{q}_2, and then subtract these projections from \\mathbf{a}_3.\n\nCalculate the projection of \\mathbf{a}_3 onto \\mathbf{q}_1:\n\n\\mathbf{a}_3 \\cdot \\mathbf{q}_1 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 7 \\end{bmatrix} \\cdot \\begin{bmatrix} \\frac{4}{5} \\\\ 0 \\\\ \\frac{3}{5} \\end{bmatrix} = \\frac{4}{5} + 0 + \\frac{21}{5} = \\frac{25}{5} = 5\n\nThen,\n\n\\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_3 = 5 \\begin{bmatrix} \\frac{4}{5} \\\\ 0 \\\\ \\frac{3}{5} \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 0 \\\\ 3 \\end{bmatrix}\n\nCalculate the projection of \\mathbf{a}_3 onto \\mathbf{q}_2:\n\n\\mathbf{a}_3 \\cdot \\mathbf{q}_2 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 7 \\end{bmatrix} \\cdot \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = -2\n\nThen,\n\n\\text{proj}_{\\mathbf{q}_2} \\mathbf{a}_3 = -2 \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ -2 \\\\ 0 \\end{bmatrix}\n\nCalculate \\mathbf{u}_3:\nSubtracting both projections from \\mathbf{a}_3 yields \\mathbf{u}_3:\n\n\\mathbf{u}_3 = \\mathbf{a}_3 - \\text{proj}_{\\mathbf{q}_1} \\mathbf{a}_3 - \\text{proj}_{\\mathbf{q}_2} \\mathbf{a}_3 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 7 \\end{bmatrix} - \\begin{bmatrix} 4 \\\\ 0 \\\\ 3 \\end{bmatrix} - \\begin{bmatrix} 0 \\\\ -2 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} -3 \\\\ 0 \\\\ 4 \\end{bmatrix}\n\nNormalize \\mathbf{u}_3 to obtain \\mathbf{q}_3:\nThe norm of \\mathbf{u}_3 is:\n\n\\|\\mathbf{u}_3\\| = \\sqrt{(-3)^2 + 0^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5\n\nThus,\n\n\\mathbf{q}_3 = \\frac{\\mathbf{u}_3}{\\|\\mathbf{u}_3\\|} = \\frac{1}{5} \\begin{bmatrix} -3 \\\\ 0 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} -\\frac{3}{5} \\\\ 0 \\\\ \\frac{4}{5} \\end{bmatrix}"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w09/exercise4-3-1d.html#constructing-mathbfq-and-mathbfr-matrices",
    "href": "mathematics/numerical-analysis/homework/w09/exercise4-3-1d.html#constructing-mathbfq-and-mathbfr-matrices",
    "title": "Exercise 4.3.1d",
    "section": "Constructing \\mathbf{Q} and \\mathbf{R} Matrices",
    "text": "Constructing \\mathbf{Q} and \\mathbf{R} Matrices\n\nOrthogonal Matrix \\mathbf{Q}\nCombine \\mathbf{q}_1, \\mathbf{q}\\_2, and \\mathbf{q}_3 as columns to form \\mathbf{Q}:\n\n\\mathbf{Q} = \\begin{bmatrix} \\frac{4}{5} & 0 & -\\frac{3}{5} \\\\ 0 & 1 & 0 \\\\ \\frac{3}{5} & 0 & \\frac{4}{5} \\end{bmatrix}\n\n\n\nUpper Triangular Matrix \\mathbf{R}\nThe entries of \\mathbf{R} are calculated as the dot products of the original columns of \\mathbf{A} with the orthonormal vectors \\mathbf{q}_1, \\mathbf{q}_2, and \\mathbf{q}_3:\n\n\\mathbf{R} = \\begin{bmatrix} \\mathbf{a}_1 \\cdot \\mathbf{q}_1 & \\mathbf{a}_2 \\cdot \\mathbf{q}_1 & \\mathbf{a}_3 \\cdot \\mathbf{q}_1 \\\\ 0 & \\mathbf{a}_2 \\cdot \\mathbf{q}_2 & \\mathbf{a}_3 \\cdot \\mathbf{q}_2 \\\\ 0 & 0 & \\mathbf{a}_3 \\cdot \\mathbf{q}_3 \\end{bmatrix} = \\begin{bmatrix} 5 & 10 & 5 \\\\ 0 & 2 & -2 \\\\ 0 & 0 & 5 \\end{bmatrix}\n\n\n\n\n\n\n\nFinal Answer:\n\n\n\nThe QR factorization of \\mathbf{A} is:\n\n\\mathbf{A} = \\mathbf{Q} \\mathbf{R} = \\begin{bmatrix} \\frac{4}{5} & 0 & -\\frac{3}{5} \\\\ 0 & 1 & 0 \\\\ \\frac{3}{5} & 0 & \\frac{4}{5} \\end{bmatrix} \\begin{bmatrix} 5 & 10 & 5 \\\\ 0 & 2 & -2 \\\\ 0 & 0 & 5 \\end{bmatrix}"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w11/exercise10-1-1a.html",
    "href": "mathematics/numerical-analysis/homework/w11/exercise10-1-1a.html",
    "title": "Exercise 10.1.1a",
    "section": "",
    "text": "Problem:\n\n\n\n\n\n\n10.1.1a\n\n\n\nFind the Discrete Fourier Transform (DFT) of the following vector:\n\n\\begin{bmatrix}\n0 \\\\ 1 \\\\ 0 \\\\ -1\n\\end{bmatrix}\n\n\n\n\n\nSolution:\n\nCompute the DFT using the matrix formulation:\n\n\\mathbf{y} = F \\mathbf{x}\n\nWhere:\n\nF is the Fourier matrix, with entries: \nF_{k,j} = \\omega^{k j}, \\quad \\omega = e^{-i \\frac{2\\pi}{n}}\n\n\\mathbf{x} = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} is the input vector\n\n\n\\mathbf{y} = \\begin{bmatrix} y_0 \\\\ y_1 \\\\ y_2 \\\\ y_3 \\end{bmatrix} is the DFT of the input vector\n\n\n\nDefine Parameters\nThe input vector is:\n\n\\mathbf{x} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ -1 \\end{bmatrix}\n\nThe length of the vector is n = 4. The primitive root of unity is:\n\n\\omega = e^{-i \\frac{2\\pi}{n}} = e^{-i \\frac{\\pi}{2}} = \\cos\\left(\\frac{\\pi}{2}\\right) - i \\sin\\left(\\frac{\\pi}{2}\\right) = -i\n\n\n\nConstruct the Fourier Matrix F_4\nThe Fourier matrix F_4 is given by:\n\nF_4 =\n\\begin{bmatrix}\n\\omega^{0 \\cdot 0} & \\omega^{0 \\cdot 1} & \\omega^{0 \\cdot 2} & \\omega^{0 \\cdot 3} \\\\\n\\omega^{1 \\cdot 0} & \\omega^{1 \\cdot 1} & \\omega^{1 \\cdot 2} & \\omega^{1 \\cdot 3} \\\\\n\\omega^{2 \\cdot 0} & \\omega^{2 \\cdot 1} & \\omega^{2 \\cdot 2} & \\omega^{2 \\cdot 3} \\\\\n\\omega^{3 \\cdot 0} & \\omega^{3 \\cdot 1} & \\omega^{3 \\cdot 2} & \\omega^{3 \\cdot 3}\n\\end{bmatrix}\n\n\n\nCompute Powers of \\omega\n\n\\omega^0 = 1\n\\omega^1 = -i\n\\omega^2 = (-i)^2 = -1\n\\omega^3 = (-i)^3 = i\n\n\nFill in the Matrix F_4:\n\nF_4 =\n\\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & -i & -1 & i \\\\\n1 & -1 & 1 & -1 \\\\\n1 & i & -1 & -i\n\\end{bmatrix}\n\n\n\n\nPerform the Matrix Multiplication\n\ny_0:\n\ny_0 = 1 \\cdot 0 + 1 \\cdot 1 + 1 \\cdot 0 + 1 \\cdot (-1) = 0 + 1 + 0 - 1 = 0\n\ny_1:\n\ny_1 = 1 \\cdot 0 + (-i) \\cdot 1 + (-1) \\cdot 0 + i \\cdot (-1)\n\n\ny_1 = 0 - i + 0 - i = -2i\n\ny_2:\n\ny_2 = 1 \\cdot 0 + (-1) \\cdot 1 + 1 \\cdot 0 + (-1) \\cdot (-1)\n\n\ny_2 = 0 - 1 + 0 + 1 = 0\n\ny_3: \ny_3 = 1 \\cdot 0 + i \\cdot 1 + (-1) \\cdot 0 + (-i) \\cdot (-1)\n \ny_3 = 0 + i + 0 + i = 2i\n\n\n\n\n\n\n\n\nFinal Answer:\n\n\n\nThe DFT of \\mathbf{x} is:\n\n\\mathbf{y} = \\begin{bmatrix} 0 \\\\ -2i \\\\ 0 \\\\ 2i \\end{bmatrix}"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-1a.html",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-1a.html",
    "title": "Exercise 10.2.1a (C10-P3)",
    "section": "",
    "text": "10.2.1b\n\n\n\nUse the Discrete Fourier Transform (DFT) and Corollary 10.8 to find the trigonometric interpolating function for the following data:\n\n\n\nt\nx\n\n\n\n\n0\n0\n\n\n\\frac{1}{4}\n1\n\n\n\\frac{1}{2}\n0\n\n\n\\frac{3}{4}\n-1"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-1a.html#corollary-10.8",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-1a.html#corollary-10.8",
    "title": "Exercise 10.2.1a (C10-P3)",
    "section": "Corollary 10.8",
    "text": "Corollary 10.8\nFor an even integer n, let\n\nt_j = c + j \\frac{(d - c)}{n}, \\quad \\text{for } j = 0, \\dots, n-1,\n\nand let\n\nx = (x_0, \\dots, x_{n-1})\n\ndenote a vector of n real numbers. Define\n\n\\mathbf{a} + \\mathbf{b} i = F_n x,\n\nwhere F_n is the Discrete Fourier Transform. Then the function\n\nP_n(t) = \\frac{a_0}{\\sqrt{n}} + \\frac{2}{\\sqrt{n}} \\sum_{k=1}^{\\frac{n}{2} - 1} \\left( a_k \\cos\\left(\\frac{2\\pi k (t - c)}{d - c}\\right) - b_k \\sin\\left(\\frac{2\\pi k (t - c)}{d - c}\\right) \\right)\n+ \\frac{a_{n/2}}{\\sqrt{n}} \\cos\\left(\\frac{n\\pi (t - c)}{d - c}\\right)\n\nsatisfies\n\nP_n(t_j) = x_j, \\quad \\text{for } j = 0, \\dots, n - 1"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-1a.html#parameters",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-1a.html#parameters",
    "title": "Exercise 10.2.1a (C10-P3)",
    "section": "Parameters:",
    "text": "Parameters:\n\nt = \\left[0, \\frac{1}{4}, \\frac{1}{2}, \\frac{3}{4} \\right]\nx = [0, 1, 0, -1]\nInterval start: c = 0\nInterval end: d = 1\nNumber of data points: n = 4"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-1a.html#compute-the-dft-of-x_j",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-1a.html#compute-the-dft-of-x_j",
    "title": "Exercise 10.2.1a (C10-P3)",
    "section": "Compute the DFT of x_j",
    "text": "Compute the DFT of x_j\nThe Discrete Fourier Transform (DFT) of x = [x_0, \\dots, x_{n-1}]^T is the n-dimensional vector y = [y_0, \\dots, y_{n-1}], where \\omega = e^{-i 2 \\pi / n} and\n\ny_k = \\frac{1}{\\sqrt{n}} \\sum_{j=0}^{n-1} x_j \\omega^{jk}\n\n\nDFT Calculations:-\n\nFor k = 0:\n\ny_0 = \\frac{1}{\\sqrt{4}} \\sum_{j=0}^3 x_j \\cdot \\omega^{0 \\cdot j}\n\nSince \\omega^{0 \\cdot j} = 1:\n\ny_0 = \\frac{1}{2} (0 + 1 + 0 - 1) = 0\n\n\n\nFor k = 1:\n\ny_1 = \\frac{1}{\\sqrt{4}} \\sum_{j=0}^3 x_j \\cdot \\omega^j\n\nSubstitute x = [0, 1, 0, -1] and \\omega = e^{-i \\pi / 2} = -i:\n\ny_1 = \\frac{1}{2} (0 + (-i) \\cdot 1 + 0 + i \\cdot (-1))\n\n\ny_1 = \\frac{1}{2} (0 - i + 0 - i) = \\frac{1}{2} (-2i) = -i\n\n\n\nFor k = 2:\n\ny_2 = \\frac{1}{\\sqrt{4}} \\sum_{j=0}^3 x_j \\cdot \\omega^{2j}\n\nSince \\omega^2 = -1:\n\ny_2 = \\frac{1}{2} (0 \\cdot 1 + 1 \\cdot (-1) + 0 \\cdot 1 + (-1) \\cdot (-1))\n\n\ny_2 = \\frac{1}{2} (0 - 1 + 0 + 1) = \\frac{1}{2} (0) = 0\n\n\n\nFor k = 3:\n\ny_3 = \\frac{1}{\\sqrt{4}} \\sum_{j=0}^3 x_j \\cdot \\omega^{3j}\n\nSince \\omega^3 = i:\n\ny_3 = \\frac{1}{2} (0 \\cdot 1 + 1 \\cdot i + 0 \\cdot (-1) + (-1) \\cdot (-i))\n\n\ny_3 = \\frac{1}{2} (0 + i + 0 + i) = \\frac{1}{2} (2i) = i\n\n\n\n\nDFT Results:\n\ny_0 = 0\ny_1 = -i\ny_2 = 0\ny_3 = i\n\n\n\nFind a_k and b_k for Each y_k:\nFor each y_k = a_k + b_k i:\n\ny_0 = 0 \\implies a_0 = 0, b_0 = 0\ny_1 = -i \\implies a_1 = 0, b_1 = -1\ny_2 = 0 \\implies a_2 = 0, b_2 = 0\ny_3 = i \\implies a_3 = 0, b_3 = 1"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-1a.html#construct-the-trigonometric-interpolating-polynomial",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-1a.html#construct-the-trigonometric-interpolating-polynomial",
    "title": "Exercise 10.2.1a (C10-P3)",
    "section": "Construct the Trigonometric Interpolating Polynomial",
    "text": "Construct the Trigonometric Interpolating Polynomial\nUsing Corollary 10.8, the interpolating polynomial is:\n\nP_4(t) = \\frac{a_0}{\\sqrt{n}} + \\frac{2}{\\sqrt{n}} \\left( a_1 \\cos(2 \\pi t) - b_1 \\sin(2 \\pi t) \\right) + \\frac{a_2}{\\sqrt{n}} \\cos(4 \\pi t)\n\n\nStep-by-Step Substitution\n\nFirst term:\n\n\\frac{a_0}{\\sqrt{n}} = \\frac{0}{2} = 0\n\nSecond Term:\n\n\\frac{2}{\\sqrt{n}} \\left( a_1 \\cos(2 \\pi t) - b_1 \\sin(2 \\pi t) \\right)\n\nFrom a_1 = 0, b_1 = -1:\n\n\\frac{2}{2} \\left( 0 \\cdot \\cos(2 \\pi t) - (-1) \\cdot \\sin(2 \\pi t) \\right) = \\sin(2 \\pi t)\n\nThird Term:\n\n\\frac{a_2}{\\sqrt{n}} \\cos(4 \\pi t)\n\nFrom a_2 = 0:\n\n\\frac{0}{2} \\cos(4 \\pi t) = 0\n\n\n\n\nFinal Polynomial\nCombine all terms:\n\nP_4(t) = \\sin(2 \\pi t)\n\n\n\nVerification\nCheck P_4(t) at each data point:\n\nAt t = 0: P_4(0) = \\sin(0) = 0\nAt t = \\frac{1}{4}: P_4\\left(\\frac{1}{4}\\right) = \\sin\\left(\\frac{\\pi}{2}\\right) = 1\nAt t = \\frac{1}{2}: P_4\\left(\\frac{1}{2}\\right) = \\sin(\\pi) = 0\nAt t = \\frac{3}{4}: P_4\\left(\\frac{3}{4}\\right) = \\sin\\left(\\frac{3\\pi}{2}\\right) = -1\n\n\n\n\n\n\n\nFinal Answer:\n\n\n\nThe trigonometric interpolating polynomial is:\n\nP_4(t) = \\sin(2 \\pi t)"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-3a-plot.html",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-3a-plot.html",
    "title": "Exercise 10.2.3a Plot (C10-P5)",
    "section": "",
    "text": "10.2.3a Plot\n\n\n\nUse the trigonometric interpolating function found previously to verify that it interpolates the given data:\n\n\n\nt\nx\n\n\n\n\n0\n0\n\n\n\\frac{1}{8}\n1\n\n\n\\frac{1}{4}\n0\n\n\n\\frac{3}{8}\n-1\n\n\n\\frac{1}{2}\n0\n\n\n\\frac{5}{8}\n1\n\n\n\\frac{3}{4}\n0\n\n\n\\frac{7}{8}\n-1"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-3a-plot.html#plot",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-3a-plot.html#plot",
    "title": "Exercise 10.2.3a Plot (C10-P5)",
    "section": "Plot",
    "text": "Plot\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Given data\nt_data = np.array([0, 1/8, 1/4, 3/8, 1/2, 5/8, 3/4, 7/8])\nx_data = np.array([0, 1, 0, -1, 0, 1, 0, -1])\n\n# Interpolating function\ndef P_8(t):\n    return np.sin(4*np.pi*t)\n\nt_plot = np.linspace(0, 1, 400)\nx_plot = P_8(t_plot)\n\n# Plot the interpolating function\nplt.figure(figsize=(8,5))\nplt.plot(t_plot, x_plot, label='Interpolating Function', color='blue')\n\n# Plot the original data points\nplt.scatter(t_data, x_data, color='red', zorder=5, label='Data Points')\n\nplt.title('Trigonometric Interpolation')\nplt.xlabel('t')\nplt.ylabel('x')\nplt.grid(True)\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVerification:\n\n\n\nAs seen in the plot, the function P_8(t) passes through all the given data points."
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-3b-plot.html",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-3b-plot.html",
    "title": "Exercise 10.2.3b Plot (C10-P5)",
    "section": "",
    "text": "10.2.3b Plot\n\n\n\nUse the trigonometric interpolating function found previously to verify that it interpolates the given data:\n\n\n\nt\nx\n\n\n\n\n0\n1\n\n\n\\frac{1}{8}\n2\n\n\n\\frac{1}{4}\n1\n\n\n\\frac{3}{8}\n0\n\n\n\\frac{1}{2}\n1\n\n\n\\frac{5}{8}\n2\n\n\n\\frac{3}{4}\n1\n\n\n\\frac{7}{8}\n0"
  },
  {
    "objectID": "mathematics/numerical-analysis/homework/w12/exercise10-2-3b-plot.html#plot",
    "href": "mathematics/numerical-analysis/homework/w12/exercise10-2-3b-plot.html#plot",
    "title": "Exercise 10.2.3b Plot (C10-P5)",
    "section": "Plot",
    "text": "Plot\n\n\nShow code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Given data\nt_data = np.array([0, 1/8, 1/4, 3/8, 1/2, 5/8, 3/4, 7/8])\nx_data = np.array([1, 2, 1, 0, 1, 2, 1, 0])\n\n# Interpolating function\ndef P_8(t):\n    return 1 + np.sin(4*np.pi*t)\n\nt_plot = np.linspace(0, 1, 400)\nx_plot = P_8(t_plot)\n\n# Plot the interpolating function\nplt.figure(figsize=(8,5))\nplt.plot(t_plot, x_plot, label='Interpolating Function', color='blue')\n\n# Plot the original data points\nplt.scatter(t_data, x_data, color='red', zorder=5, label='Data Points')\n\nplt.title('Trigonometric Interpolation')\nplt.xlabel('t')\nplt.ylabel('x')\nplt.grid(True)\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVerification:\n\n\n\nAs seen in the plot, the function P_8(t) passes through all the given data points."
  },
  {
    "objectID": "mathematics/numerical-analysis/index.html",
    "href": "mathematics/numerical-analysis/index.html",
    "title": "NUMERICAL ANALYSIS",
    "section": "",
    "text": "Root Finding\nLinear Systems\nPolynomial Interpolation\nQuadrature\nThe Fast Fourier Transform\nTrigonometric Interpolation"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/gauss-seidel-method.html",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/gauss-seidel-method.html",
    "title": "The Gauss-Seidel Method for Solving Linear Systems",
    "section": "",
    "text": "The Gauss-Seidel Method is an iterative technique for solving systems of linear equations. It improves upon the Jacobi Method by using updated values of variables immediately within each iteration, often leading to faster convergence."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/gauss-seidel-method.html#overview",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/gauss-seidel-method.html#overview",
    "title": "The Gauss-Seidel Method for Solving Linear Systems",
    "section": "",
    "text": "The Gauss-Seidel Method is an iterative technique for solving systems of linear equations. It improves upon the Jacobi Method by using updated values of variables immediately within each iteration, often leading to faster convergence."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/gauss-seidel-method.html#the-gauss-seidel-method",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/gauss-seidel-method.html#the-gauss-seidel-method",
    "title": "The Gauss-Seidel Method for Solving Linear Systems",
    "section": "The Gauss-Seidel Method",
    "text": "The Gauss-Seidel Method\nConsider the system:\n\nA\\mathbf{x} = \\mathbf{b}\n\nwhere A is decomposed into:\n\nD: The diagonal components of A,\nL: The strictly lower triangular components of A,\nU: The strictly upper triangular components of A.\n\nThus:\n\nA = D + L + U\n\nRewriting the system:\n\n(D + L + U)\\mathbf{x} = \\mathbf{b}\n\nSolving for \\mathbf{x}, the iterative formula for the Gauss-Seidel Method is:\n\n\\mathbf{x}_{k+1} = D^{-1} \\left( \\mathbf{b} - U\\mathbf{x}_{k} - L\\mathbf{x}_{k+1} \\right), \\quad \\text{for } k = 0, 1, 2, \\dots\n\nHere:\n\n\\mathbf{x}_0: Initial guess vector.\nk: Iteration number."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/gauss-seidel-method.html#example",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/gauss-seidel-method.html#example",
    "title": "The Gauss-Seidel Method for Solving Linear Systems",
    "section": "Example",
    "text": "Example\n\nSystem of Equations\nConsider the system:\n\n4u + v + w = 7, \\quad u + 3v + w = 8, \\quad u + v + 5w = 6\n\n\n\nStep 1: Rearrange Equations\nRewriting each equation to isolate the variables:\n\nu = \\frac{7 - v - w}{4}, \\quad v = \\frac{8 - u - w}{3}, \\quad w = \\frac{6 - u - v}{5}\n\n\n\nStep 2: Iterative Updates\nStart with an initial guess, \\mathbf{x}_0 = \\begin{bmatrix} u_0 \\\\ v_0 \\\\ w_0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}. The iterations proceed as follows:\n\nIteration 1 (k = 1):\n\nu^{(1)} = \\frac{7 - 0 - 0}{4} = 1.75, \\quad\nv^{(1)} = \\frac{8 - 1.75 - 0}{3} = 2.083, \\quad\nw^{(1)} = \\frac{6 - 1.75 - 2.083}{5} = 0.833\n\n\n\\mathbf{x}^{(1)} = \\begin{bmatrix} 1.75 \\\\ 2.083 \\\\ 0.833 \\end{bmatrix}\n\nIteration 2 (k = 2):\nUsing updated values:\n\nu^{(2)} = \\frac{7 - 2.083 - 0.833}{4} = 1.521\n\n\nv^{(2)} = \\frac{8 - 1.521 - 0.833}{3} = 1.882\n\n\nw^{(2)} = \\frac{6 - 1.521 - 1.882}{5} = 0.919\n\n\n\\mathbf{x}^{(2)} = \\begin{bmatrix} 1.521 \\\\ 1.882 \\\\ 0.919 \\end{bmatrix}\n\nIteration 3 (k = 3):\nRepeating the process:\n\nu^{(3)} = 1.550, \\quad v^{(3)} = 1.866, \\quad w^{(3)} = 0.916\n\n\n\\mathbf{x}^{(3)} = \\begin{bmatrix} 1.550 \\\\ 1.866 \\\\ 0.916 \\end{bmatrix}"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/gauss-seidel-method.html#convergence",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/gauss-seidel-method.html#convergence",
    "title": "The Gauss-Seidel Method for Solving Linear Systems",
    "section": "Convergence",
    "text": "Convergence\nThe iterations converge to:\n\n\\mathbf{x} = \\begin{bmatrix} 1.6 \\\\ 1.8 \\\\ 0.9 \\end{bmatrix}"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/gauss-seidel-method.html#convergence-conditions",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/gauss-seidel-method.html#convergence-conditions",
    "title": "The Gauss-Seidel Method for Solving Linear Systems",
    "section": "Convergence Conditions",
    "text": "Convergence Conditions\nThe Gauss-Seidel Method converges if A is strictly diagonally dominant, meaning:\n\n|a_{ii}| &gt; \\sum_{j \\neq i} |a_{ij}|\n\nfor all rows i, or if A is symmetric positive definite."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/gauss-seidel-method.html#advantages",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/gauss-seidel-method.html#advantages",
    "title": "The Gauss-Seidel Method for Solving Linear Systems",
    "section": "Advantages",
    "text": "Advantages\n\nFaster convergence than the Jacobi Method due to the use of updated values.\nEfficient for solving large systems with appropriate properties."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/gauss-seidel-method.html#limitations",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/gauss-seidel-method.html#limitations",
    "title": "The Gauss-Seidel Method for Solving Linear Systems",
    "section": "Limitations",
    "text": "Limitations\n\nMay fail to converge if A is not strictly diagonally dominant or symmetric positive definite.\nSequential updates limit parallel computation."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/gauss-seidel-method.html#summary",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/gauss-seidel-method.html#summary",
    "title": "The Gauss-Seidel Method for Solving Linear Systems",
    "section": "Summary",
    "text": "Summary\nThe Gauss-Seidel Method is a powerful iterative technique for solving linear systems, particularly when A is strictly diagonally dominant or symmetric positive definite. Its reliance on updated values accelerates convergence but also imposes matrix property requirements for guaranteed success."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/jacobi-method.html",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/jacobi-method.html",
    "title": "The Jacobi Method for Solving Linear Systems",
    "section": "",
    "text": "The Jacobi Method is a type of fixed-point iteration specifically designed for solving systems of linear equations. The approach involves rewriting each equation to solve for one variable at a time and iterating over these equations to approximate the solution.\nFor a system of equations A\\mathbf{x} = \\mathbf{b}, the Jacobi Method works by isolating the i-th variable in the i-th equation, updating its value based on the other variables’ values from the previous iteration."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/jacobi-method.html#overview",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/jacobi-method.html#overview",
    "title": "The Jacobi Method for Solving Linear Systems",
    "section": "",
    "text": "The Jacobi Method is a type of fixed-point iteration specifically designed for solving systems of linear equations. The approach involves rewriting each equation to solve for one variable at a time and iterating over these equations to approximate the solution.\nFor a system of equations A\\mathbf{x} = \\mathbf{b}, the Jacobi Method works by isolating the i-th variable in the i-th equation, updating its value based on the other variables’ values from the previous iteration."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/jacobi-method.html#the-jacobi-method",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/jacobi-method.html#the-jacobi-method",
    "title": "The Jacobi Method for Solving Linear Systems",
    "section": "The Jacobi Method",
    "text": "The Jacobi Method\nConsider the system of equations:\n\nA\\mathbf{x} = \\mathbf{b}\n\nLet A be decomposed into three parts:\n\nD: The diagonal components of A.\nL: The strictly lower triangular components of A.\nU: The strictly upper triangular components of A.\n\nThus,\n\nA = D + L + U\n\nRewriting A\\mathbf{x} = \\mathbf{b}, we have:\n\nD\\mathbf{x} = \\mathbf{b} - (L + U)\\mathbf{x}\n\nSolving for \\mathbf{x}:\n\n\\mathbf{x} = D^{-1} \\left( \\mathbf{b} - (L + U)\\mathbf{x} \\right)\n\nThe Jacobi Method then iteratively updates the values of \\mathbf{x} as:\n\n\\mathbf{x}_{k+1} = D^{-1} \\left( \\mathbf{b} - (L + U)\\mathbf{x}_{k} \\right)\n\nwhere k denotes the iteration step."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/jacobi-method.html#example",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/jacobi-method.html#example",
    "title": "The Jacobi Method for Solving Linear Systems",
    "section": "Example",
    "text": "Example\n\nSystem of Equations\nConsider the system:\n\n3u + v = 5, \\quad u + 2v = 5\n\n\n\nStep 1: Rearrange Equations\nRewrite each equation to isolate the variables:\n\nu = \\frac{5 - v}{3}, \\quad v = \\frac{5 - u}{2}\n\n\n\nStep 2: Iterative Updates\nStart with an initial guess \\mathbf{x}^{(0)} = \\begin{bmatrix} u_0 \\\\ v_0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}. The iterations proceed as follows:\n\nIteration 1 (k = 1):\n\nu_1 = \\frac{5 - v_0}{3} = \\frac{5 - 0}{3} = \\frac{5}{3}, \\quad v_1 = \\frac{5 - u_0}{2} = \\frac{5 - 0}{2} = \\frac{5}{2}\n\nThus, \\mathbf{x}^{(1)} = \\begin{bmatrix} \\frac{5}{3} \\\\ \\frac{5}{2} \\end{bmatrix}.\nIteration 2 (k = 2):\n\nu_2 = \\frac{5 - v_1}{3} = \\frac{5 - \\frac{5}{2}}{3} = \\frac{5}{6}, \\quad v_2 = \\frac{5 - u_1}{2} = \\frac{5 - \\frac{5}{3}}{2} = \\frac{5}{3}\n\nThus, \\mathbf{x}^{(2)} = \\begin{bmatrix} \\frac{5}{6} \\\\ \\frac{5}{3} \\end{bmatrix}.\nIteration 3 (k = 3):\n\nu_3 = \\frac{5 - v_2}{3} = \\frac{5 - \\frac{5}{3}}{3} = \\frac{10}{9}, \\quad v_3 = \\frac{5 - u_2}{2} = \\frac{5 - \\frac{5}{6}}{2} = \\frac{25}{12}\n\nThus, \\mathbf{x}^{(3)} = \\begin{bmatrix} \\frac{10}{9} \\\\ \\frac{25}{12} \\end{bmatrix}.\n\n\n\nConvergence\nThe iterative updates show convergence to the solution \\mathbf{x} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/jacobi-method.html#convergence-conditions",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/jacobi-method.html#convergence-conditions",
    "title": "The Jacobi Method for Solving Linear Systems",
    "section": "Convergence Conditions",
    "text": "Convergence Conditions\nThe Jacobi Method converges if the matrix A is strictly diagonally dominant, meaning:\n\n|a_{ii}| &gt; \\sum_{j \\neq i} |a_{ij}|\n\nfor all rows i. Alternatively, if A is symmetric positive definite, the Jacobi Method is also guaranteed to converge."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/jacobi-method.html#limitations",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/jacobi-method.html#limitations",
    "title": "The Jacobi Method for Solving Linear Systems",
    "section": "Limitations",
    "text": "Limitations\nIn cases where A is not strictly diagonally dominant or symmetric positive definite, the Jacobi Method may fail to converge.\n\nExample of Divergence\nConsider the system:\n\nu + 2v = 5, \\quad 3u + v = 5\n\nRewriting:\n\nu = 5 - 2v, \\quad v = 5 - 3u\n\nStarting with \\mathbf{x}^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, the iterations diverge:\n\nIteration 1 (k = 1):\n\nu_1 = 5 - 2v_0 = 5, \\quad v_1 = 5 - 3u_0 = 5\n\n\\mathbf{x}^{(1)} = \\begin{bmatrix} 5 \\\\ 5 \\end{bmatrix}.\nIteration 2 (k = 2): \nu_2 = 5 - 2v_1 = -5, \\quad v_2 = 5 - 3u_1 = -10\n \\mathbf{x}^{(2)} = \\begin{bmatrix} -5 \\\\ -10 \\end{bmatrix}\n\nThe divergence occurs because the matrix is not strictly diagonally dominant."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/jacobi-method.html#summary",
    "href": "mathematics/numerical-analysis/linear-systems/ax-b-iterative-methods/jacobi-method.html#summary",
    "title": "The Jacobi Method for Solving Linear Systems",
    "section": "Summary",
    "text": "Summary\nThe Jacobi Method is a simple and effective iterative technique for solving linear systems, particularly when the system matrix is strictly diagonally dominant or symmetric positive definite. However, it is sensitive to the properties of the matrix A and may not converge for all systems."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/backward-error.html",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/backward-error.html",
    "title": "Backward Error in Linear Systems",
    "section": "",
    "text": "The backward error is a crucial concept in numerical linear algebra, measuring the smallest perturbation in the right-hand side \\mathbf{b} that would make the approximate solution \\mathbf{x_a} satisfy the linear system A\\mathbf{x} = \\mathbf{b} exactly. In other words, it quantifies how much we need to adjust \\mathbf{b} so that \\mathbf{x_a} becomes an exact solution to a slightly modified system. This concept helps us understand the sensitivity of solutions and the reliability of numerical methods.\nThe backward error is defined as:\n\n\\text{BE} = \\|\\mathbf{r}\\|_\\infty = \\|\\mathbf{b} - A\\mathbf{x_a}\\|_\\infty\n\nwhere:\n\n\\mathbf{r}: The residual vector, \\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a},\n\\|\\cdot\\|_\\infty: The infinity norm, measuring the maximum absolute entry of the residual."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/backward-error.html#overview",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/backward-error.html#overview",
    "title": "Backward Error in Linear Systems",
    "section": "",
    "text": "The backward error is a crucial concept in numerical linear algebra, measuring the smallest perturbation in the right-hand side \\mathbf{b} that would make the approximate solution \\mathbf{x_a} satisfy the linear system A\\mathbf{x} = \\mathbf{b} exactly. In other words, it quantifies how much we need to adjust \\mathbf{b} so that \\mathbf{x_a} becomes an exact solution to a slightly modified system. This concept helps us understand the sensitivity of solutions and the reliability of numerical methods.\nThe backward error is defined as:\n\n\\text{BE} = \\|\\mathbf{r}\\|_\\infty = \\|\\mathbf{b} - A\\mathbf{x_a}\\|_\\infty\n\nwhere:\n\n\\mathbf{r}: The residual vector, \\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a},\n\\|\\cdot\\|_\\infty: The infinity norm, measuring the maximum absolute entry of the residual."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/backward-error.html#what-backward-error-represents",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/backward-error.html#what-backward-error-represents",
    "title": "Backward Error in Linear Systems",
    "section": "What Backward Error Represents",
    "text": "What Backward Error Represents\n\nResidual Perspective:\n\nThe backward error reflects the size of the residual in the infinity norm. A smaller residual indicates that \\mathbf{x_a} nearly satisfies the system A\\mathbf{x} = \\mathbf{b}.\n\nAdjustment to \\mathbf{b}:\n\nIt provides an estimate of the minimal adjustment needed in \\mathbf{b} to make \\mathbf{x_a} an exact solution. Essentially, it tells us how much we need to perturb \\mathbf{b} so that A\\mathbf{x_a} = \\mathbf{b}' holds exactly for some \\mathbf{b}' close to \\mathbf{b}.\n\nExact Solution:\n\nIf \\mathbf{x_a} is the exact solution, then the residual \\mathbf{r} = \\mathbf{0}, and hence:\n\n\\text{BE} = 0"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/backward-error.html#why-backward-error-matters",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/backward-error.html#why-backward-error-matters",
    "title": "Backward Error in Linear Systems",
    "section": "Why Backward Error Matters",
    "text": "Why Backward Error Matters\n\nAssessing Solution Quality:\n\nThe backward error helps evaluate how good the approximate solution \\mathbf{x_a} is by measuring its exactness for a nearby system.\n\nNumerical Stability:\n\nAlgorithms with small backward errors are considered numerically stable because they produce solutions that are accurate for slightly perturbed inputs.\n\nError Analysis:\n\nUnderstanding the backward error allows us to relate it to the forward error (the difference between \\mathbf{x_a} and the true solution \\mathbf{x}) and to analyze the overall accuracy of numerical methods."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/backward-error.html#example",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/backward-error.html#example",
    "title": "Backward Error in Linear Systems",
    "section": "Example",
    "text": "Example\nConsider the system:\n\nA = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}, \\quad\n\\mathbf{x_a} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n\n\nStep 1: Compute A\\mathbf{x_a}\nMultiply A by \\mathbf{x_a}:\n\nA\\mathbf{x_a} = \\begin{bmatrix} 1 \\times 1 + 1 \\times 1 \\\\ 3 \\times 1 + (-4) \\times 1 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}\n\n\n\nStep 2: Compute the Residual \\mathbf{r}\nSubtract A\\mathbf{x_a} from \\mathbf{b}:\n\n\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a} = \\begin{bmatrix} 3 - 2 \\\\ 2 - (-1) \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n\n\n\nStep 3: Compute the Backward Error\nThe backward error is the infinity norm of the residual:\n\n\\text{BE} = \\|\\mathbf{r}\\|_\\infty = \\max(|1|, |3|) = 3\n\nThis means the largest adjustment needed in \\mathbf{b} is 3, making \\mathbf{x_a} an exact solution for the perturbed system A\\mathbf{x_a} = \\mathbf{b}', where \\mathbf{b}' = A\\mathbf{x_a}.\n\n\nStep 4: Interpretation\n\nResidual Components:\n\nThe residual vector \\mathbf{r} has components:\n\n\\mathbf{r} = \\begin{bmatrix} r_1 \\\\ r_2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix}\n\nThe infinity norm is the maximum absolute value among these components:\n\n\\|\\mathbf{r}\\|_\\infty = \\max(|r_1|, |r_2|) = \\max(1, 3) = 3\n\nThis highlights that the backward error is dominated by the change in the y-component (r_2 = 3)."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/backward-error.html#visualization-of-backward-error",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/backward-error.html#visualization-of-backward-error",
    "title": "Backward Error in Linear Systems",
    "section": "Visualization of Backward Error",
    "text": "Visualization of Backward Error\nThe graph below illustrates the backward error:\n\nBlue Vector (\\mathbf{b}): The target vector in the system.\nGreen Vector (A\\mathbf{x_a}): The vector computed by substituting the approximate solution \\mathbf{x_a}.\nRed Vector (\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a}): The residual vector, representing the discrepancy.\nResidual Components: Projections of \\mathbf{r} onto the x and y axes, showing r_1 and r_2.\n\n\n\nShow Code\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nA = np.array([[1, 1], [3, -4]])\nb = np.array([3, 2])\nx_a = np.array([1, 1])\n\nAx_a = A @ x_a\nr = b - Ax_a\n\nplt.figure(figsize=(10, 8))\norigin = np.zeros(2)\n\nplt.quiver(*origin, *b, color='blue', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{b}$')\nplt.quiver(*origin, *Ax_a, color='green', angles='xy', scale_units='xy', scale=1, label=r'$A\\mathbf{x}_a$')\nplt.quiver(*Ax_a, *r, color='red', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{r} = \\mathbf{b} - A\\mathbf{x}_a$')\n\n\nplt.annotate(r'$A\\mathbf{x}_a$', (Ax_a[0], Ax_a[1]), textcoords=\"offset points\", xytext=(-60,10), ha='center', color='green', fontsize=12)\nplt.annotate(r'$\\mathbf{b}$', (b[0], b[1]), textcoords=\"offset points\", xytext=(-20,15), ha='center', color='blue', fontsize=12)\nplt.annotate(r'$\\mathbf{r}$', (Ax_a[0] + r[0]/2, Ax_a[1] + r[1]/2), textcoords=\"offset points\", xytext=(10,0), ha='center', color='red', fontsize=12)\n\nplt.plot([Ax_a[0], b[0]], [Ax_a[1], b[1]], 'r--', linewidth=1)\n\nplt.quiver(*Ax_a, r[0], 0, color='orange', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{r_1}$')\nplt.quiver(Ax_a[0] + r[0], Ax_a[1], 0, r[1], color='purple', angles='xy', scale_units='xy', scale=1, label=r'$\\mathbf{r_2}$')\n\nplt.annotate(r'$\\mathbf{r_1}$', (Ax_a[0] + r[0]/2, Ax_a[1] - 0.2), textcoords=\"offset points\", xytext=(0,-10), ha='center', color='orange', fontsize=12)\nplt.annotate(r'$\\mathbf{r_2}$', (Ax_a[0] + r[0] + 0.1, Ax_a[1] + r[1]/2), textcoords=\"offset points\", xytext=(10,0), ha='center', color='purple', fontsize=12)\n\nplt.text(Ax_a[0] + r[0] + 0.1, Ax_a[1] + r[1] -0.3, r'$\\max(|\\mathbf{r_1}|, |\\mathbf{r_2}|) = |\\mathbf{r_2}| = 3$', color='purple', fontsize=10)\n\nplt.xlim(-1, 5)\nplt.ylim(-2, 5)\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.legend(loc='upper left', fontsize=12)\nplt.title('Visualization of Backward Error with Residual Components', fontsize=16)\nplt.xlabel('x-axis', fontsize=14)\nplt.ylabel('y-axis', fontsize=14)\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/error-magnification-factor.html",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/error-magnification-factor.html",
    "title": "Understanding the Error Magnification Factor (EMF)",
    "section": "",
    "text": "When tackling mathematical problems, especially those involving systems of equations, understanding how errors influence the solutions is paramount. The Error Magnification Factor (EMF) is a crucial tool that allows us to analyze the relationship between different types of errors in numerical computations, helping us assess the reliability and stability of our solutions."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/error-magnification-factor.html#overview",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/error-magnification-factor.html#overview",
    "title": "Understanding the Error Magnification Factor (EMF)",
    "section": "",
    "text": "When tackling mathematical problems, especially those involving systems of equations, understanding how errors influence the solutions is paramount. The Error Magnification Factor (EMF) is a crucial tool that allows us to analyze the relationship between different types of errors in numerical computations, helping us assess the reliability and stability of our solutions."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/error-magnification-factor.html#what-is-emf",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/error-magnification-factor.html#what-is-emf",
    "title": "Understanding the Error Magnification Factor (EMF)",
    "section": "What is EMF?",
    "text": "What is EMF?\nThe Error Magnification Factor (EMF) quantifies how an initial error, known as the backward error, is amplified when computing a solution, leading to the forward error. Think of EMF as a magnifying glass that reveals how minor inaccuracies can escalate into significant discrepancies in the final answer.\nMathematically, EMF is defined as:\n\n\\text{EMF} = \\frac{\\text{Relative Forward Error (RFE)}}{\\text{Relative Backward Error (RBE)}}\n\n\nBreaking Down the Components\n\nRelative Forward Error (RFE): Measures the deviation of the computed solution from the true solution relative to the true solution’s magnitude.\n\n\\text{RFE} = \\frac{\\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty}{\\|\\mathbf{x}\\|_\\infty}\n\nRelative Backward Error (RBE): Quantifies the adjustment needed in the original problem data to make the computed solution exact, relative to the input data’s magnitude.\n\n\\text{RBE} = \\frac{\\|\\mathbf{b} - A\\mathbf{x_a}\\|_\\infty}{\\|\\mathbf{b}\\|_\\infty}\n\n\nSubstituting these into the EMF formula:\n\n\\text{EMF} = \\frac{\\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty / \\|\\mathbf{x}\\|_\\infty}{\\|\\mathbf{b} - A\\mathbf{x_a}\\|_\\infty / \\|\\mathbf{b}\\|_\\infty} = \\frac{\\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty}{\\|\\mathbf{x}\\|_\\infty} \\cdot \\frac{\\|\\mathbf{b}\\|_\\infty}{\\|\\mathbf{b} - A\\mathbf{x_a}\\|_\\infty}\n\nWhere:\n\n\\mathbf{x}: True Solution\n\\mathbf{x_a}: Approximate (Computed) Solution\n\\mathbf{b}: Original Input Data\nA: Coefficient Matrix in the system of equations"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/error-magnification-factor.html#what-emf-represents",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/error-magnification-factor.html#what-emf-represents",
    "title": "Understanding the Error Magnification Factor (EMF)",
    "section": "What EMF Represents",
    "text": "What EMF Represents\nUnderstanding EMF involves dissecting its components and recognizing what they reveal about the system under analysis.\n\n1. Forward vs. Backward Error\n\nBackward Error (RBE): Think of it as a small mistake in the initial setup of a problem, such as an incorrect measurement or input. It quantifies this initial discrepancy.\nForward Error (RFE): Represents how this initial mistake propagates and affects the final solution, showing the deviation from the intended outcome.\n\nEMF bridges these two by indicating how much an initial error (RBE) influences the final result (RFE). A higher EMF implies that even minor initial errors can lead to significant deviations in the solution.\n\n\n2. Sensitivity of Solutions\n\nHigh EMF: Indicates that the system is sensitive or unstable. Small errors in the input data can cause large errors in the solution.\nLow EMF: Suggests that the system is stable, with errors in input data having minimal impact on the solution.\n\n\n\n3. Well-Conditioned vs. Ill-Conditioned Systems\n\nWell-Conditioned Systems: EMF values close to 1. Errors do not get significantly magnified, making the solutions reliable.\nIll-Conditioned Systems: High EMF values. Small errors can lead to large discrepancies in solutions, rendering them unreliable."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/error-magnification-factor.html#why-emf-matters",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/error-magnification-factor.html#why-emf-matters",
    "title": "Understanding the Error Magnification Factor (EMF)",
    "section": "Why EMF Matters",
    "text": "Why EMF Matters\nUnderstanding EMF is essential for several reasons:\n\nStability Analysis: EMF helps determine whether a numerical algorithm will produce reliable results or amplify errors.\nCondition Number Connection: EMF is related to the condition number of matrix A, another measure of sensitivity in systems.\nError Propagation: By analyzing EMF, we can predict how errors in our input data will affect the final solution, enabling us to take corrective measures."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/error-magnification-factor.html#a-practical-example",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/error-magnification-factor.html#a-practical-example",
    "title": "Understanding the Error Magnification Factor (EMF)",
    "section": "A Practical Example",
    "text": "A Practical Example\nLet’s delve into a concrete example to illustrate how EMF operates in practice.\n\nThe Problem Setup\nConsider the system of equations:\n\nA\\mathbf{x} = \\mathbf{b}\n\nWhere:\n\n\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} is the true solution.\n\\mathbf{x_a} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} is the approximate solution.\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} is the input data.\nA = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix} is the coefficient matrix.\n\n\n\nStep 1: Compute the Relative Forward Error (RFE)\nThe RFE measures the deviation of the computed solution from the true solution.\n\n\\text{RFE} = \\frac{\\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty}{\\|\\mathbf{x}\\|_\\infty}\n\nCalculations:\n\nDifference Vector:\n\n\\mathbf{e} = \\mathbf{x} - \\mathbf{x_a} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\n\nInfinity Norm of \\mathbf{e}:\n\n\\|\\mathbf{e}\\|_\\infty = \\max(|1|, |2|) = 2\n\nInfinity Norm of \\mathbf{x}:\n\n\\|\\mathbf{x}\\|_\\infty = \\max(|2|, |1|) = 2\n\nRFE:\n\n\\text{RFE} = \\frac{2}{2} = 1\n\n\n\n\nStep 2: Compute the Relative Backward Error (RBE)\nThe RBE measures the adjustment needed in the original input data to make the computed solution exact.\n\n\\text{RBE} = \\frac{\\|\\mathbf{b} - A\\mathbf{x_a}\\|_\\infty}{\\|\\mathbf{b}\\|_\\infty}\n\nCalculations:\n\nCompute A\\mathbf{x_a}:\n\nA\\mathbf{x_a} = \\begin{bmatrix} 1 \\cdot 1 + 1 \\cdot (-1) \\\\ 3 \\cdot 1 + (-4) \\cdot (-1) \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 7 \\end{bmatrix}\n\nResidual Vector:\n\n\\mathbf{r} = \\mathbf{b} - A\\mathbf{x_a} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 0 \\\\ 7 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ -5 \\end{bmatrix}\n\nInfinity Norm of \\mathbf{r}:\n\n\\|\\mathbf{r}\\|_\\infty = \\max(|3|, |5|) = 5\n\nInfinity Norm of \\mathbf{b}:\n\n\\|\\mathbf{b}\\|_\\infty = \\max(|3|, |2|) = 3\n\nRBE:\n\n\\text{RBE} = \\frac{5}{3} \\approx 1.6667\n\n\n\n\nStep 3: Compute the EMF\nUsing the RFE and RBE:\n\n\\text{EMF} = \\frac{\\text{RFE}}{\\text{RBE}} = \\frac{1}{1.6667} \\approx 0.6\n\n\n\nStep 4: Interpretation\nAn EMF of 0.6 implies that the backward error is magnified by a factor of 0.6 in the forward error. This indicates that the system is moderately sensitive but not highly unstable. While there is some error amplification, it isn’t excessively large, suggesting a reasonable level of stability in the solution."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/error-magnification-factor.html#visualization-of-errors",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/error-magnification-factor.html#visualization-of-errors",
    "title": "Understanding the Error Magnification Factor (EMF)",
    "section": "Visualization of Errors",
    "text": "Visualization of Errors\nVisual representations can significantly enhance our understanding of EMF. Below is a refined graph that visually illustrates the backward and forward errors in our example, with synchronized domains and ranges for better comparison.\n\nGraph of EMF Components\n\n\nShow Code\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import FancyArrowPatch\nfrom matplotlib.patches import Patch\nimport matplotlib.patheffects as pe\n\ndef create_fancy_arrow(start, end, color, width=2, alpha=1.0, zorder=5):\n    \"\"\"Create a fancy arrow without shadow effect for simplicity\"\"\"\n    arrow = FancyArrowPatch(\n        start, end,\n        arrowstyle='-&gt;',\n        color=color,\n        linewidth=width,\n        alpha=alpha,\n        zorder=zorder\n    )\n    return arrow\n\nx_true = np.array([2, 1])\nx_a = np.array([1, -1])\nb = np.array([3, 2])\nA = np.array([[1, 1], [3, -4]])\n\nAx_a = A @ x_a\nresidual = b - Ax_a\nforward_error = x_true - x_a\n\nall_vectors = [\n    [0, 0], b, Ax_a, x_true, x_a\n]\nmin_vals = np.min(all_vectors, axis=0) - 1\nmax_vals = np.max(all_vectors, axis=0) + 1\nx_min, x_max = min_vals[0], max_vals[0]\ny_min, y_max = min_vals[1], max_vals[1]\n\nplt.style.use('seaborn-v0_8-whitegrid')\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n\ncolors = {\n    'b': '#1f77b4',        # blue\n    'Ax_a': '#2ca02c',     # green\n    'residual': '#d62728', # red\n    'x_true': '#000000',   # black\n    'x_a': '#7f7f7f',      # gray\n    'forward': '#ff7f0e'   # orange\n}\n\ninput_vectors = [\n    ((0, 0), b, colors['b'], r'$\\mathbf{b}$'),\n    ((0, 0), Ax_a, colors['Ax_a'], r'$A\\mathbf{x}_a$'),\n    (Ax_a, b, colors['residual'], r'$\\mathbf{r}$')\n]\n\nfor start, end, color, name in input_vectors:\n    ax1.quiver(*start, *(np.array(end) - np.array(start)), color=color, angles='xy', scale_units='xy', scale=1, width=0.01, zorder=5)\n\n    vector = np.array(end) - np.array(start)\n    vector_norm = np.linalg.norm(vector)\n    if vector_norm &gt; 0:\n        direction = vector / vector_norm\n    else:\n        direction = np.array([0, 0])\n\n    perp_direction = np.array([-direction[1], direction[0]])\n    offset = perp_direction * 0.5\n\n    if name == r'$A\\mathbf{x}_a$':\n        offset += np.array([-0.3, 0])\n\n    mid_point = np.array(start) + vector / 2\n    ax1.text(mid_point[0] + offset[0], mid_point[1] + offset[1],\n             name, color=color, fontsize=10, fontweight='bold', zorder=6)\n\nsolution_vectors = [\n    ((0, 0), x_true, colors['x_true'], r'$\\mathbf{x}$'),\n    ((0, 0), x_a, colors['x_a'], r'$\\mathbf{x}_a$'),\n    (x_a, x_true, colors['forward'], r'$\\mathbf{e}$')\n]\n\nfor start, end, color, name in solution_vectors:\n    ax2.quiver(*start, *(np.array(end) - np.array(start)), color=color, angles='xy', scale_units='xy', scale=1, width=0.01, zorder=5)\n\n    vector = np.array(end) - np.array(start)\n    mid_point = np.array(start) + vector / 2\n    offset = np.array([0.3, 0.3])\n\n    if name == r'$\\mathbf{x}_a$':\n        offset += np.array([-0.1, -0.3])\n\n    ax2.text(mid_point[0] + offset[0], mid_point[1] + offset[1],\n             name, color=color, fontsize=10, fontweight='bold', zorder=6)\n\nfor ax, title in zip([ax1, ax2], ['Input Space (Backward Error)', 'Solution Space (Forward Error)']):\n    ax.set_aspect('equal')\n\n    x_padding = (x_max - x_min) * 0.1\n    y_padding = (y_max - y_min) * 0.1\n    ax.set_xlim(x_min - x_padding, x_max + x_padding)\n    ax.set_ylim(y_min - y_padding, y_max + y_padding)\n\n    ax.set_title(title, fontsize=12, pad=15, fontweight='bold')\n\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n\n    ax.grid(color='lightgray', linestyle='--', linewidth=0.7, zorder=0)\n\n    ax.axhline(0, color='black', linewidth=0.8, zorder=1)\n    ax.axvline(0, color='black', linewidth=0.8, zorder=1)\n\nlegend_elements = [\n    Patch(facecolor='none', edgecolor=colors['b'], label=r'$\\mathbf{b}$ (Input)'),\n    Patch(facecolor='none', edgecolor=colors['Ax_a'], label=r'$A\\mathbf{x}_a$ (Computed)'),\n    Patch(facecolor='none', edgecolor=colors['residual'], label=r'$\\mathbf{r}$ (Residual)'),\n    Patch(facecolor='none', edgecolor=colors['x_true'], label=r'$\\mathbf{x}$ (True Solution)'),\n    Patch(facecolor='none', edgecolor=colors['x_a'], label=r'$\\mathbf{x}_a$ (Approximate)'),\n    Patch(facecolor='none', edgecolor=colors['forward'], label=r'$\\mathbf{e}$ (Forward Error)')\n]\n\nfig.legend(handles=legend_elements,\n           loc='upper center',\n           bbox_to_anchor=(0.5, 0.05),\n           ncol=3,\n           fontsize=9)\n\nfig.suptitle('Error Magnification Factor Components', fontsize=14, y=0.98, fontweight='bold')\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.87, bottom=0.15, right=0.95, wspace=-0.4)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nExplanation of the Visualization\n\nLeft Plot: Input Space (Backward Error)\n\n\\mathbf{b} (Input): Represents the original input data vector.\nA\\mathbf{x}_a (Computed): The result of applying the coefficient matrix A to the approximate solution \\mathbf{x_a}.\n\\mathbf{r} (Residual): The difference between the original input \\mathbf{b} and the computed A\\mathbf{x}_a, indicating the backward error.\n\nRight Plot: Solution Space (Forward Error)\n\n\\mathbf{x} (True Solution): The actual solution to the system.\n\\mathbf{x}_a (Approximate): The computed solution.\n\\mathbf{e} (Forward Error): The difference between the true solution and the approximate solution, representing the forward error.\n\n\nThe arrows visually demonstrate how the backward error in the input space relates to the forward error in the solution space, encapsulating the essence of the EMF."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/error-magnification-factor.html#conclusion",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/error-magnification-factor.html#conclusion",
    "title": "Understanding the Error Magnification Factor (EMF)",
    "section": "Conclusion",
    "text": "Conclusion\nThe Error Magnification Factor (EMF) is a vital concept in numerical analysis, providing insight into how errors propagate through computational processes. By quantifying the relationship between backward and forward errors, EMF helps in assessing the stability and reliability of numerical solutions. Understanding and calculating EMF enables mathematicians and engineers to design more robust algorithms and make informed decisions when interpreting computational results."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/index.html",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/index.html",
    "title": "LINEAR SYSTEM ERROR ANALYSIS",
    "section": "",
    "text": "Residual\nBackward Error\nRelative Backward Error\nForward Error\nRelative Forward Error\nError Magnification Factor (EMF)\nCondition Number\nSwamping"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-forward-error.html",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-forward-error.html",
    "title": "Relative Forward Error in Linear Systems",
    "section": "",
    "text": "The relative forward error measures the ratio of the forward error to the norm of the true solution \\mathbf{x}. This concept provides a normalized metric for quantifying the error relative to the magnitude of the true solution, making it particularly useful for comparing errors across solutions of different scales.\nThe relative forward error is defined as:\n\n\\text{RFE} = \\frac{\\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty}{\\|\\mathbf{x}\\|_\\infty}\n\nwhere:\n\n\\mathbf{x}: The true solution of the system A\\mathbf{x} = \\mathbf{b},\n\\mathbf{x_a}: The approximate solution,\n\\|\\cdot\\|_\\infty: The infinity norm, measuring the maximum absolute entry of a vector."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-forward-error.html#overview",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-forward-error.html#overview",
    "title": "Relative Forward Error in Linear Systems",
    "section": "",
    "text": "The relative forward error measures the ratio of the forward error to the norm of the true solution \\mathbf{x}. This concept provides a normalized metric for quantifying the error relative to the magnitude of the true solution, making it particularly useful for comparing errors across solutions of different scales.\nThe relative forward error is defined as:\n\n\\text{RFE} = \\frac{\\|\\mathbf{x} - \\mathbf{x_a}\\|_\\infty}{\\|\\mathbf{x}\\|_\\infty}\n\nwhere:\n\n\\mathbf{x}: The true solution of the system A\\mathbf{x} = \\mathbf{b},\n\\mathbf{x_a}: The approximate solution,\n\\|\\cdot\\|_\\infty: The infinity norm, measuring the maximum absolute entry of a vector."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-forward-error.html#what-relative-forward-error-represents",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-forward-error.html#what-relative-forward-error-represents",
    "title": "Relative Forward Error in Linear Systems",
    "section": "What Relative Forward Error Represents",
    "text": "What Relative Forward Error Represents\n\nNormalized Accuracy:\n\nThe relative forward error quantifies the error in the approximate solution relative to the size of the true solution.\n\nScale Invariance:\n\nBy dividing the forward error by \\|\\mathbf{x}\\|_\\infty, the metric becomes independent of the magnitude of \\mathbf{x}, allowing comparisons across systems of varying scales.\n\nExact Solution:\n\nIf \\mathbf{x_a} is the exact solution, then:\n\n\\text{RFE} = 0"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-forward-error.html#why-relative-forward-error-matters",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-forward-error.html#why-relative-forward-error-matters",
    "title": "Relative Forward Error in Linear Systems",
    "section": "Why Relative Forward Error Matters",
    "text": "Why Relative Forward Error Matters\n\nComparing Errors Across Scales:\n\nFor large-scale systems, the absolute forward error may not be meaningful. The relative forward error provides a better sense of the proportionate error.\n\nAssessing Numerical Stability:\n\nA small relative forward error suggests that the numerical method produces a solution that is proportionately close to the true solution.\n\nImproving Computation Reliability:\n\nBy minimizing the relative forward error, algorithms can be optimized for consistent accuracy."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-forward-error.html#example",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-forward-error.html#example",
    "title": "Relative Forward Error in Linear Systems",
    "section": "Example",
    "text": "Example\nConsider the system:\n\nA = \\begin{bmatrix} 1 & 1 \\\\ 3 & -4 \\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}, \\quad\n\\mathbf{x} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}, \\quad\n\\mathbf{x_a} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\n\n\nStep 1: Verify the True Solution\nCheck that \\mathbf{x} satisfies A\\mathbf{x} = \\mathbf{b}:\n\nA\\mathbf{x} = \\begin{bmatrix} 1 \\times 2 + 1 \\times 1 \\\\ 3 \\times 2 + (-4) \\times 1 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} = \\mathbf{b}\n\n\n\nStep 2: Compute the Forward Error\nCompute the difference between \\mathbf{x} and \\mathbf{x_a}:\n\n\\mathbf{e} = \\mathbf{x} - \\mathbf{x_a} = \\begin{bmatrix} 2 - 1 \\\\ 1 - (-1) \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}\n\nThe forward error is the infinity norm of \\mathbf{e}:\n\n\\text{FE} = \\|\\mathbf{e}\\|_\\infty = \\max(|1|, |2|) = 2\n\n\n\nStep 3: Compute the Relative Forward Error\nCompute the norm of the true solution \\mathbf{x}:\n\n\\|\\mathbf{x}\\|_\\infty = \\max(|2|, |1|) = 2\n\nThe relative forward error is:\n\n\\text{RFE} = \\frac{\\|\\mathbf{e}\\|_\\infty}{\\|\\mathbf{x}\\|_\\infty} = \\frac{2}{2} = 1\n\n\n\nInterpretation\n\nThe relative forward error indicates that the forward error is equal to the size of the largest component of the true solution."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-forward-error.html#conclusion",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/relative-forward-error.html#conclusion",
    "title": "Relative Forward Error in Linear Systems",
    "section": "Conclusion",
    "text": "Conclusion\n\nRelative Forward Error Significance:\n\nThe relative forward error normalizes the forward error, making it meaningful regardless of the scale of the true solution.\n\nPractical Implications:\n\nA small relative forward error indicates that the numerical method is proportionately accurate, ensuring reliability across systems of varying magnitudes."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/swamping.html",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/swamping.html",
    "title": "Understanding Swamping in Numerical Computations",
    "section": "",
    "text": "In the realm of numerical computations, precision and accuracy are paramount. However, various phenomena can undermine these qualities, leading to erroneous results. One such phenomenon is swamping, a term that describes the masking or overwhelming of smaller errors by larger ones, making it difficult to detect or mitigate the underlying issues. Understanding swamping is essential for developing robust numerical algorithms and ensuring the reliability of computational results."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/swamping.html#overview",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/swamping.html#overview",
    "title": "Understanding Swamping in Numerical Computations",
    "section": "",
    "text": "In the realm of numerical computations, precision and accuracy are paramount. However, various phenomena can undermine these qualities, leading to erroneous results. One such phenomenon is swamping, a term that describes the masking or overwhelming of smaller errors by larger ones, making it difficult to detect or mitigate the underlying issues. Understanding swamping is essential for developing robust numerical algorithms and ensuring the reliability of computational results."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/swamping.html#what-is-swamping",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/swamping.html#what-is-swamping",
    "title": "Understanding Swamping in Numerical Computations",
    "section": "What is Swamping?",
    "text": "What is Swamping?\nSwamping refers to the scenario where smaller errors or perturbations in a numerical computation are obscured or dominated by larger errors. This can occur in various contexts, such as solving linear systems, eigenvalue computations, or iterative algorithms. When swamping happens, it becomes challenging to identify and correct minor inaccuracies, potentially leading to significant deviations in the final outcome.\nMathematically, consider a computation where multiple sources of errors are present. If one error component is significantly larger than others, it can overshadow the smaller ones, effectively “swamping” them. This makes it difficult to assess the cumulative effect of all errors accurately."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/swamping.html#causes-of-swamping",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/swamping.html#causes-of-swamping",
    "title": "Understanding Swamping in Numerical Computations",
    "section": "Causes of Swamping",
    "text": "Causes of Swamping\nSwamping can arise from several factors in numerical computations:\n\nFinite Precision Arithmetic:\n\nRound-Off Errors: In floating-point computations, numbers are represented with limited precision. Repeated arithmetic operations can accumulate round-off errors, where smaller errors become overshadowed by larger ones.\nCancellation Errors: Subtracting nearly equal numbers can result in significant loss of precision, amplifying existing errors.\n\nIll-Conditioned Systems:\n\nSystems with a high condition number are sensitive to perturbations. Small errors in input data or intermediate computations can lead to large errors in the solution, causing swamping.\n\nAlgorithmic Instabilities:\n\nCertain numerical algorithms may amplify specific error components due to their inherent design, leading to swamping of other errors.\n\nData Noise:\n\nIn data-driven computations, high levels of noise can mask underlying signals, making it difficult to detect subtle patterns or trends."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/swamping.html#implications-of-swamping",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/swamping.html#implications-of-swamping",
    "title": "Understanding Swamping in Numerical Computations",
    "section": "Implications of Swamping",
    "text": "Implications of Swamping\nSwamping has several critical implications for numerical computations:\n\nReduced Accuracy: The dominance of larger errors can significantly reduce the overall accuracy of the computation.\nError Propagation: Swamping can lead to uncontrolled error propagation, where inaccuracies at one stage of computation affect subsequent stages.\nAlgorithm Reliability: Algorithms susceptible to swamping may produce unreliable results, undermining their applicability in sensitive applications.\nDifficulty in Debugging: Identifying and isolating the sources of errors becomes challenging when swamping occurs, complicating the debugging process."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/swamping.html#detecting-and-mitigating-swamping",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/swamping.html#detecting-and-mitigating-swamping",
    "title": "Understanding Swamping in Numerical Computations",
    "section": "Detecting and Mitigating Swamping",
    "text": "Detecting and Mitigating Swamping\nEffective detection and mitigation strategies are essential to manage swamping in numerical computations:\n\n1. Error Analysis:\n\nRelative and Absolute Errors: Monitor both relative and absolute errors to identify when smaller errors are being overshadowed.\nResidual Analysis: In solving linear systems, analyze the residual \\mathbf{r} = \\mathbf{b} - A\\mathbf{x} to assess the accuracy of the solution.\n\n\n\n2. Condition Number Assessment:\n\nCompute Condition Numbers: Evaluate the condition number of matrices involved using appropriate norms (e.g., Infinity Norm) to gauge sensitivity.\nWell-Conditioned vs. Ill-Conditioned: Prefer algorithms and formulations that minimize the condition number to reduce susceptibility to swamping.\n\n\n\n3. Algorithm Selection and Improvement:\n\nStable Algorithms: Choose numerical methods known for their stability and resistance to error amplification (e.g., using QR decomposition over Gaussian elimination in certain cases).\nPivoting Techniques: Implement pivoting strategies in matrix factorizations to enhance numerical stability.\n\n\n\n4. Precision Management:\n\nHigher Precision Arithmetic: Utilize higher precision data types (e.g., double-precision instead of single-precision) to minimize round-off and cancellation errors.\nAdaptive Precision: Dynamically adjust the precision based on the sensitivity of the computation stages.\n\n\n\n5. Regularization Techniques:\n\nTikhonov Regularization: Introduce regularization terms to stabilize solutions, especially in ill-posed problems.\nNoise Filtering: Apply filtering techniques to data to reduce noise levels and prevent swamping of subtle signals."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/swamping.html#a-practical-example",
    "href": "mathematics/numerical-analysis/linear-systems/errors-analysis-linear-systems/swamping.html#a-practical-example",
    "title": "Understanding Swamping in Numerical Computations",
    "section": "A Practical Example",
    "text": "A Practical Example\nTo illustrate swamping, let’s consider the problem of solving a linear system using Gaussian elimination without pivoting, which can be susceptible to error amplification in ill-conditioned systems.\n\nThe Problem Setup\nConsider the system:\n\nA\\mathbf{x} = \\mathbf{b}\n\nWhere:\n\nA = \\begin{bmatrix}\n1 & 1 \\\\\n1 & 1.0001 \\\\\n\\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix}\n2 \\\\\n2.0001 \\\\\n\\end{bmatrix}\n\nThe true solution is:\n\n\\mathbf{x} = \\begin{bmatrix}\n1 \\\\\n1 \\\\\n\\end{bmatrix}\n\n\n\nStep 1: Compute the Condition Number \\kappa_\\infty(A)\nUsing the Infinity Norm:\n\n\\|A\\|_\\infty = \\max \\left\\{ |1| + |1|, \\ |1| + |1.0001| \\right\\} = \\max \\{2, 2.0001\\} = 2.0001\n\nCompute A^{-1}:\n\n\\det(A) = (1)(1.0001) - (1)(1) = 0.0001\n\n\nA^{-1} = \\frac{1}{0.0001} \\begin{bmatrix}\n1.0001 & -1 \\\\\n-1 & 1 \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n10001 & -10000 \\\\\n-10000 & 10000 \\\\\n\\end{bmatrix}\n\n\n\\|A^{-1}\\|_\\infty = \\max \\left\\{ |10001| + |-10000|, \\ |-10000| + |10000| \\right\\} = \\max \\{20001, 20000\\} = 20001\n\nThus,\n\n\\kappa_\\infty(A) = \\|A\\|_\\infty \\cdot \\|A^{-1}\\|_\\infty = 2.0001 \\times 20001 \\approx 40004\n\nA condition number of approximately 40004 indicates that the matrix A is ill-conditioned, making the system highly sensitive to perturbations.\n\n\nStep 2: Solve the System Using Gaussian Elimination Without Pivoting\nPerforming Gaussian elimination:\n\nFirst Pivot: The element a_{11} = 1 is used to eliminate the first entry in the second row.\nElimination Step:\n\\text{Multiplier} = \\frac{a_{21}}{a_{11}} = \\frac{1}{1} = 1\nUpdate the second row:\na_{22}' = a_{22} - \\text{Multiplier} \\times a_{12} = 1.0001 - 1 \\times 1 = 0.0001\nb_2' = b_2 - \\text{Multiplier} \\times b_1 = 2.0001 - 1 \\times 2 = 0.0001\nBack Substitution:\nx_2 = \\frac{b_2'}{a_{22}'} = \\frac{0.0001}{0.0001} = 1\nx_1 = \\frac{b_1 - a_{12}x_2}{a_{11}} = \\frac{2 - 1 \\times 1}{1} = 1\n\nThe computed solution is \\mathbf{x} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, which matches the true solution. However, this accuracy is contingent on the precision of computations and the absence of rounding errors.\n\n\nStep 3: Introduce Perturbations to Simulate Swamping\nNow, let’s introduce a more realistic perturbation that simulates how a computer might round off numbers during computations.\n\nAnalogy: Computer Rounding Leading to Swamping\nComputers represent numbers with finite precision, typically using floating-point arithmetic. Suppose during calculations, the computer rounds the number 2.0001 to 2 due to limited precision. This seemingly minor adjustment can have a drastic impact on the solution.\n\nPerturbed Equation 2:\nx + 1.0001y = 2\n(Notice the right-hand side changed from 2.0001 to 2 due to rounding)\n\nThis small change is akin to a computer rounding 2.0001 down to 2, illustrating how precision limitations can lead to significant deviations.\n\n\n\nStep 4: Solving the Perturbed System\nWith the perturbed equation, the system becomes:\n\nA\\mathbf{x} = \\mathbf{b}\n\nWhere:\n\n\\mathbf{b} = \\begin{bmatrix}\n2 \\\\\n2 \\\\\n\\end{bmatrix}\n\nReapplying Gaussian elimination:\n\nElimination Step:\n\\text{Multiplier} = \\frac{a_{21}}{a_{11}} = \\frac{1}{1} = 1\nUpdate the second row:\na_{22}' = a_{22} - \\text{Multiplier} \\times a_{12} = 1.0001 - 1 \\times 1 = 0.0001\nb_2' = b_2 - \\text{Multiplier} \\times b_1 = 2 - 1 \\times 2 = 0\nBack Substitution:\nx_2 = \\frac{b_2'}{a_{22}'} = \\frac{0}{0.0001} = 0\nx_1 = \\frac{b_1 - a_{12}x_2}{a_{11}} = \\frac{2 - 1 \\times 0}{1} = 2\n\nThe computed solution is \\mathbf{x} = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}, which deviates significantly from the true solution \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}.\n\n\nStep 5: Analyzing the Impact\n\nOriginal Solution: (x, y) = (1, 1)\nAfter Rounding Error: (x, y) = (2, 0)\n\nObservation:\nA minuscule rounding error in the constant term of the second equation (from 2.0001 to 2) caused the solution to shift dramatically from (1, 1) to (2, 0).\n\n\nStep 6: Interpretation\nThe small perturbation in \\mathbf{b} led to a substantial error in the solution, illustrating how swamping can occur in ill-conditioned systems. The large condition number amplified the minor change in \\mathbf{b}, resulting in a significant deviation in \\mathbf{x}."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/index.html",
    "href": "mathematics/numerical-analysis/linear-systems/index.html",
    "title": "LINEAR SYSTEMS",
    "section": "",
    "text": "Linear Systems: A\\mathbf{x} = \\mathbf{b}\nSpectral Radius\nLU Factorization\nIterative Methods for Solving Linear Systems\nChapter 2 Section 3 Notes\nNorms\nLinear System Error Analysis\nLeast Squares Solution for Inconsistent Systems\nGram-Schmidt Orthogonalization\nModified Gram-Schmidt Orthogonalization"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/linear-systems.html",
    "href": "mathematics/numerical-analysis/linear-systems/linear-systems.html",
    "title": "Linear Systems: A\\mathbf{x} = \\mathbf{b}",
    "section": "",
    "text": "In the realm of linear algebra, the equation A\\mathbf{x} = \\mathbf{b} serves as a foundational representation of a system of linear equations. Understanding the components of this equation is crucial for solving linear systems, analyzing their properties, and applying them to real-world problems. This note delves into the inputs of the equation A\\mathbf{x} = \\mathbf{b}, elucidating their roles, characteristics, and significance in the context of linear systems."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/linear-systems.html#overview",
    "href": "mathematics/numerical-analysis/linear-systems/linear-systems.html#overview",
    "title": "Linear Systems: A\\mathbf{x} = \\mathbf{b}",
    "section": "",
    "text": "In the realm of linear algebra, the equation A\\mathbf{x} = \\mathbf{b} serves as a foundational representation of a system of linear equations. Understanding the components of this equation is crucial for solving linear systems, analyzing their properties, and applying them to real-world problems. This note delves into the inputs of the equation A\\mathbf{x} = \\mathbf{b}, elucidating their roles, characteristics, and significance in the context of linear systems."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/linear-systems.html#components-of-amathbfx-mathbfb",
    "href": "mathematics/numerical-analysis/linear-systems/linear-systems.html#components-of-amathbfx-mathbfb",
    "title": "Linear Systems: A\\mathbf{x} = \\mathbf{b}",
    "section": "Components of A\\mathbf{x} = \\mathbf{b}",
    "text": "Components of A\\mathbf{x} = \\mathbf{b}\nTo comprehensively grasp the inputs of the equation A\\mathbf{x} = \\mathbf{b}, it’s essential to break down each component:\n\nMatrix A (Coefficient Matrix):\n\nDefinition: A rectangular array of numbers arranged in rows and columns.\nRole: Encapsulates the coefficients of the variables in the system of equations.\nDimensions: If A is an m \\times n matrix, there are m equations and n variables.\nExample: \nA = \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n\\end{bmatrix}\n\n\nVector \\mathbf{b} (Right-Hand Side Vector):\n\nDefinition: A column vector representing the constants from the right side of each equation.\nRole: Contains the target values that the linear combination of columns of A (scaled by \\mathbf{x}) should equal.\nDimensions: An m \\times 1 vector, corresponding to the number of equations.\nExample: \n\\mathbf{b} = \\begin{bmatrix}\n5 \\\\\n11 \\\\\n\\end{bmatrix}\n\n\nVector \\mathbf{x} (Solution Vector):\n\nDefinition: A column vector containing the variables of the system.\nRole: Represents the values of the variables that satisfy all equations in the system.\nDimensions: An n \\times 1 vector, where n is the number of variables.\nExample: \n\\mathbf{x} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\end{bmatrix}"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/linear-systems.html#identifying-the-inputs",
    "href": "mathematics/numerical-analysis/linear-systems/linear-systems.html#identifying-the-inputs",
    "title": "Linear Systems: A\\mathbf{x} = \\mathbf{b}",
    "section": "Identifying the Inputs",
    "text": "Identifying the Inputs\nIn the equation A\\mathbf{x} = \\mathbf{b}:\n\nInputs: A and \\mathbf{b}\nOutput (Solution): \\mathbf{x}\n\n\n1. Matrix A as an Input\n\nPurpose: Defines the relationships between the variables in the system. Each row of A corresponds to an equation, and each column corresponds to a variable.\nExample Interpretation:\nConsider the matrix:\n\nA = \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n\\end{bmatrix}\n\nThis represents the system of equations:\n\n\\begin{cases}\n1x_1 + 2x_2 = b_1 \\\\\n3x_1 + 4x_2 = b_2 \\\\\n\\end{cases}\n\n\n\n\n2. Vector \\mathbf{b} as an Input\n\nPurpose: Provides the constants against which the linear combinations of variables (as defined by A) are measured. Essentially, \\mathbf{b} represents the desired outcomes or targets for each equation.\nExample Interpretation:\nGiven:\n\n\\mathbf{b} = \\begin{bmatrix}\n5 \\\\\n11 \\\\\n\\end{bmatrix}\n\nThe system becomes:\n\n\\begin{cases}\n1x_1 + 2x_2 = 5 \\\\\n3x_1 + 4x_2 = 11 \\\\\n\\end{cases}"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/linear-systems.html#the-role-of-inputs-in-solving-amathbfx-mathbfb",
    "href": "mathematics/numerical-analysis/linear-systems/linear-systems.html#the-role-of-inputs-in-solving-amathbfx-mathbfb",
    "title": "Linear Systems: A\\mathbf{x} = \\mathbf{b}",
    "section": "The Role of Inputs in Solving A\\mathbf{x} = \\mathbf{b}",
    "text": "The Role of Inputs in Solving A\\mathbf{x} = \\mathbf{b}\nThe inputs A and \\mathbf{b} determine the nature of the system and influence the methods used to find the solution \\mathbf{x}. Here’s how:\n\n1. System Properties Influenced by A and \\mathbf{b}\n\nUniqueness of Solution:\n\nIf A is invertible (i.e., \\det(A) \\neq 0 for square matrices), the system has a unique solution.\nIf A is singular (i.e., \\det(A) = 0), the system may have infinitely many solutions or no solution.\n\nConsistency:\n\nThe system is consistent if at least one solution exists.\nIt is inconsistent if no solution satisfies all equations simultaneously.\n\n\n\n\n2. Influence on Solution Methods\n\nDirect Methods:\n\nApplicable when A is well-conditioned and invertible.\nExamples include Gaussian elimination and matrix inversion.\n\nIterative Methods:\n\nUseful for large or sparse systems where direct methods are computationally expensive.\nExamples include Jacobi, Gauss-Seidel, and Conjugate Gradient methods.\n\nLeast Squares:\n\nEmployed when the system is overdetermined (more equations than variables) and no exact solution exists.\nSeeks to minimize the residual \\|\\mathbf{b} - A\\mathbf{x}\\|."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/linear-systems.html#a-practical-example",
    "href": "mathematics/numerical-analysis/linear-systems/linear-systems.html#a-practical-example",
    "title": "Linear Systems: A\\mathbf{x} = \\mathbf{b}",
    "section": "A Practical Example",
    "text": "A Practical Example\nTo solidify the understanding of inputs in A\\mathbf{x} = \\mathbf{b}, let’s walk through a concrete example.\n\nThe Problem Setup\nConsider the following system of linear equations:\n\n\\begin{cases}\n1x_1 + 2x_2 = 5 \\\\\n3x_1 + 4x_2 = 11 \\\\\n\\end{cases}\n\nThis can be written in matrix form as:\n\nA\\mathbf{x} = \\mathbf{b}\n\nWhere:\n\nA = \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n\\end{bmatrix}, \\quad\n\\mathbf{x} = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix}\n5 \\\\\n11 \\\\\n\\end{bmatrix}\n\n\n\nStep 1: Define the Inputs\n\nCoefficient Matrix A:\n\nA = \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n\\end{bmatrix}\n\nRight-Hand Side Vector \\mathbf{b}:\n\n\\mathbf{b} = \\begin{bmatrix}\n5 \\\\\n11 \\\\\n\\end{bmatrix}\n\n\n\n\nStep 2: Analyze the Inputs\n\nInvertibility of A:\nCompute the determinant of A:\n\n\\det(A) = (1)(4) - (2)(3) = 4 - 6 = -2 \\neq 0\n\nSince \\det(A) \\neq 0, A is invertible, and the system has a unique solution.\n\n\n\nStep 3: Solve for \\mathbf{x}\nUsing the inverse of A:\n\n\\mathbf{x} = A^{-1}\\mathbf{b}\n\nFirst, compute A^{-1}:\n\nA^{-1} = \\frac{1}{\\det(A)} \\begin{bmatrix}\n4 & -2 \\\\\n-3 & 1 \\\\\n\\end{bmatrix} = \\frac{1}{-2} \\begin{bmatrix}\n4 & -2 \\\\\n-3 & 1 \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n-2 & 1 \\\\\n1.5 & -0.5 \\\\\n\\end{bmatrix}\n\nNow, multiply A^{-1} by \\mathbf{b}:\n\n\\mathbf{x} = \\begin{bmatrix}\n-2 & 1 \\\\\n1.5 & -0.5 \\\\\n\\end{bmatrix} \\begin{bmatrix}\n5 \\\\\n11 \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n(-2)(5) + (1)(11) \\\\\n(1.5)(5) + (-0.5)(11) \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n-10 + 11 \\\\\n7.5 - 5.5 \\\\\n\\end{bmatrix} = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n\\end{bmatrix}\n\nSolution:\n\n\\mathbf{x} = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n\\end{bmatrix} \\implies x_1 = 1, \\quad x_2 = 2\n\n\n\nStep 4: Interpretation of Inputs and Solution\n\nInputs A and \\mathbf{b}:\n\nDefine the system’s structure and target outcomes.\nDetermine the method used for solving (e.g., direct inversion due to invertibility).\n\nSolution \\mathbf{x}:\n\nRepresents the values that satisfy both equations simultaneously.\nIn this case, x_1 = 1 and x_2 = 2 are the unique values that make both equations true."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/linear-systems.html#geometric-interpretation",
    "href": "mathematics/numerical-analysis/linear-systems/linear-systems.html#geometric-interpretation",
    "title": "Linear Systems: A\\mathbf{x} = \\mathbf{b}",
    "section": "Geometric Interpretation",
    "text": "Geometric Interpretation\nVisualizing A\\mathbf{x} = \\mathbf{b} can enhance comprehension, especially in two dimensions.\n\nExample Visualization\nConsider the system:\n\n\\begin{cases}\n1x_1 + 2x_2 = 5 \\\\\n3x_1 + 4x_2 = 11 \\\\\n\\end{cases}\n\nGraphically, each equation represents a line in the x_1-x_2 plane.\n\nFirst Equation: 1x_1 + 2x_2 = 5\n\nSlope: -\\frac{1}{2}\nIntercepts: (5, 0) and (0, 2.5)\n\nSecond Equation: 3x_1 + 4x_2 = 11\n\nSlope: -\\frac{3}{4}\nIntercepts: (\\frac{11}{3}, 0) and (0, \\frac{11}{4})\n\n\nThe unique solution (1, 2) is the intersection point of these two lines.\n\n\nVisual Representation\n\n\nShow Code\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx1 = np.linspace(-1, 4, 400)\n\nx2_eq1 = (5 - x1) / 2\n\nx2_eq2 = (11 - 3*x1) / 4\n\n# Create the plot\nplt.figure(figsize=(8, 6))\nplt.plot(x1, x2_eq1, label=r'$1x_1 + 2x_2 = 5$', color='blue')\nplt.plot(x1, x2_eq2, label=r'$3x_1 + 4x_2 = 11$', color='green')\n\nplt.plot(1, 2, 'ro', label=r'Solution $(1, 2)$')\n\nplt.xlim(-1, 4)\nplt.ylim(-1, 4)\n\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.xlabel(r'$x_1$', fontsize=12)\nplt.ylabel(r'$x_2$', fontsize=12)\nplt.title('Geometric Interpretation of $A\\mathbf{x} = \\mathbf{b}$', fontsize=14)\nplt.legend()\nplt.axhline(0, color='black', linewidth=1)\nplt.axvline(0, color='black', linewidth=1)\nplt.show()\n\n\n&lt;&gt;:23: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:23: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\nC:\\Users\\nater\\AppData\\Local\\Temp\\ipykernel_23060\\905493902.py:23: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation of the Visualization\n\nBlue Line: Represents the equation 1x_1 + 2x_2 = 5.\nGreen Line: Represents the equation 3x_1 + 4x_2 = 11.\nRed Dot: Marks the unique solution (1, 2), where the two lines intersect.\n\nThis visualization underscores how the inputs A and \\mathbf{b} define the system’s structure and determine the solution’s existence and uniqueness."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/linear-systems.html#importance-of-understanding-inputs",
    "href": "mathematics/numerical-analysis/linear-systems/linear-systems.html#importance-of-understanding-inputs",
    "title": "Linear Systems: A\\mathbf{x} = \\mathbf{b}",
    "section": "Importance of Understanding Inputs",
    "text": "Importance of Understanding Inputs\nGrasping the role and nature of inputs in A\\mathbf{x} = \\mathbf{b} is pivotal for several reasons:\n\nSolution Strategy:\n\nKnowing whether A is invertible informs the choice of solution methods (e.g., using the inverse matrix, LU decomposition, etc.).\n\nNumerical Stability:\n\nThe properties of A (such as its condition number) influence the sensitivity of the solution to perturbations in A or \\mathbf{b}.\n\nApplicability to Real-World Problems:\n\nIn fields like engineering, economics, and computer science, accurately modeling problems as linear systems depends on correctly identifying A and \\mathbf{b}.\n\nPerformance Optimization:\n\nUnderstanding the size and sparsity of A can guide the selection of efficient algorithms for large-scale systems."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/linear-systems.html#conclusion",
    "href": "mathematics/numerical-analysis/linear-systems/linear-systems.html#conclusion",
    "title": "Linear Systems: A\\mathbf{x} = \\mathbf{b}",
    "section": "Conclusion",
    "text": "Conclusion\nIn the linear system A\\mathbf{x} = \\mathbf{b}:\n\nInputs:\n\nMatrix A: Defines the coefficients and relationships between variables.\nVector \\mathbf{b}: Specifies the target outcomes or constants for each equation.\n\nOutput:\n\nVector \\mathbf{x}: The solution that satisfies all equations in the system.\n\n\nRecognizing and comprehending the roles of A and \\mathbf{b} are fundamental for effectively solving linear systems, analyzing their properties, and applying them to diverse practical scenarios. Mastery of these concepts equips mathematicians, engineers, and scientists with the tools necessary to tackle complex problems with confidence and precision."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/modified-gram-schmidt-orthogonalization.html",
    "href": "mathematics/numerical-analysis/linear-systems/modified-gram-schmidt-orthogonalization.html",
    "title": "Modified Gram-Schmidt Orthogonalization",
    "section": "",
    "text": "The Modified Gram-Schmidt Orthogonalization (MGS) is a variation of the classical Gram-Schmidt process that improves numerical stability during computations. While it produces the same mathematical results as the classical process, MGS incrementally updates the working vector, avoiding the accumulation of rounding errors. This is especially beneficial when working with floating-point arithmetic in machine computations."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/modified-gram-schmidt-orthogonalization.html#overview",
    "href": "mathematics/numerical-analysis/linear-systems/modified-gram-schmidt-orthogonalization.html#overview",
    "title": "Modified Gram-Schmidt Orthogonalization",
    "section": "",
    "text": "The Modified Gram-Schmidt Orthogonalization (MGS) is a variation of the classical Gram-Schmidt process that improves numerical stability during computations. While it produces the same mathematical results as the classical process, MGS incrementally updates the working vector, avoiding the accumulation of rounding errors. This is especially beneficial when working with floating-point arithmetic in machine computations."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/modified-gram-schmidt-orthogonalization.html#definition-and-process",
    "href": "mathematics/numerical-analysis/linear-systems/modified-gram-schmidt-orthogonalization.html#definition-and-process",
    "title": "Modified Gram-Schmidt Orthogonalization",
    "section": "Definition and Process",
    "text": "Definition and Process\nGiven a set of linearly independent vectors A_1, A_2, \\dots, A_n, the Modified Gram-Schmidt process produces an orthogonal (or orthonormal) set of vectors q_1, q_2, \\dots, q_n such that:\n\nEach vector q_i is orthogonal to the previous vectors q_1, q_2, \\dots, q_{i-1}.\nThe span of q_1, q_2, \\dots, q_n is the same as the span of A_1, A_2, \\dots, A_n.\n\nThe primary distinction of MGS is its incremental update of the working vector y, ensuring orthogonality is preserved at every step."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/modified-gram-schmidt-orthogonalization.html#steps-of-the-modified-gram-schmidt-process",
    "href": "mathematics/numerical-analysis/linear-systems/modified-gram-schmidt-orthogonalization.html#steps-of-the-modified-gram-schmidt-process",
    "title": "Modified Gram-Schmidt Orthogonalization",
    "section": "Steps of the Modified Gram-Schmidt Process",
    "text": "Steps of the Modified Gram-Schmidt Process\n\nInitialize with the First Vector: Normalize A_1 to obtain q_1:\n\nq_1 = \\frac{A_1}{\\|A_1\\|}\n\nCompute Subsequent Vectors Incrementally:\n\nStart with y = A_i (the current vector being processed).\nSubtract the projections of y onto all previously computed q_j vectors, updating y step-by-step: \ny \\gets y - (q_j^\\top y) q_j \\quad \\text{for } j = 1, 2, \\dots, i-1\n\nNormalize the updated y to obtain q_i: \nq_i = \\frac{y}{\\|y\\|}\n\n\nRepeat this process for all vectors A_i."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/modified-gram-schmidt-orthogonalization.html#differences-between-classical-and-modified-gram-schmidt",
    "href": "mathematics/numerical-analysis/linear-systems/modified-gram-schmidt-orthogonalization.html#differences-between-classical-and-modified-gram-schmidt",
    "title": "Modified Gram-Schmidt Orthogonalization",
    "section": "Differences Between Classical and Modified Gram-Schmidt",
    "text": "Differences Between Classical and Modified Gram-Schmidt\n\n\n\n\nAspect\n\n\nClassical Gram-Schmidt\n\n\nModified Gram-Schmidt\n\n\n\n\nProjection Calculation\n\n\nSubtracts all projections at once.\n\n\nSubtracts projections incrementally, one at a time.\n\n\n\n\nNumerical Stability\n\n\nProne to rounding errors, especially for small or nearly parallel vectors.\n\n\nLess prone to rounding errors due to incremental updates.\n\n\n\n\nIntermediate Vector Updates\n\n\nProjections are computed using the original vector, so errors accumulate.\n\n\nProjections are computed step-by-step using updated vectors, reducing error propagation.\n\n\n\n\nOrthogonality of Output\n\n\nOrthogonality may degrade due to numerical issues.\n\n\nBetter orthogonality preservation in finite-precision arithmetic.\n\n\n\n\nUse Case\n\n\nUseful for theoretical computations and small-scale problems.\n\n\nPreferred for numerical applications and large-scale computations."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/modified-gram-schmidt-orthogonalization.html#properties-of-modified-gram-schmidt-orthogonalization",
    "href": "mathematics/numerical-analysis/linear-systems/modified-gram-schmidt-orthogonalization.html#properties-of-modified-gram-schmidt-orthogonalization",
    "title": "Modified Gram-Schmidt Orthogonalization",
    "section": "Properties of Modified Gram-Schmidt Orthogonalization",
    "text": "Properties of Modified Gram-Schmidt Orthogonalization\n\nOrthogonality: Each vector q_i is orthogonal to all previously generated vectors q_1, \\dots, q_{i-1}.\nSpan Preservation: The set \\{q_1, q_2, \\dots, q_n\\} spans the same subspace as \\{A_1, A_2, \\dots, A_n\\}.\nNumerical Stability: Incremental updates reduce the impact of rounding errors, making it more robust in practice."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/modified-gram-schmidt-orthogonalization.html#example-problem",
    "href": "mathematics/numerical-analysis/linear-systems/modified-gram-schmidt-orthogonalization.html#example-problem",
    "title": "Modified Gram-Schmidt Orthogonalization",
    "section": "Example Problem",
    "text": "Example Problem\nProblem: Given the matrix\n\n\\mathbf{A} =\n\\begin{bmatrix}\n1 & 1 & 1 \\\\\n\\delta & 0 & 0 \\\\\n0 & \\delta & 0 \\\\\n0 & 0 & \\delta\n\\end{bmatrix}, \\quad \\delta = 10^{-10}\n\nfind the orthonormal basis Q = [q_1, q_2, q_3] using the Modified Gram-Schmidt process.\n\nSolution Steps\n\nCompute q_1: Normalize A_1:\n\nq_1 = \\frac{A_1}{\\|A_1\\|} =\n\\begin{bmatrix}\n1 \\\\ \\delta \\\\ 0 \\\\ 0\n\\end{bmatrix}\n\nCompute q_2:\n\nStart with y = A_2 = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\delta \\\\ 0 \\end{bmatrix}.\nSubtract the projection onto q_1:\n\n\\text{proj}_{q_1}(y) = (q_1^\\top y) q_1, \\quad q_1^\\top y = 1\n\n\ny \\gets y - \\text{proj}_{q_1}(y) =\n\\begin{bmatrix}\n1 \\\\ 0 \\\\ \\delta \\\\ 0\n\\end{bmatrix}\n-\n\\begin{bmatrix}\n1 \\\\ \\delta \\\\ 0 \\\\ 0\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\ -\\delta \\\\ \\delta \\\\ 0\n\\end{bmatrix}\n\nNormalize y: \nq_2 = \\frac{y}{\\|y\\|} = \\frac{1}{\\delta \\sqrt{2}}\n\\begin{bmatrix}\n0 \\\\ -\\delta \\\\ \\delta \\\\ 0\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\ -\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0\n\\end{bmatrix}\n\n\nCompute q_3:\n\nStart with y = A_3 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\delta \\end{bmatrix}.\nSubtract the projection onto q_1: \ny \\gets y - \\text{proj}_{q_1}(y), \\quad \\text{proj}_{q_1}(y) =\n\\begin{bmatrix}\n1 \\\\ \\delta \\\\ 0 \\\\ 0\n\\end{bmatrix}\n \ny = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\delta \\end{bmatrix} -\n\\begin{bmatrix} 1 \\\\ \\delta \\\\ 0 \\\\ 0 \\end{bmatrix} =\n\\begin{bmatrix} 0 \\\\ -\\delta \\\\ 0 \\\\ \\delta \\end{bmatrix}\n\nSubtract the projection onto q_2: \ny \\gets y - \\text{proj}_{q_2}(y), \\quad q_2^\\top y = \\frac{\\delta}{\\sqrt{2}}\n \ny = \\begin{bmatrix} 0 \\\\ -\\delta \\\\ 0 \\\\ \\delta \\end{bmatrix} -\n\\begin{bmatrix} 0 \\\\ -\\frac{\\delta}{2} \\\\ \\frac{\\delta}{2} \\\\ 0 \\end{bmatrix} =\n\\begin{bmatrix} 0 \\\\ -\\frac{\\delta}{2} \\\\ -\\frac{\\delta}{2} \\\\ \\delta \\end{bmatrix}\n\nNormalize y: \nq_3 = \\frac{1}{\\|y\\|} \\begin{bmatrix} 0 \\\\ -\\frac{\\delta}{2} \\\\ -\\frac{\\delta}{2} \\\\ \\delta \\end{bmatrix} =\n\\begin{bmatrix}\n0 \\\\ -\\frac{1}{\\sqrt{6}} \\\\ -\\frac{1}{\\sqrt{6}} \\\\ \\frac{2}{\\sqrt{6}}\n\\end{bmatrix}"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/modified-gram-schmidt-orthogonalization.html#conclusion",
    "href": "mathematics/numerical-analysis/linear-systems/modified-gram-schmidt-orthogonalization.html#conclusion",
    "title": "Modified Gram-Schmidt Orthogonalization",
    "section": "Conclusion",
    "text": "Conclusion\nModified Gram-Schmidt is a more stable alternative to the classical process, particularly when dealing with small or near-parallel vectors. By incrementally updating the working vector y, MGS ensures numerical stability and maintains orthogonality even in the presence of rounding errors. It is especially useful in applications like QR factorization and solving least squares problems in computational settings."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/index.html",
    "href": "mathematics/numerical-analysis/linear-systems/norms/index.html",
    "title": "NORMS",
    "section": "",
    "text": "Euclidean Vector Norm\nTaxicab Vector Norm\nInfinity Vector Norm\nInfinity Norm for Matrices"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/infinity-vector-norm.html",
    "href": "mathematics/numerical-analysis/linear-systems/norms/infinity-vector-norm.html",
    "title": "Infinity Vector Norm",
    "section": "",
    "text": "The infinity norm (also called the maximum norm or \\ell_\\infty-norm) measures the size of a vector by taking the maximum absolute value of its components. Unlike the Euclidean or Taxicab norms, which involve summing components, the infinity norm focuses on the “largest step” in any single direction."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/infinity-vector-norm.html#overview",
    "href": "mathematics/numerical-analysis/linear-systems/norms/infinity-vector-norm.html#overview",
    "title": "Infinity Vector Norm",
    "section": "",
    "text": "The infinity norm (also called the maximum norm or \\ell_\\infty-norm) measures the size of a vector by taking the maximum absolute value of its components. Unlike the Euclidean or Taxicab norms, which involve summing components, the infinity norm focuses on the “largest step” in any single direction."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/infinity-vector-norm.html#definition",
    "href": "mathematics/numerical-analysis/linear-systems/norms/infinity-vector-norm.html#definition",
    "title": "Infinity Vector Norm",
    "section": "Definition",
    "text": "Definition\nFor a vector \\mathbf{v} = [v_1, v_2, \\dots, v_n] in \\mathbb{R}^n, the infinity norm is defined as:\n\n\\|\\mathbf{v}\\|_\\infty = \\max_{1 \\leq i \\leq n} |v_i|\n\nThis norm is often used in settings where the largest component dominates or in grid-based systems where movement is limited by a single axis."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/infinity-vector-norm.html#properties",
    "href": "mathematics/numerical-analysis/linear-systems/norms/infinity-vector-norm.html#properties",
    "title": "Infinity Vector Norm",
    "section": "Properties",
    "text": "Properties\nThe infinity norm satisfies the following properties:\n\nNon-negativity: \\|\\mathbf{v}\\|_\\infty \\geq 0, and \\|\\mathbf{v}\\|_\\infty = 0 if and only if \\mathbf{v} = \\mathbf{0}.\nHomogeneity: For any scalar c, \\|c \\mathbf{v}\\|_\\infty = |c| \\|\\mathbf{v}\\|_\\infty.\nTriangle Inequality: \\|\\mathbf{u} + \\mathbf{v}\\|_\\infty \\leq \\|\\mathbf{u}\\|_\\infty + \\|\\mathbf{v}\\|_\\infty."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/infinity-vector-norm.html#examples",
    "href": "mathematics/numerical-analysis/linear-systems/norms/infinity-vector-norm.html#examples",
    "title": "Infinity Vector Norm",
    "section": "Examples",
    "text": "Examples\n\n1. 2D Example\nFor \\mathbf{v} = [3, -4]:\n\n\\|\\mathbf{v}\\|_\\infty = \\max(|3|, |-4|) = \\max(3, 4) = 4\n\n\n\n2. 3D Example\nFor \\mathbf{v} = [1, -2, 3]:\n\n\\|\\mathbf{v}\\|_\\infty = \\max(|1|, |-2|, |3|) = \\max(1, 2, 3) = 3\n\n\n\n3. General Case\nFor \\mathbf{v} = [v_1, v_2, \\dots, v_n]:\n\n\\|\\mathbf{v}\\|_\\infty = \\max(|v_1|, |v_2|, \\dots, |v_n|)"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/infinity-vector-norm.html#applications",
    "href": "mathematics/numerical-analysis/linear-systems/norms/infinity-vector-norm.html#applications",
    "title": "Infinity Vector Norm",
    "section": "Applications",
    "text": "Applications\n\n1. Error Analysis\nThe infinity norm is used to measure the largest error in numerical solutions, ensuring that no individual error component dominates the result.\n\n\n2. Optimization\nIn optimization problems, the infinity norm simplifies constraints by focusing on the largest deviation in variables.\n\n\n3. Machine Learning\nThe infinity norm is used in regularization techniques and as a metric in certain classification problems.\n\n\n4. Computational Efficiency\nSince the infinity norm involves only a maximum operation, it is computationally inexpensive compared to other norms."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/infinity-vector-norm.html#visualization",
    "href": "mathematics/numerical-analysis/linear-systems/norms/infinity-vector-norm.html#visualization",
    "title": "Infinity Vector Norm",
    "section": "Visualization",
    "text": "Visualization\nIn 2D, the set of points at a fixed infinity norm distance from the origin forms a square aligned with the coordinate axes. For example, all points satisfying \\|\\mathbf{v}\\|_\\infty = 3 in \\mathbb{R}^2 would form the square:\n\n\\max(|x|, |y|) = 3\n\nor equivalently:\n\n-3 \\leq x \\leq 3, \\quad -3 \\leq y \\leq 3"
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/infinity-vector-norm.html#example-problem",
    "href": "mathematics/numerical-analysis/linear-systems/norms/infinity-vector-norm.html#example-problem",
    "title": "Infinity Vector Norm",
    "section": "Example Problem",
    "text": "Example Problem\nProblem: Compute the infinity norm of \\mathbf{v} = [-3, 4, -5].\n\nSolution:\n\nTake the absolute values of the components: |-3| = 3, |4| = 4, |-5| = 5.\nFind the maximum: \\|\\mathbf{v}\\|_\\infty = \\max(3, 4, 5) = 5."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/norms/infinity-vector-norm.html#conclusion",
    "href": "mathematics/numerical-analysis/linear-systems/norms/infinity-vector-norm.html#conclusion",
    "title": "Infinity Vector Norm",
    "section": "Conclusion",
    "text": "Conclusion\nThe infinity norm provides a simple and efficient way to measure vector size by focusing on the largest component. It is particularly useful in applications like error analysis, optimization, and machine learning where the largest deviation or influence is of primary interest."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/practice-problems/jacobi-method-convergence/jacobi-method-convergence.html",
    "href": "mathematics/numerical-analysis/linear-systems/practice-problems/jacobi-method-convergence/jacobi-method-convergence.html",
    "title": "Jacobi Method - Convergence Proof",
    "section": "",
    "text": "Theorem 2.10 (p. 107)\nIf the n \\times n matrix A is strictly diagonally dominant, then:\n\nA is a nonsingular matrix (invertible matrix).\nFor every vector b and every starting guess, the Jacobi Method applied to A \\mathbf{x} = \\mathbf{b} converges to the (unique) solution."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/practice-problems/jacobi-method-convergence/jacobi-method-convergence.html#diagonal-dominance-convergence-theorem",
    "href": "mathematics/numerical-analysis/linear-systems/practice-problems/jacobi-method-convergence/jacobi-method-convergence.html#diagonal-dominance-convergence-theorem",
    "title": "Jacobi Method - Convergence Proof",
    "section": "",
    "text": "Theorem 2.10 (p. 107)\nIf the n \\times n matrix A is strictly diagonally dominant, then:\n\nA is a nonsingular matrix (invertible matrix).\nFor every vector b and every starting guess, the Jacobi Method applied to A \\mathbf{x} = \\mathbf{b} converges to the (unique) solution."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/practice-problems/jacobi-method-convergence/jacobi-method-convergence.html#spectral-radius-convergence-theorem",
    "href": "mathematics/numerical-analysis/linear-systems/practice-problems/jacobi-method-convergence/jacobi-method-convergence.html#spectral-radius-convergence-theorem",
    "title": "Jacobi Method - Convergence Proof",
    "section": "Spectral Radius Convergence Theorem",
    "text": "Spectral Radius Convergence Theorem\nTheorem A.7 (p. 588)\nIf the n \\times n matrix A has spectral radius \\rho(A) &lt; 1, and \\mathbf{b} is arbitrary, then, for any vector \\mathbf{x}_0, the iteration \\mathbf{x}_{k+1} = A \\mathbf{x}_k + \\mathbf{b} converges. In fact, there exists a unique \\mathbf{x}, such that \\lim_{k \\to \\infty} \\mathbf{x}_k = \\mathbf{x}, and \\mathbf{x} = A \\mathbf{x} + \\mathbf{b}."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/practice-problems/jacobi-method-convergence/jacobi-method-convergence.html#definitions",
    "href": "mathematics/numerical-analysis/linear-systems/practice-problems/jacobi-method-convergence/jacobi-method-convergence.html#definitions",
    "title": "Jacobi Method - Convergence Proof",
    "section": "Definitions",
    "text": "Definitions\n\nSpectral radius:\nThe spectral radius \\rho(A) of a square matrix A is the maximum magnitude of its eigenvalues.\nInfinity or max norm:\nFor a vector \\mathbf{x} \\in \\mathbb{R}^n, the infinity norm is \\|\\mathbf{x}\\|_\\infty = \\max_{1 \\leq i \\leq n} |x_i|."
  },
  {
    "objectID": "mathematics/numerical-analysis/linear-systems/practice-problems/jacobi-method-convergence/jacobi-method-convergence.html#proof",
    "href": "mathematics/numerical-analysis/linear-systems/practice-problems/jacobi-method-convergence/jacobi-method-convergence.html#proof",
    "title": "Jacobi Method - Convergence Proof",
    "section": "Proof",
    "text": "Proof\nRecall that the Jacobi Method for solving A \\mathbf{x} = \\mathbf{b} is\n\n\\mathbf{x}_{k+1} = -D^{-1}(L + U) \\mathbf{x}_k + D^{-1} \\mathbf{b},\n\nwhere\n\nA = L + D + U,\n\nL is the lower triangular part of A, D is the diagonal part of A, and U is the upper triangular part of A.\nWe will apply Theorem A.7 by showing that the spectral radius of -D^{-1}(L + U) is less than 1:\n\n\\rho(D^{-1}(L + U)) &lt; 1\n\nFor notational convenience, let R = L + U denote the non-diagonal part of the matrix A. Then we must show that \\rho(D^{-1}R) &lt; 1.\n\n\n\n\n\n\n1. Scaled Vector \\mathbf{v}:\n\n\n\nGiven any vector \\mathbf{x}, we can create a scaled version of \\mathbf{x}, say \\mathbf{v}, as \\mathbf{v} = \\frac{\\mathbf{x}}{c}. What value of c will guarantee that \\|\\mathbf{v}\\|_\\infty = 1?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nTo ensure \\|\\mathbf{v}\\|_\\infty = 1, define \\mathbf{v} = \\frac{\\mathbf{x}}{c}, where c is a scalar.\nThe infinity norm of \\mathbf{v} is:\n\n\\|\\mathbf{v}\\|_\\infty = \\max_{1 \\leq i \\leq n} \\left| \\frac{x_i}{c} \\right|\n\nSet \\|\\mathbf{v}\\|_\\infty = 1, so:\n\n\\frac{\\max_{1 \\leq i \\leq n} |x_i|}{c} = 1\n\nSolve for c:\n\nc = \\|\\mathbf{x}\\|_\\infty = \\max_{1 \\leq i \\leq n} |x_i|\n\nThus, scaling \\mathbf{x} by c = \\|\\mathbf{x}\\|_\\infty guarantees \\|\\mathbf{v}\\|_\\infty = 1.\n\n\n\n\n\n\n\n\n\n2. Eigenvalue Analysis:\n\n\n\nLet \\lambda represent an arbitrary eigenvalue of D^{-1}R with corresponding eigenvector \\mathbf{v}. Then D^{-1}R \\mathbf{v} = \\lambda \\mathbf{v}, or R \\mathbf{v} = \\lambda D \\mathbf{v}.\nWhy?\nWe’ll look at each side of this equation in turn. Suppose we scale the eigenvector \\mathbf{v} such that \\|\\mathbf{v}\\|_\\infty = 1. Then |v_i| \\leq 1 for every index i, 1 \\leq i \\leq n, and |v_m| = 1 for at least one index m, 1 \\leq m \\leq n.\nUsing this index m, explain why the absolute value of the m-th row of R \\mathbf{v} is:\n\n|r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n|\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nTo analyze the absolute value of the m-th row of R \\mathbf{v}, start with the eigenvalue equation:\n\nD^{-1} R \\mathbf{v} = \\lambda \\mathbf{v}\n\nMultiply through by D to rewrite it as:\n\nR \\mathbf{v} = \\lambda D \\mathbf{v}\n\nHere:\n\nR = L + U, where L is the strictly lower triangular part of A and U is the strictly upper triangular part of A.\nD is the diagonal part of A.\n\\mathbf{v} is an eigenvector scaled such that \\|\\mathbf{v}\\|_\\infty = 1, meaning |v_i| \\leq 1 for all i, and |v_m| = 1 for at least one m.\n\nThe m-th row of R is:\n\n\\begin{bmatrix}\nr_{m,1} & r_{m,2} & \\cdots & r_{m,m-1} & 0 & r_{m,m+1} & \\cdots & r_{m,n}\n\\end{bmatrix}\n\nMultiplying this row by the vector \\mathbf{v}, the m-th entry of R \\mathbf{v} is:\n\n\\left( R \\mathbf{v} \\right)_m = \\sum_{i=1}^n r_{m,i} v_i\n\nSince r_{m,m} = 0 (as R = L + U excludes the diagonal), this simplifies to:\n\n\\left( R \\mathbf{v} \\right)_m = \\sum_{i=1, i \\neq m}^n r_{m,i} v_i\n\n\n\n\n\n\n\n\n\n\n3. Scaling with D:\n\n\n\nNow, explain why the absolute value of the m-th row of \\lambda D \\mathbf{v} is |\\lambda| |d_{m,m}|.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nTo explain why the absolute value of the m-th row of \\lambda D \\mathbf{v} is |\\lambda| |d_{m,m}|, begin by recalling the structure of D, the diagonal matrix of A:\n\nD =\n\\begin{bmatrix}\nd_{1,1} & 0 & \\cdots & 0 \\\\\n0 & d_{2,2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & d_{n,n}\n\\end{bmatrix}\n\nWhen D is multiplied by the eigenvector \\mathbf{v}, the result is:\n\nD \\mathbf{v} =\n\\begin{bmatrix}\nd_{1,1} v_1 \\\\\nd_{2,2} v_2 \\\\\n\\vdots \\\\\nd_{n,n} v_n\n\\end{bmatrix}\n\nNow multiply by \\lambda, giving:\n\n\\lambda D \\mathbf{v} =\n\\begin{bmatrix}\n\\lambda d_{1,1} v_1 \\\\\n\\lambda d_{2,2} v_2 \\\\\n\\vdots \\\\\n\\lambda d_{n,n} v_n\n\\end{bmatrix}\n\nThe m-th row of this result is:\n\n\\left( \\lambda D \\mathbf{v} \\right)_m = \\lambda d_{m,m} v_m\n\nTaking the absolute value:\n\n\\left| \\left( \\lambda D \\mathbf{v} \\right)_m \\right| = |\\lambda| |d_{m,m}| |v_m|\n\nSince \\|\\mathbf{v}\\|_\\infty = 1, we know:\n\n|v_i| \\leq 1 \\quad \\text{for all } i, \\quad \\text{and} \\quad |v_m| = 1\n\nSubstitute |v_m| = 1:\n\n\\left| \\left( \\lambda D \\mathbf{v} \\right)_m \\right| = |\\lambda| |d_{m,m}|\n\nThus, the absolute value of the m-th row of \\lambda D \\mathbf{v} is determined by |\\lambda|, the eigenvalue, and |d_{m,m}|, the diagonal entry of D at row m.\n\n\n\n\n\n\n\n\n\nEQUATION 1\n\n\n\nCombining steps (2) and (3), we can write:\n\n|\\lambda||d_{m,m}| = |r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n|\n\n\n\n\n\n\n\n\n\n4. Explain why:\n\n\n\n\n|r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n| \\leq \\sum_{j \\neq m} |r_{m,j}|\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nTo explain why\n\n\\big| r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n \\big| \\leq \\sum_{j \\neq m} |r_{m,j}|,\n\nwe begin by recalling the triangle inequality for absolute values. For any sum of terms a_1, a_2, \\dots, a_k, the triangle inequality ensures:\n\n|a_1 + a_2 + \\cdots + a_k| \\leq |a_1| + |a_2| + \\cdots + |a_k|\n\nIn our case, the sum of interest is:\n\nr_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n\n\nApplying the triangle inequality to this sum gives:\n\\quad \\big| r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n \\big|\n\n\\leq |r_{m,1}v_1| + |r_{m,2}v_2| + \\cdots + |r_{m,m-1}v_{m-1}| + |r_{m,m+1}v_{m+1}| + \\cdots + |r_{m,n}v_n|\n\nEach term in the sum has the form |r_{m,j}v_j|. Using the property of absolute values |ab| = |a||b|, we can rewrite each term as:\n\n|r_{m,j}v_j| = |r_{m,j}| \\cdot |v_j|\n\nSince it is assumed that |v_j| \\leq 1 for all j, it follows that:\n\n|r_{m,j}v_j| \\leq |r_{m,j}|\n\nSubstituting this bound for each term into the inequality gives:\n\\quad \\big| r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n \\big|\n\n\\leq |r_{m,1}v_1| + |r_{m,2}v_2| + \\cdots + |r_{m,m-1}v_{m-1}| + |r_{m,m+1}v_{m+1}| + \\cdots + |r_{m,n}v_n|\n\nThe indices j \\neq m correspond to all off-diagonal entries in the m-th row of the matrix. Thus, we can express the sum of the absolute values of the coefficients as:\n\n|r_{m,1}| + |r_{m,2}| + \\cdots + |r_{m,m-1}| + |r_{m,m+1}| + \\cdots + |r_{m,n}| = \\sum_{j \\neq m} |r_{m,j}|\n\nSubstituting this back, we find:\n\n\\big| r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n \\big| \\leq \\sum_{j \\neq m} |r_{m,j}|\n\nThe inequality holds because:\n\nThe triangle inequality ensures that the absolute value of a sum is at most the sum of the absolute values of its terms.\nThe assumption |v_j| \\leq 1 allows us to bound |r_{m,j}v_j| by |r_{m,j}|.\n\nThus, the magnitude of the weighted sum of v_j values (for j \\neq m) is always less than or equal to the sum of the absolute values of the off-diagonal entries in the m-th row of the matrix.\n\n\n\n\n\n\n\n\n\n5. Explain why:\n\n\n\n\n\\sum_{j \\neq m} |r_{m,j}| &lt; |d_{m,m}|\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBy assumption, the matrix A is strictly diagonally dominant. This means that, for each row of A, the absolute value of the diagonal entry |d_{m,m}| is strictly greater than the sum of the absolute values of all the off-diagonal entries in that row:\n\n|d_{m,m}| &gt; \\sum_{j \\neq m} |r_{m,j}|\n\nIn other words, the diagonal entry d_{m,m} has the largest contribution in the row, ensuring that the total influence of the off-diagonal terms is strictly smaller.\n\n\n\n\n\n\n\n\n\n6. Use the results from Steps (4) and (5) with EQUATION 1 to show show that:\n\n\n\n\n|\\lambda||d_{m,m}| &lt; |d_{m,m}|\n\nWhat does this say about |\\lambda|?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nTo explain why\n\n|\\lambda||d_{m,m}| &lt; |d_{m,m}|\n\nwe combine the results from Step (4) and Step (5) with EQUATION 1.\nEQUATION 1 states:\n\n|\\lambda||d_{m,m}| = \\big| r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n \\big|\n\nFrom Step (4), we know:\n\n\\big| r_{m,1}v_1 + r_{m,2}v_2 + \\cdots + r_{m,m-1}v_{m-1} + r_{m,m+1}v_{m+1} + \\cdots + r_{m,n}v_n \\big| \\leq \\sum_{j \\neq m} |r_{m,j}|\n\nSubstituting this into EQUATION 1, we have:\n\n|\\lambda||d_{m,m}| \\leq \\sum_{j \\neq m} |r_{m,j}|\n\nFrom Step (5), we know:\n\n\\sum_{j \\neq m} |r_{m,j}| &lt; |d_{m,m}|\n\nCombining this with the inequality above gives:\n\n|\\lambda||d_{m,m}| &lt; |d_{m,m}|\n\nDividing both sides of the inequality by |d_{m,m}| (which is nonzero), we find:\n\n|\\lambda| &lt; 1\n\nThis result shows that the magnitude of the eigenvalue |\\lambda| is strictly less than 1 and this implies that the spectral radius \\rho(D^{-1}R) &lt; 1.\n\n\n\n\n\n\n\n\n\n7. Final Conclusion\n\n\n\nSince \\lambda is an arbitrary eigenvalue, then |\\lambda|_{\\text{max}} &lt; 1. In other words, the spectral radius \\rho(D^{-1}R) &lt; 1. Thus, by the Spectral Radius Convergence Theorem (Theorem A.7), the Jacobi Method (iteration with A = D^{-1}R) converges for any starting point \\mathbf{x}_0.\nLet \\mathbf{x}_* = \\lim_{k \\to \\infty} \\mathbf{x}_k, and show that \\mathbf{x}_* is the solution to A \\mathbf{x} = \\mathbf{b}, so A must be nonsingular. This completes the proof of the Diagonal Dominance Convergence Theorem (Theorem 2.10).\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFrom earlier, we showed that the spectral radius \\rho(D^{-1}R), which is the largest magnitude of the eigenvalues of D^{-1}R, satisfies:\n\n\\rho(D^{-1}R) &lt; 1\n\nThis result guarantees that the Jacobi method converges to a solution \\mathbf{x}_* of A\\mathbf{x} = \\mathbf{b} for any starting point \\mathbf{x}_0, as stated in the Diagonal Dominance Convergence Theorem (Theorem A.7).\nTherefore, we can write the limit of the iterates \\mathbf{x}_k as:\n\n\\mathbf{x}_* = \\lim_{k \\to \\infty} \\mathbf{x}_k\n\nThe Jacobi iteration formula is:\n\n\\mathbf{x}_{k+1} = D^{-1}(\\mathbf{b} - R\\mathbf{x}_k)\n\nwhere A = D - R, D is the diagonal matrix, and R is the remainder matrix (containing the off-diagonal terms).\nSubstituting the limit \\mathbf{x}_* into this equation (as \\mathbf{x}_{k+1} \\to \\mathbf{x}_* and \\mathbf{x}_k \\to \\mathbf{x}_*), we get:\n\n\\mathbf{x}_* = D^{-1}(\\mathbf{b} - R\\mathbf{x}_*)\n\nExpanding this:\n\n\\mathbf{x}_* = -D^{-1}R\\mathbf{x}_* + D^{-1}\\mathbf{b}\n\nRewriting:\n\n\n\n\n\n\n‎\n\n\n\n\n\\mathbf{x}_* = -D^{-1}(L + U)\\mathbf{x}_* + D^{-1}\\mathbf{b}\n\n\n\nwhere R = L + U, with L being the strictly lower triangular part of A and U the strictly upper triangular part.\nTo verify \\mathbf{x}_* satisfies A\\mathbf{x}_* = \\mathbf{b}, substitute A = D + L + U into the system:\n\n(D + L + U)\\mathbf{x}_* = \\mathbf{b}\n\nMultiply both sides by D^{-1}:\n\n\\mathbf{x}_* + D^{-1}(L + U)\\mathbf{x}_* = D^{-1}\\mathbf{b}\n\nRearranging terms gives:\n\n\\mathbf{x}_* = -D^{-1}(L + U)\\mathbf{x}_* + D^{-1}\\mathbf{b}\n\nThis matches the Jacobi iteration formula, verifying \\mathbf{x}_* satisfies A\\mathbf{x}_* = \\mathbf{b}.\nConclusion\n\nWe proved that \\rho(D^{-1}R) &lt; 1, so the Jacobi method converges to \\mathbf{x}_*.\nSubstituting \\mathbf{x}_* into A\\mathbf{x} = \\mathbf{b}, we verified that it satisfies the system.\nSince A\\mathbf{x}_* = \\mathbf{b} has a solution, A is nonsingular."
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/chebyshev-interpolation.html",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/chebyshev-interpolation.html",
    "title": "Chebyshev Interpolation",
    "section": "",
    "text": "Chebyshev Interpolation is a powerful technique in numerical analysis used to approximate functions with polynomials, particularly minimizing errors and avoiding issues like Runge’s phenomenon. This note provides an overview of Chebyshev Interpolation, including its definition, properties, advantages, and practical implementation."
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/chebyshev-interpolation.html#introduction",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/chebyshev-interpolation.html#introduction",
    "title": "Chebyshev Interpolation",
    "section": "Introduction",
    "text": "Introduction\nInterpolation involves approximating a function f(x) using a polynomial P_n(x) that passes through a set of points (nodes). While polynomial interpolation is straightforward, selecting appropriate nodes is crucial to ensure accuracy and stability. Chebyshev Interpolation leverages Chebyshev nodes to construct interpolating polynomials that minimize the maximum error across the interpolation interval."
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/chebyshev-interpolation.html#chebyshev-polynomials",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/chebyshev-interpolation.html#chebyshev-polynomials",
    "title": "Chebyshev Interpolation",
    "section": "Chebyshev Polynomials",
    "text": "Chebyshev Polynomials\nChebyshev polynomials are a sequence of orthogonal polynomials that arise in various approximation and interpolation problems. They are defined recursively and have properties that make them ideal for minimizing interpolation errors.\n\nDefinition\nThe Chebyshev polynomials of the first kind, T_n(x), are defined by:\n\nT_n(x) = \\cos(n \\arccos x), \\quad \\text{for } x \\in [-1, 1]\n\n\n\nProperties\n\nOrthogonality: Chebyshev polynomials are orthogonal with respect to the weight w(x) = \\frac{1}{\\sqrt{1 - x^2}} on the interval [-1, 1].\nExtremal Property: Among all polynomials of degree n with leading coefficient 2^{n-1}, T_n(x) has the smallest maximum deviation from zero on [-1, 1].\nRoots and Extremes: The roots of T_n(x) are given by:\n\nx_k = \\cos\\left( \\frac{2k - 1}{2n} \\pi \\right), \\quad k = 1, 2, \\dots, n\n\nThe extrema (maximum and minimum points) of T_n(x) occur at:\n\nx_k = \\cos\\left( \\frac{k}{n} \\pi \\right), \\quad k = 0, 1, \\dots, n"
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/chebyshev-interpolation.html#chebyshev-nodes",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/chebyshev-interpolation.html#chebyshev-nodes",
    "title": "Chebyshev Interpolation",
    "section": "Chebyshev Nodes",
    "text": "Chebyshev Nodes\nChebyshev nodes are specific points in the interval [-1, 1] used as interpolation nodes to minimize the interpolation error.\n\nDefinition\nFor n+1 Chebyshev nodes, the k-th node x_k is given by:\n\nx_k = \\cos\\left( \\frac{2k + 1}{2n + 2} \\pi \\right), \\quad k = 0, 1, 2, \\dots, n\n\nAlternatively, they can be expressed as:\n\nx_k = \\cos\\left( \\frac{(2k + 1)\\pi}{2(n + 1)} \\right), \\quad k = 0, 1, 2, \\dots, n\n\n\n\nMapping to Arbitrary Intervals\nFor an interval [a, b], Chebyshev nodes are mapped as:\n\nx_k = \\frac{a + b}{2} + \\frac{b - a}{2} \\cos\\left( \\frac{(2k + 1)\\pi}{2(n + 1)} \\right), \\quad k = 0, 1, 2, \\dots, n\n\n\n\nImportance\nUsing Chebyshev nodes instead of equally spaced nodes helps in minimizing the oscillatory behavior (Runge’s phenomenon) and ensures better convergence properties of the interpolating polynomial."
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/chebyshev-interpolation.html#interpolation-process",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/chebyshev-interpolation.html#interpolation-process",
    "title": "Chebyshev Interpolation",
    "section": "Interpolation Process",
    "text": "Interpolation Process\nThe Chebyshev Interpolation process involves the following steps:\n\nSelect Chebyshev Nodes: Determine the n+1 Chebyshev nodes x_0, x_1, \\dots, x_n in the interval [-1, 1].\nEvaluate the Function: Compute the function values f(x_0), f(x_1), \\dots, f(x_n).\nConstruct the Interpolating Polynomial: Use methods such as the Chebyshev series expansion or the barycentric interpolation formula to construct the interpolating polynomial Q_n(x).\nApproximate the Function: Use Q_n(x) to approximate f(x) within the interval.\n\n\nBarycentric Interpolation Formula\nOne efficient method to compute the interpolating polynomial is the barycentric interpolation formula:\n\nQ_n(x) = \\frac{\\sum_{k=0}^n \\frac{w_k f(x_k)}{x - x_k}}{\\sum_{k=0}^n \\frac{w_k}{x - x_k}}\n\nwhere w_k are the barycentric weights defined as:\n\nw_k = (-1)^k \\sin\\left( \\frac{(2k + 1)\\pi}{2n + 2} \\right)"
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/chebyshev-interpolation.html#advantages-of-chebyshev-interpolation",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/chebyshev-interpolation.html#advantages-of-chebyshev-interpolation",
    "title": "Chebyshev Interpolation",
    "section": "Advantages of Chebyshev Interpolation",
    "text": "Advantages of Chebyshev Interpolation\n\nMinimized Error: Chebyshev nodes minimize the maximum error (uniform convergence) of the interpolating polynomial.\nReduced Oscillations: Avoids Runge’s phenomenon, where high-degree polynomial interpolations at equally spaced nodes exhibit large oscillations near the interval endpoints.\nEfficient Computation: The barycentric interpolation formula allows for efficient and stable computation of interpolating polynomials.\nOrthogonality: Leveraging the orthogonality of Chebyshev polynomials aids in various approximation and numerical integration techniques."
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/chebyshev-interpolation.html#error-analysis",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/chebyshev-interpolation.html#error-analysis",
    "title": "Chebyshev Interpolation",
    "section": "Error Analysis",
    "text": "Error Analysis\nUnderstanding the error associated with Chebyshev Interpolation is crucial for assessing the approximation’s reliability.\n\nInterpolation Error Formula\nFor a function f(x) sufficiently smooth on [-1, 1], the error of the Chebyshev interpolating polynomial Q_n(x) of degree n is given by:\n\n|f(x) - Q_n(x)| \\leq \\frac{M}{(n+1)!} \\cdot \\frac{1}{2^{n+1}}\n\nwhere:\n\nM is an upper bound on the (n+1)-th derivative of f(x) on [-1, 1].\n\n\n\nWorst-Case Error Estimate\nFor example, if f(x) = e^x, all derivatives are f^{(k)}(x) = e^x. On [-1, 1], e^x \\leq e, so M = e.\nFor a fifth-degree polynomial (n = 5):\n\n|e^x - Q_5(x)| \\leq \\frac{e}{6!} \\cdot \\frac{1}{2^5} = \\frac{e}{720 \\times 32} \\approx 0.000118\n\n\n\nImplications\nAn error bound of approximately 1.18 \\times 10^{-4} implies that at least three decimal digits of the approximation Q_5(x) are accurate across the interval [-1, 1]."
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/chebyshev-interpolation.html#example",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/chebyshev-interpolation.html#example",
    "title": "Chebyshev Interpolation",
    "section": "Example",
    "text": "Example\nProblem: Approximate f(x) = e^x on [-1, 1] using a fifth-degree Chebyshev interpolating polynomial Q_5(x). Estimate the worst-case error and determine the number of correct decimal digits in the approximation.\nSolution:\n\nDetermine M:\n\nf^{(6)}(x) = e^x, so M = e.\n\nCompute the Error Bound:\n\n|e^x - Q_5(x)| \\leq \\frac{e}{6!} \\cdot \\frac{1}{2^5} = \\frac{2.71828}{720 \\times 32} \\approx 0.000118\n\nInterpret the Error:\n\nThe approximation Q_5(x) deviates from e^x by less than 1.18 \\times 10^{-4}.\nAt least three decimal digits of Q_5(x) are accurate.\n\n\nConclusion: The fifth-degree Chebyshev interpolating polynomial Q_5(x) approximates e^x with an error less than 0.00012, ensuring at least three correct decimal digits across [-1, 1]."
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/chebyshev-interpolation.html#applications",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/chebyshev-interpolation.html#applications",
    "title": "Chebyshev Interpolation",
    "section": "Applications",
    "text": "Applications\nChebyshev Interpolation is widely used in various fields due to its robustness and efficiency:\n\nNumerical Integration: Chebyshev polynomials are used in Gaussian quadrature methods for approximating integrals.\nApproximation Theory: Provides optimal polynomial approximations for continuous functions.\nSignal Processing: Used in filter design and spectral analysis.\nComputer Graphics: Facilitates curve and surface modeling with minimal errors.\nScientific Computing: Enhances the accuracy of simulations and numerical solutions to differential equations."
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/chebyshev-interpolation.html#conclusion",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/chebyshev-interpolation.html#conclusion",
    "title": "Chebyshev Interpolation",
    "section": "Conclusion",
    "text": "Conclusion\nChebyshev Interpolation offers a reliable method for polynomial approximation, leveraging Chebyshev nodes to minimize interpolation errors and avoid common pitfalls like Runge’s phenomenon. Its mathematical foundations, combined with practical computational techniques, make it an essential tool in numerical analysis and various applied disciplines."
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/interpolation-error-formula.html",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/interpolation-error-formula.html",
    "title": "Interpolation Error Formula",
    "section": "",
    "text": "The interpolation error formula is an expression used to estimate the error between a true function f(x) and its interpolating polynomial P(x) at a specific point. This formula helps us understand how closely the interpolating polynomial approximates the function, especially based on the distribution of interpolation points and the smoothness of f(x).\nThe interpolation error formula is given by:\n\nf(x) - P(x) = \\frac{(x - x_1)(x - x_2) \\cdots (x - x_n)}{n!} f^{(n)}(c)\n\nwhere:\n\nn is the number of interpolated points.\nP(x) is the interpolating polynomial of degree n - 1 that fits the points (x_1, y_1), \\ldots, (x_n, y_n),\nf^{(n)}(c) is the n-th derivative of f(x) evaluated at some unknown point c in the interval [x_1, x_n]."
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/interpolation-error-formula.html#overview",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/interpolation-error-formula.html#overview",
    "title": "Interpolation Error Formula",
    "section": "",
    "text": "The interpolation error formula is an expression used to estimate the error between a true function f(x) and its interpolating polynomial P(x) at a specific point. This formula helps us understand how closely the interpolating polynomial approximates the function, especially based on the distribution of interpolation points and the smoothness of f(x).\nThe interpolation error formula is given by:\n\nf(x) - P(x) = \\frac{(x - x_1)(x - x_2) \\cdots (x - x_n)}{n!} f^{(n)}(c)\n\nwhere:\n\nn is the number of interpolated points.\nP(x) is the interpolating polynomial of degree n - 1 that fits the points (x_1, y_1), \\ldots, (x_n, y_n),\nf^{(n)}(c) is the n-th derivative of f(x) evaluated at some unknown point c in the interval [x_1, x_n]."
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/interpolation-error-formula.html#what-this-formula-shows",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/interpolation-error-formula.html#what-this-formula-shows",
    "title": "Interpolation Error Formula",
    "section": "What This Formula Shows",
    "text": "What This Formula Shows\nThis formula provides the error at a specific point x between the true function f(x) and the interpolating polynomial P(x). It does not directly give the maximum error across the entire interval, but rather the error at a particular x based on the distribution of interpolation points and the properties of f(x) at that point.\n\nKey Points\n\nPoint-Specific Error: This formula gives the interpolation error at a particular point x. It shows the difference f(x) - P(x) at that point rather than over the entire interval.\nDependence on Higher Derivatives: The error depends on the n-th derivative of f(x) evaluated at some unknown point c within [x_1, x_n]. This term reflects how “curved” f(x) is over the interval. A larger |f^{(n)}(c)| generally results in a larger error, as higher derivatives capture more variation in f(x).\nApproximate Error Size: Although c is unknown, we can approximate the error by assuming f^{(n)}(c) reaches its maximum absolute value over [x_1, x_n]. This allows us to estimate an upper bound on the error at any point in the interval, though it’s still approximate.\nEffect of Distance from Nodes: The term (x - x_1)(x - x_2) \\cdots (x - x_n) grows as x moves away from the interpolation nodes. This indicates that interpolation error typically increases the farther x is from the interpolation points, which is why interpolation tends to be most accurate near the nodes."
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/interpolation-error-formula.html#finding-the-maximum-error-over-the-interval",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/interpolation-error-formula.html#finding-the-maximum-error-over-the-interval",
    "title": "Interpolation Error Formula",
    "section": "Finding the Maximum Error Over the Interval",
    "text": "Finding the Maximum Error Over the Interval\nTo find the maximum error over the entire interval [x_1, x_n], we can use the interpolation error formula to create an upper bound for the error on the interval:\n\nMaximize |f^{(n)}(x)| over the interval: Find the maximum of the n-th derivative of f(x), f^{(n)}(x), over the interval [x_1, x_n]. This value represents the largest possible influence of the function’s curvature on the error.\nMaximize |(x - x_1)(x - x_2) \\cdots (x - x_n)|: Determine the maximum value of the product |(x - x_1)(x - x_2) \\cdots (x - x_n)| over the interval [x_1, x_n]. This product is largest near the midpoint of the interval (between the nodes) and tends to be smaller near the endpoints.\nCombine the Results: Multiply these two maximum values and divide by n! to get an upper bound on the maximum error over the interval:\n\n\\max_{x \\in [x_1, x_n]} |f(x) - P(x)| \\approx \\frac{\\max_{x \\in [x_1, x_n]} |(x - x_1)(x - x_2) \\cdots (x - x_n)|}{n!} \\cdot \\max_{x \\in [x_1, x_n]} |f^{(n)}(x)|\n\n\nThis approach provides an approximate maximum error over the interval. By using these maximum values, we can ensure that the error does not exceed this bound anywhere in [x_1, x_n].\nIn summary, the interpolation error formula gives insight into the local error at a specific point and can be used to estimate the maximum error on the interval by considering the maximum values of the components in the formula."
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/newtons-divided-differences.html",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/newtons-divided-differences.html",
    "title": "Newton’s Divided Differences",
    "section": "",
    "text": "Newton’s Divided Differences is an efficient method for computing an interpolating polynomial for a given set of data points. This method builds the polynomial iteratively and offers better efficiency for incremental data points compared to Lagrange interpolation."
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/newtons-divided-differences.html#the-newton-divided-difference-formula",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/newtons-divided-differences.html#the-newton-divided-difference-formula",
    "title": "Newton’s Divided Differences",
    "section": "The Newton Divided Difference Formula",
    "text": "The Newton Divided Difference Formula\nGiven n data points (x_1, y_1), (x_2, y_2), ..., (x_n, y_n), the Newton divided difference interpolating polynomial P(x) can be expressed as:\n\nP(x) = f[x_1] + f[x_1, x_2](x - x_1) + f[x_1, x_2, x_3](x - x_1)(x - x_2) + \\cdots + f[x_1, x_2, \\ldots, x_n](x - x_1)(x - x_2)\\cdots(x - x_{n-1})\n\nWhere f[x_i, x_j, ..., x_k] are the divided differences and are recursively defined as follows:\n\nf[x_i] = y_i\n\n\nf[x_i, x_j] = \\frac{f[x_j] - f[x_i]}{x_j - x_i}\n\n\nf[x_i, x_j, x_k] = \\frac{f[x_j, x_k] - f[x_i, x_j]}{x_k - x_i}\n\nAnd so on for higher orders of divided differences.\n\nRecursive Formula for Divided Differences\nThe divided differences are computed recursively. For the first-order difference between two points, the formula is:\n\nf[x_i, x_{i+1}] = \\frac{f(x_{i+1}) - f(x_i)}{x_{i+1} - x_i}\n\nFor the second-order difference between three points:\n\nf[x_i, x_{i+1}, x_{i+2}] = \\frac{f[x_{i+1}, x_{i+2}] - f[x_i, x_{i+1}]}{x_{i+2} - x_i}\n\nThis recursive approach continues for higher orders of differences.\n\n\nStep-by-Step Construction of the Newton Polynomial\n\nStart with the first point (x_1, y_1), where f[x_1] = y_1.\nFirst-order divided difference between (x_1, y_1) and (x_2, y_2) is:\n\n\nf[x_1, x_2] = \\frac{y_2 - y_1}{x_2 - x_1}\n\n\nSecond-order divided difference between (x_1, y_1), (x_2, y_2), and (x_3, y_3):\n\n\nf[x_1, x_2, x_3] = \\frac{f[x_2, x_3] - f[x_1, x_2]}{x_3 - x_1}\n\n\nContinue this process for higher-order divided differences.\n\nHere’s the corrected example in the same format as your original:\n\n\nExample\nLet’s consider the data points (1, 1), (2, 4), (3, 9), and (4, 16). The goal is to find the interpolating polynomial using Newton’s divided differences.\n\nFirst point:\n\n\nf[1] = 1\n\n\nFirst-order divided differences:\n\n\nf[1, 2] = \\frac{4 - 1}{2 - 1} = 3\n\n\nf[2, 3] = \\frac{9 - 4}{3 - 2} = 5\n\n\nf[3, 4] = \\frac{16 - 9}{4 - 3} = 7\n\n\nSecond-order divided differences:\n\n\nf[1, 2, 3] = \\frac{f[2, 3] - f[1, 2]}{3 - 1} = \\frac{5 - 3}{2} = 1\n\n\nf[2, 3, 4] = \\frac{f[3, 4] - f[2, 3]}{4 - 2} = \\frac{7 - 5}{2} = 1\n\n\nThird-order divided difference:\n\n\nf[1, 2, 3, 4] = \\frac{f[2, 3, 4] - f[1, 2, 3]}{4 - 1} = \\frac{1 - 1}{3} = 0\n\nNow, the Newton polynomial can be written as:\n\nP(x) = 1 + 3(x - 1) + 1(x - 1)(x - 2) + 0(x - 1)(x - 2)(x - 3)\n\nSimplifying:\n\nP(x) = 1 + 3(x - 1) + (x - 1)(x - 2)\n\nExpanding the terms:\n\nP(x) = 1 + 3x - 3 + (x^2 - 3x + 2)\n\nSimplifying further:\n\nP(x) = x^2\n\n\n\nGeneral Properties of Newton’s Divided Differences\n\nEfficiency: Newton’s divided differences offer better computational efficiency when adding new points to the data set compared to Lagrange interpolation because earlier divided differences can be reused.\nUniqueness: The Newton polynomial is unique, meaning for a given set of n distinct points, there is exactly one polynomial of degree n-1 that interpolates the points.\nIterative Construction: The method allows for iterative construction, which is useful when dealing with real-time updates or adding new data points.\n\n\n\nApplications of Newton’s Divided Differences\n\nPolynomial Interpolation: Newton’s divided differences are commonly used to find an interpolating polynomial for a given set of data points.\nNumerical Differentiation: The method is used to approximate derivatives of functions when analytical differentiation is not feasible.\nCurve Fitting: It is used in applications requiring curve fitting, especially in scientific computing and data analysis.\n\n\n\nAdvantages of Newton’s Divided Differences\n\nEfficient for Incremental Data: If you need to add a new data point, you don’t have to recompute the entire polynomial. Only the new divided differences need to be computed.\nEasy to Implement: The recursive approach to finding divided differences makes this method easy to implement in code.\n\n\n\nLimitations of Newton’s Divided Differences\n\nNumerical Stability: Like other polynomial interpolation methods, Newton’s divided differences can suffer from numerical instability, especially with large datasets or unevenly spaced data.\nOscillations: High-degree interpolating polynomials may oscillate significantly between data points, especially if the data is not well-distributed (similar to Runge’s phenomenon).\n\n\n\nConclusion\nNewton’s Divided Differences is a powerful method for constructing interpolating polynomials, especially when efficiency and incremental data updates are needed. Its recursive nature allows for fast updates when new data points are added, making it useful in applications such as numerical analysis, interpolation, and curve fitting."
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/practice-problems/root-analysis.html",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/practice-problems/root-analysis.html",
    "title": "Interpolation Polynomial - Root Analysis",
    "section": "",
    "text": "Let p_{n-1}(x) be the degree n-1 polynomial that interpolates the points (x_i, f(x_i)), where i = 1, \\dots, n. Now, suppose we add a new point (x, f(x)) and assume that a is chosen such that the polynomial\n\np_n(t) = p_{n-1}(t) + a(t - x_1)(t - x_2)\\dots(t - x_n)\n\ninterpolates the point (x, f(x)) in addition to the original n points (x_i, f(x_i)), i = 1, \\dots, n.\nThen h(t) = f(t) - p_n(t) must have at least n+1 roots. What are these roots?"
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/practice-problems/root-analysis.html#problem-statement",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/practice-problems/root-analysis.html#problem-statement",
    "title": "Interpolation Polynomial - Root Analysis",
    "section": "",
    "text": "Let p_{n-1}(x) be the degree n-1 polynomial that interpolates the points (x_i, f(x_i)), where i = 1, \\dots, n. Now, suppose we add a new point (x, f(x)) and assume that a is chosen such that the polynomial\n\np_n(t) = p_{n-1}(t) + a(t - x_1)(t - x_2)\\dots(t - x_n)\n\ninterpolates the point (x, f(x)) in addition to the original n points (x_i, f(x_i)), i = 1, \\dots, n.\nThen h(t) = f(t) - p_n(t) must have at least n+1 roots. What are these roots?"
  },
  {
    "objectID": "mathematics/numerical-analysis/polynomial-interpolation/practice-problems/root-analysis.html#solution",
    "href": "mathematics/numerical-analysis/polynomial-interpolation/practice-problems/root-analysis.html#solution",
    "title": "Interpolation Polynomial - Root Analysis",
    "section": "Solution",
    "text": "Solution\n\n1. Analyze the roots of h(t)\nBy definition:\n\nh(t) = f(t) - p_n(t)\n\nSince p_n(t) interpolates f(x) at n+1 points (the original n points and the newly added point (x, f(x))), h(t) is zero at these n+1 points. Therefore, the roots of h(t) include:\n\nThe original interpolation points: t = x_1, x_2, \\dots, x_n\nThe newly added point: t = x\n\nThus, the roots of h(t) are \\{ x_1, x_2, \\dots, x_n, x \\}.\n\n\n2. Polynomial degree and root count\nThe degree of p_n(t) is n, since p_n(t) adds a single term of degree n to p_{n-1}(t). Therefore, h(t) = f(t) - p_n(t), being the difference of two degree n polynomials, is also a degree n polynomial.\nHowever, since h(t) is zero at n+1 distinct points, h(t) must be identically zero if f(t) is a polynomial of degree at most n. Otherwise, f(t) introduces higher-order terms that cancel the polynomial structure of p_n(t).\n\n\nConclusion\nThe n+1 roots of h(t) = f(t) - p_n(t) are precisely:\n\nx_1, x_2, \\dots, x_n, x"
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/index.html",
    "href": "mathematics/numerical-analysis/projects/index.html",
    "title": "REALITY CHECKS",
    "section": "",
    "text": "Reality Check 01\n\nStewart Platform Kinematics\n\n\n\nReality Check 04\n\nGPS, Conditioning, and Nonlinear Least Squares\n\n\n\nReality Check 05\n\nMotion Control in Computer-Aided Modeling"
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04-info.html",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04-info.html",
    "title": "REALITY CHECK 04 Info",
    "section": "",
    "text": "The global positioning system (GPS) consists of 24 satellites carrying atomic clocks, orbiting the earth at an altitude of 20,200 km. Four satellites in each of six planes, slanted at 55° with respect to the poles, make two revolutions per day. At any time, from any point on earth, five to eight satellites are in the direct line of sight. Each satellite has a simple mission: to transmit carefully synchronized signals from predetermined positions in space, to be picked up by GPS receivers on earth. The receivers use the information, with some mathematics (described shortly), to determine accurate (x, y, z) coordinates of the receiver.\nAt a given instant, the receiver collects the synchronized signal from the i-th satellite and determines its transmission time t_i, the difference between the times the signal was transmitted and received. The nominal speed of the signal is the speed of light, c ≈ 299792.458 \\, \\text{km/sec}. Multiplying transmission time by c gives the distance of the satellite from the receiver, putting the receiver on the surface of a sphere centered at the satellite position and with radius ct_i. If three satellites are available, then three spheres are known, whose intersection consists of two points, as shown in Figure 4.16. One intersection point is the location of the receiver. The other is normally far from the earth’s surface and can be safely disregarded. In theory, the problem is reduced to computing this intersection, the common solution of three sphere equations.\nHowever, there is a major problem with this analysis. First, although the transmissions from the satellites are timed nearly to the nanosecond by onboard atomic clocks, the clock in the typical low-cost receiver on earth has relatively poor accuracy. If we solve the three equations with slightly inaccurate timing, the calculated position could be wrong by several kilometers. Fortunately, there is a way to fix this problem. The price to pay is one extra satellite. Define d to be the difference between the synchronized time on the (now four) satellite clocks and the earth-bound receiver clock. Denote the location of satellite i by (A_i, B_i, C_i). Then the true intersection point (x, y, z) satisfies\n(4.37)\n\nr_1(x, y, z, d) = \\sqrt{(x - A_1)^2 + (y - B_1)^2 + (z - C_1)^2} - c(t_1 - d) = 0\n\n\nr_2(x, y, z, d) = \\sqrt{(x - A_2)^2 + (y - B_2)^2 + (z - C_2)^2} - c(t_2 - d) = 0\n\n\nr_3(x, y, z, d) = \\sqrt{(x - A_3)^2 + (y - B_3)^2 + (z - C_3)^2} - c(t_3 - d) = 0\n\n\nr_4(x, y, z, d) = \\sqrt{(x - A_4)^2 + (y - B_4)^2 + (z - C_4)^2} - c(t_4 - d) = 0\n\nto be solved for the unknowns x, y, z, d. Solving the system reveals not only the receiver location, but also the correct time from the satellite clocks, due to knowing d. Therefore, the inaccuracy in the GPS receiver clock can be fixed by using one extra satellite.\nGeometrically speaking, four spheres may not have a common intersection point, but they will if the radii are expanded or contracted by the right common amount.\nThe system (4.37) representing the intersection of four spheres is the three-dimensional analogue of (4.35), representing the intersection point of three circles in the plane.\nThe system (4.37) can be seen to have two solutions (x, y, z, d). The equations can be equivalently written\n\n(x - A_1)^2 + (y - B_1)^2 + (z - C_1)^2 = [c(t_1 - d)]^2\n\n\n(x - A_2)^2 + (y - B_2)^2 + (z - C_2)^2 = [c(t_2 - d)]^2\n\n\n(x - A_3)^2 + (y - B_3)^2 + (z - C_3)^2 = [c(t_3 - d)]^2\n\n\n(x - A_4)^2 + (y - B_4)^2 + (z - C_4)^2 = [c(t_4 - d)]^2\n\nNote that by subtracting the last three equations from the first, three linear equations are obtained. Each linear equation can be used to eliminate a variable x, y, z, and by substituting into any of the original equations, a quadratic equation in the single variable d results. Therefore, system (4.37) has at most two real solutions, and they can be found by the quadratic formula.\nTwo further problems emerge when GPS is deployed. First is the conditioning of the system of equations (4.37). We will find that solving for (x, y, z, d) is ill-conditioned when the satellites are bunched closely in the sky.\nThe second difficulty is that the transmission speed of the signals is not precisely c. The signals pass through 100 km of ionosphere and 10 km of troposphere, whose electromagnetic properties may affect the transmission speed. Furthermore, the signals may encounter obstacles on earth before reaching the receiver, an effect called multipath interference. To the extent that these obstacles have an equal impact on each satellite path, introducing the time correction d on the right side of (4.37) helps. In general, however, this assumption is not viable and will lead us to add information from more satellites and consider applying Gauss–Newton to solve a least squares problem.\nConsider a three-dimensional coordinate system whose origin is the center of the earth (radius ≈ 6370 km). GPS receivers convert these coordinates into latitude, longitude, and elevation data for readout and more sophisticated mapping applications using global information systems (GIS), a process we will not consider here."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04-info.html#gps-conditioning-and-nonlinear-least-squares",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04-info.html#gps-conditioning-and-nonlinear-least-squares",
    "title": "REALITY CHECK 04 Info",
    "section": "",
    "text": "The global positioning system (GPS) consists of 24 satellites carrying atomic clocks, orbiting the earth at an altitude of 20,200 km. Four satellites in each of six planes, slanted at 55° with respect to the poles, make two revolutions per day. At any time, from any point on earth, five to eight satellites are in the direct line of sight. Each satellite has a simple mission: to transmit carefully synchronized signals from predetermined positions in space, to be picked up by GPS receivers on earth. The receivers use the information, with some mathematics (described shortly), to determine accurate (x, y, z) coordinates of the receiver.\nAt a given instant, the receiver collects the synchronized signal from the i-th satellite and determines its transmission time t_i, the difference between the times the signal was transmitted and received. The nominal speed of the signal is the speed of light, c ≈ 299792.458 \\, \\text{km/sec}. Multiplying transmission time by c gives the distance of the satellite from the receiver, putting the receiver on the surface of a sphere centered at the satellite position and with radius ct_i. If three satellites are available, then three spheres are known, whose intersection consists of two points, as shown in Figure 4.16. One intersection point is the location of the receiver. The other is normally far from the earth’s surface and can be safely disregarded. In theory, the problem is reduced to computing this intersection, the common solution of three sphere equations.\nHowever, there is a major problem with this analysis. First, although the transmissions from the satellites are timed nearly to the nanosecond by onboard atomic clocks, the clock in the typical low-cost receiver on earth has relatively poor accuracy. If we solve the three equations with slightly inaccurate timing, the calculated position could be wrong by several kilometers. Fortunately, there is a way to fix this problem. The price to pay is one extra satellite. Define d to be the difference between the synchronized time on the (now four) satellite clocks and the earth-bound receiver clock. Denote the location of satellite i by (A_i, B_i, C_i). Then the true intersection point (x, y, z) satisfies\n(4.37)\n\nr_1(x, y, z, d) = \\sqrt{(x - A_1)^2 + (y - B_1)^2 + (z - C_1)^2} - c(t_1 - d) = 0\n\n\nr_2(x, y, z, d) = \\sqrt{(x - A_2)^2 + (y - B_2)^2 + (z - C_2)^2} - c(t_2 - d) = 0\n\n\nr_3(x, y, z, d) = \\sqrt{(x - A_3)^2 + (y - B_3)^2 + (z - C_3)^2} - c(t_3 - d) = 0\n\n\nr_4(x, y, z, d) = \\sqrt{(x - A_4)^2 + (y - B_4)^2 + (z - C_4)^2} - c(t_4 - d) = 0\n\nto be solved for the unknowns x, y, z, d. Solving the system reveals not only the receiver location, but also the correct time from the satellite clocks, due to knowing d. Therefore, the inaccuracy in the GPS receiver clock can be fixed by using one extra satellite.\nGeometrically speaking, four spheres may not have a common intersection point, but they will if the radii are expanded or contracted by the right common amount.\nThe system (4.37) representing the intersection of four spheres is the three-dimensional analogue of (4.35), representing the intersection point of three circles in the plane.\nThe system (4.37) can be seen to have two solutions (x, y, z, d). The equations can be equivalently written\n\n(x - A_1)^2 + (y - B_1)^2 + (z - C_1)^2 = [c(t_1 - d)]^2\n\n\n(x - A_2)^2 + (y - B_2)^2 + (z - C_2)^2 = [c(t_2 - d)]^2\n\n\n(x - A_3)^2 + (y - B_3)^2 + (z - C_3)^2 = [c(t_3 - d)]^2\n\n\n(x - A_4)^2 + (y - B_4)^2 + (z - C_4)^2 = [c(t_4 - d)]^2\n\nNote that by subtracting the last three equations from the first, three linear equations are obtained. Each linear equation can be used to eliminate a variable x, y, z, and by substituting into any of the original equations, a quadratic equation in the single variable d results. Therefore, system (4.37) has at most two real solutions, and they can be found by the quadratic formula.\nTwo further problems emerge when GPS is deployed. First is the conditioning of the system of equations (4.37). We will find that solving for (x, y, z, d) is ill-conditioned when the satellites are bunched closely in the sky.\nThe second difficulty is that the transmission speed of the signals is not precisely c. The signals pass through 100 km of ionosphere and 10 km of troposphere, whose electromagnetic properties may affect the transmission speed. Furthermore, the signals may encounter obstacles on earth before reaching the receiver, an effect called multipath interference. To the extent that these obstacles have an equal impact on each satellite path, introducing the time correction d on the right side of (4.37) helps. In general, however, this assumption is not viable and will lead us to add information from more satellites and consider applying Gauss–Newton to solve a least squares problem.\nConsider a three-dimensional coordinate system whose origin is the center of the earth (radius ≈ 6370 km). GPS receivers convert these coordinates into latitude, longitude, and elevation data for readout and more sophisticated mapping applications using global information systems (GIS), a process we will not consider here."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04-info.html#instructions",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04-info.html#instructions",
    "title": "REALITY CHECK 04 Info",
    "section": "Instructions",
    "text": "Instructions\n\nSolve the system (4.37) by using a multivariate root finder in Python. There are\nmultiple options in scipy optimize, including root, fsolve, and newton_krylov.\nFind the receiver position (x, y, z) near earth and time correction d for known,\nsimultaneous satellite positions\n\n\n(15 600, 7540, 20 140), \\quad (18 760, 2750, 18 610),\n(17 610, 14 630, 13 480), \\quad (19 170, 610, 18 390)\n\nin km, and measured time intervals 0.07074, 0.07220, 0.07690, 0.07242 in\nseconds, respectively. Set the initial vector to be (x_0, y_0, z_0, d_0) = (0, 0, 6370.0, 0).\nAs a check, the answers are approximately\n\n(x, y, z) = (-41.77271, -16.78919, 6370.0596)\n\nand\n\nd = -3.201566 \\times 10^{-3} \\text{ seconds}.\n\n\nWrite a Python program to carry out the solution via the quadratic formula.\nHint: Subtracting the last three equations of (4.37) from the first yields three\nlinear equations in the four unknowns\n\n\nx u_x + y u_y + z u_z + d u_d + w = 0,\n\nexpressed in vector form. A formula for x in terms of d can be obtained from\n\n0 = \\text{det}[u_y | u_z | x u_x + y u_y + z u_z + d u_d + w]\n\nnoting that the determinant is linear in its columns and that a matrix with a\nrepeated column has determinant zero. Similarly, we can arrive at formulas for y\nand z, respectively, in terms of d, that can be substituted in the first quadratic\nequation of (4.37), to make it an equation in one variable.\n\nSkip\nNow set up a test of the conditioning of the GPS problem\n\nDefine satellite positions (A_i, B_i, C_i) from spherical coordinates (\\rho, \\phi_i, \\theta_i) as:\n\nA_i = \\rho \\cos(\\phi_i) \\cos(\\theta_i)\n\n\nB_i = \\rho \\cos(\\phi_i) \\sin(\\theta_i)\n\n\nC_i = \\rho \\sin(\\phi_i)\n\nwhere \\rho = 26 \\, 570 \\, \\text{km} is fixed, while 0 \\leq \\phi_i \\leq \\pi / 2 and 0 \\leq \\theta_i \\leq 2 \\pi for i = 1, \\ldots, 4 are chosen arbitrarily. The \\phi coordinate is restricted so that the four satellites are in the upper hemisphere. Set x = 0, y = 0, z = 6370, d = 0.0001, and calculate the corresponding satellite ranges:\n\nR_i = \\sqrt{A_i^2 + B_i^2 + (C_i - 6370)^2}\n\nand travel times:\n\nt_i = d + R_i / c\n\nWe will define an error magnification factor specially tailored to the situation. The atomic clocks aboard the satellites are correct up to about 10 nanoseconds, or 10^{-8} \\, \\text{second}. Therefore, it is important to study the effect of changes in the transmission time of this magnitude.\nLet the backward, or input error, be the input change in meters. At the speed of light, \\Delta t*i = 10^{-8} \\, \\text{second} corresponds to 10^{-8} s \\approx 3 \\, \\text{meters}. Let the forward, or output error, be the change in position \\|\\Delta x, \\Delta y, \\Delta z\\|*\\infty, caused by such a change in t_i, also in meters. Then we can define the dimensionless error magnification factor (EMF):\n\n\\text{EMF} = \\frac{\\|\\Delta x, \\Delta y, \\Delta z\\|_\\infty}{c \\|\\Delta t_1, \\ldots, \\Delta t_m\\|_\\infty}\n\nand the condition number of the problem to be the maximum error magnification factor for all small \\Delta t_i (say, 10^{-8} or less).\nChange each t*i defined in the foregoing by \\Delta t_i = \\pm 10^{-8}, not all the same. Denote the new solution of the equations (4.37) by (x, y, z, d) and compute the difference in position \\|\\Delta x, \\Delta y, \\Delta z\\|*\\infty and the error magnification factor. Try different variations of the \\Delta t_i’s.\nWhat is the maximum position error found, in meters? Estimate the condition number of the problem, on the basis of the error magnification factors you have computed."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04.html",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04.html",
    "title": "REALITY CHECK 04",
    "section": "",
    "text": "The Global Positioning System (GPS) is a satellite-based navigation system consisting of 24 satellites equipped with atomic clocks, orbiting the Earth at an altitude of 20,200 km. By measuring the travel time of signals transmitted from satellites to a receiver, GPS calculates the receiver’s position in three-dimensional space. Each signal defines a sphere with a radius equal to the distance traveled by the signal, and the receiver’s position is at the intersection of these spheres.\nTo determine an accurate position (x, y, z) and synchronize the receiver’s clock, at least four satellites are required. The system of nonlinear equations representing the sphere intersections can be formulated as:\n\nr_i(x, y, z, d) = \\sqrt{(x - A_i)^2 + (y - B_i)^2 + (z - C_i)^2} - c(t_i - d) = 0\n\nwhere d is a clock correction factor for the receiver. Solving this system reveals both the receiver’s coordinates and the corrected clock time.\nHowever, the system faces challenges due to:\n\nReceiver Clock Inaccuracy: Affordable GPS receivers lack the precision of satellite atomic clocks, necessitating the inclusion of d in the equations.\nIll-Conditioning: The system becomes sensitive to errors when satellites are clustered close together in the sky.\nEnvironmental Factors: Signal transmission speed is affected by atmospheric interference and obstacles, introducing further inaccuracies.\n\nTo address these challenges:\n\nNumerical Root-Finding Methods (e.g., Newton-Krylov, Gauss-Newton) iteratively solve the equations but can struggle with sensitivity to initial guesses and ill-conditioning.\nError Analysis evaluates the system’s sensitivity to input timing errors, quantified through the Error Magnification Factor (EMF), which assesses the impact of small changes in signal timing on positional accuracy.\n\nBy leveraging precise satellite positions and signal timing data, this system provides accurate navigation but requires advanced mathematical techniques to manage inherent nonlinearities and ensure robustness. This report focuses on exploring and solving the GPS positioning problem through numerical and analytical approaches, providing insights into accuracy, stability, and efficiency."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04.html#overview",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04.html#overview",
    "title": "REALITY CHECK 04",
    "section": "",
    "text": "The Global Positioning System (GPS) is a satellite-based navigation system consisting of 24 satellites equipped with atomic clocks, orbiting the Earth at an altitude of 20,200 km. By measuring the travel time of signals transmitted from satellites to a receiver, GPS calculates the receiver’s position in three-dimensional space. Each signal defines a sphere with a radius equal to the distance traveled by the signal, and the receiver’s position is at the intersection of these spheres.\nTo determine an accurate position (x, y, z) and synchronize the receiver’s clock, at least four satellites are required. The system of nonlinear equations representing the sphere intersections can be formulated as:\n\nr_i(x, y, z, d) = \\sqrt{(x - A_i)^2 + (y - B_i)^2 + (z - C_i)^2} - c(t_i - d) = 0\n\nwhere d is a clock correction factor for the receiver. Solving this system reveals both the receiver’s coordinates and the corrected clock time.\nHowever, the system faces challenges due to:\n\nReceiver Clock Inaccuracy: Affordable GPS receivers lack the precision of satellite atomic clocks, necessitating the inclusion of d in the equations.\nIll-Conditioning: The system becomes sensitive to errors when satellites are clustered close together in the sky.\nEnvironmental Factors: Signal transmission speed is affected by atmospheric interference and obstacles, introducing further inaccuracies.\n\nTo address these challenges:\n\nNumerical Root-Finding Methods (e.g., Newton-Krylov, Gauss-Newton) iteratively solve the equations but can struggle with sensitivity to initial guesses and ill-conditioning.\nError Analysis evaluates the system’s sensitivity to input timing errors, quantified through the Error Magnification Factor (EMF), which assesses the impact of small changes in signal timing on positional accuracy.\n\nBy leveraging precise satellite positions and signal timing data, this system provides accurate navigation but requires advanced mathematical techniques to manage inherent nonlinearities and ensure robustness. This report focuses on exploring and solving the GPS positioning problem through numerical and analytical approaches, providing insights into accuracy, stability, and efficiency."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04.html#objective",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04.html#objective",
    "title": "REALITY CHECK 04",
    "section": "1.1 Objective",
    "text": "1.1 Objective\nThe objective of this study is to calculate the position of a GPS receiver (x, y, z) and correct the receiver’s clock bias d by solving a system of nonlinear equations. These equations model the distance between the receiver and multiple satellites based on signal travel times. The numerical solution relies on iterative root-finding methods to achieve high accuracy."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04.html#background",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04.html#background",
    "title": "REALITY CHECK 04",
    "section": "1.2 Background",
    "text": "1.2 Background\nIn a GPS system, the position of a receiver is determined by measuring the time it takes for signals to travel from multiple satellites. The propagation time, when multiplied by the speed of light c, provides the distance between the satellite and the receiver. Mathematically, the relationship between the receiver’s unknown position (x, y, z) and the clock correction d is expressed as a system of nonlinear equations:\n\n\\sqrt{(x - A_i)^2 + (y - B_i)^2 + (z - C_i)^2} = c(t_i - d), \\quad i = 1, 2, 3, 4\n\nWhere:\n\n(A_i, B_i, C_i) are the known satellite positions.\nt_i are the measured signal travel times.\nc \\approx 299,792.458 \\, \\text{km/s} is the speed of light.\n(x, y, z) are the unknown receiver coordinates.\nd is the receiver clock correction.\n\nEach equation corresponds to a sphere centered at the satellite’s position, with the radius equal to the computed distance. The receiver’s position lies at the intersection of these spheres. The inclusion of d accounts for the clock bias in the receiver, which is less precise than the atomic clocks onboard GPS satellites."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04.html#methodology",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04.html#methodology",
    "title": "REALITY CHECK 04",
    "section": "1.3 Methodology",
    "text": "1.3 Methodology\nA numerical root-finding approach is employed to solve this system of nonlinear equations. Specifically, Python’s fsolve function from the scipy.optimize library is utilized. This method iteratively refines estimates for (x, y, z, d) until the residuals—the differences between the measured and calculated distances—are minimized.\n\n1.3.1 Formulation of the Problem\nThe system of equations is reformulated as residual functions f_i, where each function evaluates to zero when the system is satisfied:\n\nf_i(x, y, z, d) = \\sqrt{(x - A_i)^2 + (y - B_i)^2 + (z - C_i)^2} - c(t_i - d), \\quad i = 1, 2, 3, 4\n\nThe root-finding process involves solving for (x, y, z, d) such that all f_i(x, y, z, d) = 0.\n\n\n1.3.2 Numerical Solution\nThe computational steps are as follows:\n\nInput Data: Known satellite positions (A_i, B_i, C_i) and signal travel times t_i are provided as inputs, along with the speed of light c.\nInitial Guess: A starting point of (x_0, y_0, z_0) = (0, 0, 6370) km is assumed, placing the receiver near the Earth’s surface. The initial clock correction is set to d_0 = 0.\nIterative Solver: The fsolve function iteratively adjusts (x, y, z, d) to minimize the residuals f_i. The algorithm terminates when all residuals approach zero, indicating convergence to a solution.\n\nThe implementation was tested using the following dataset:\n\nSatellite positions:\n\n(15,600, 7,540, 20,140)\n(18,760, 2,750, 18,610)\n(17,610, 14,630, 13,480)\n(19,170, 610, 18,390)\n\nSignal travel times: \nt = [0.07074, 0.07220, 0.07690, 0.07242] \\, \\text{(in seconds)}\n\n\n\n\n\n\n\n\nAccompanying Code\n\n\n\nThe accompanying code implementation defines the residual function, applies the fsolve solver, and outputs the computed results."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04.html#results",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04.html#results",
    "title": "REALITY CHECK 04",
    "section": "1.4 Results",
    "text": "1.4 Results\nThe numerical root-finding method produced the following results:\n\nReceiver Position (in km): \n(x, y, z) = (-41.77271, -16.78919, 6370.0596)\n\nClock Correction (in seconds): \nd = -3.201566 \\times 10^{-3}\n\n\nThese values were verified against expected results, confirming the accuracy of the numerical solution. The computed receiver position aligns with the Earth’s surface, and the clock correction accounts for the slight bias in the receiver’s timing."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04.html#limitations",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04.html#limitations",
    "title": "REALITY CHECK 04",
    "section": "1.5 Limitations",
    "text": "1.5 Limitations\nWhile the numerical root-finding approach proved effective, several limitations should be noted:\n\nSensitivity to Ill-Conditioning: The system may become ill-conditioned when satellite positions are clustered, amplifying numerical errors and reducing accuracy.\nDependence on Initial Guesses: Poor initial guesses can lead to non-convergence or convergence to an incorrect solution.\nComputational Cost: Iterative methods like fsolve require multiple evaluations of the residuals, making them computationally expensive for real-time applications.\nLack of Analytical Insights: The numerical solution provides no explicit relationships between the variables, limiting its use for sensitivity analysis or theoretical exploration."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04.html#motivation-for-an-improved-approach",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04.html#motivation-for-an-improved-approach",
    "title": "REALITY CHECK 04",
    "section": "1.6 Motivation for an Improved Approach",
    "text": "1.6 Motivation for an Improved Approach\nThe limitations of the numerical root-finding method highlight the need for a more robust and efficient solution. Specifically:\n\nA method that avoids iterative guesswork.\nImproved handling of ill-conditioned systems.\nA more analytical approach that isolates variables and reduces the problem complexity.\n\nTo address these concerns, the next section introduces a determinant-based analytical approach. This method linearizes the system of equations, isolates variables explicitly, and reduces the problem to solving a single quadratic equation for the clock correction d. It offers both computational efficiency and greater stability, making it well-suited for real-time GPS positioning."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04.html#conclusion",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04.html#conclusion",
    "title": "REALITY CHECK 04",
    "section": "1.7 Conclusion",
    "text": "1.7 Conclusion\nThe numerical root-finding approach provides an approximate solution to the GPS equations but is limited by its sensitivity, inefficiency, and lack of robustness. These shortcomings motivate the need for an improved analytical method, which will be explored in the following section."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04.html#objective-1",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04.html#objective-1",
    "title": "REALITY CHECK 04",
    "section": "2.1 Objective",
    "text": "2.1 Objective\nThe objective of this section is to solve the GPS equations analytically using a determinant-based approach. Unlike numerical root-finding methods, this analytical technique avoids iterative guesswork by isolating the variables (x, y, z, d) explicitly. By systematically transforming the original nonlinear system into a more manageable linear form, this method ensures greater robustness, stability, and computational efficiency—critical for real-time or precision GPS applications.\nThe approach has been implemented in Python using the SymPy library to symbolically manipulate the equations, solve the system, and isolate the unknowns step-by-step."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04.html#problem-formulation",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04.html#problem-formulation",
    "title": "REALITY CHECK 04",
    "section": "2.2 Problem Formulation",
    "text": "2.2 Problem Formulation\nThe GPS receiver’s position (x, y, z) and clock offset d are determined from four satellite equations of the form:\n\n\\sqrt{(x - A_i)^2 + (y - B_i)^2 + (z - C_i)^2} = c(t_i - d), \\quad i = 1, 2, 3, 4\n\nHere:\n\n(A_i, B_i, C_i) are known satellite coordinates.\nt_i are measured signal travel times.\nc \\approx 299792.458\\,\\text{km/s} is the speed of light.\n(x, y, z) and d are the unknown receiver coordinates and clock correction.\n\nEach equation represents a sphere with the satellite at its center. The receiver lies at the intersection of these four spheres. However, the presence of square roots and the unknown d makes the system inherently nonlinear and challenging to solve directly."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04.html#methodology-1",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04.html#methodology-1",
    "title": "REALITY CHECK 04",
    "section": "2.3 Methodology",
    "text": "2.3 Methodology\n\n\n\n\n\n\nCode Implementation\n\n\n\nThe solution process involves three key stages, all of which are implemented in the accompanying Python code.\n\n\n\n2.3.1 Reducing the Nonlinear System to a Linear Form\nTo eliminate the square roots, we square each equation. After doing so, we have:\n\n(x - A_i)^2 + (y - B_i)^2 + (z - C_i)^2 = c^2(t_i - d)^2\n\nWe then subtract the equations for satellites i=2,3,4 from the equation for i=1. This subtraction removes the x^2 + y^2 + z^2 terms and results in three linear equations in the four unknowns x, y, z, d:\n\nx u_{x}^{(j)} + y u_{y}^{(j)} + z u_{z}^{(j)} + d u_{d}^{(j)} + w^{(j)} = 0,\\quad j=1,2,3\n\nwhere u_x^{(j)}, u_y^{(j)}, u_z^{(j)}, u_d^{(j)}, w^{(j)} are constants determined by the satellite positions and travel times.\nAt this point, we have three linear equations with four unknowns:\n\nx u_{x1} + y u_{y1} + z u_{z1} + d u_{d1} + w_1 = 0\n\n\nx u_{x2} + y u_{y2} + z u_{z2} + d u_{d2} + w_2 = 0\n\n\nx u_{x3} + y u_{y3} + z u_{z3} + d u_{d3} + w_3 = 0\n\n\n\n\n\n\n\nAccompanying Code\n\n\n\nIn the accompanying code, this system of equations is constructed symbolically using SymPy, with the simplify() function applied to ensure clarity and precision in the linearized expressions.\n\n\n\nWhy Solve for the Variables in Terms of d?\n\nWith three linear equations and four unknowns, the system is underdetermined. We cannot directly solve for all four variables at once. To proceed, we:\n\nTreat d as a parameter.\nExpress x, y, z as linear functions of d:\n\nx = f_x(d), \\quad y = f_y(d), \\quad z = f_z(d)\n\n\nOnce x, y, z are known in terms of d, we substitute these functions back into one of the original nonlinear equations. This will yield a single quadratic equation in d. Solving that quadratic equation gives us d. With d in hand, we easily find x, y, z.\nThis approach reduces the complexity: instead of trying to solve four nonlinear equations simultaneously, we simplify the problem to solving one quadratic equation after isolating variables in terms of d.\n\n\nIntroducing the Determinant Equation\n\nTo isolate x, y, z in terms of d, we use a determinant-based approach. Determinants are a powerful linear algebra tool. By arranging our linear equations in a matrix form and considering certain determinants, we can:\n\nIdentify linear dependencies,\nIsolate one variable at a time,\nAvoid ambiguity and instability.\n\nThe idea is to construct a determinant from the system of equations and carefully manipulate it so that one variable (e.g., x) can be extracted in terms of y, z, d and constants. We then repeat or apply a similar reasoning for y and z.\n\n\n\n2.3.2 Isolating x in Terms of d Using the Determinant\nStep-by-Step for x:\n\nSet Up the Determinant:\nConsider a matrix formed from the coefficient vectors \\mathbf{u_x}, \\mathbf{u_y}, \\mathbf{u_z}, \\mathbf{u_d}, \\mathbf{w}. A key construction is:\n\n\\det[\\mathbf{u_y} \\mid \\mathbf{u_z} \\mid x\\mathbf{u_x} + y\\mathbf{u_y} + z\\mathbf{u_z} + d\\mathbf{u_d} + \\mathbf{w}] = 0\n\nThis determinant equals zero because if -\\mathbf{w} lies in the span of the other vectors, the system is consistent. Expanding this determinant along the third column will separate terms involving x, y, z, d.\n\n\\text{det} \\begin{bmatrix}\nu_{y1} & u_{z1} & x u_{x1} + y u_{y1} + z u_{z1} + d u_{d1} + w_1 \\\\\nu_{y2} & u_{z2} & x u_{x2} + y u_{y2} + z u_{z2} + d u_{d2} + w_2 \\\\\nu_{y3} & u_{z3} & x u_{x3} + y u_{y3} + z u_{z3} + d u_{d3} + w_3\n\\end{bmatrix} = 0\n\nExpand the Determinant:\nExpanding along the third column gives:\n\n\\text{det}[\\dots] = (x u_{x1} + y u_{y1} + z u_{z1} + d u_{d1} + w_1) \\cdot \\text{det} \\begin{bmatrix}\nu_{y2} & u_{z2} \\\\\nu_{y3} & u_{z3}\n\\end{bmatrix}\n\n\n- (x u_{x2} + y u_{y2} + z u_{z2} + d u_{d2} + w_2) \\cdot \\text{det} \\begin{bmatrix}\nu_{y1} & u_{z1} \\\\\nu_{y3} & u_{z3}\n\\end{bmatrix}\n\n\n+ (x u_{x3} + y u_{y3} + z u_{z3} + d u_{d3} + w_3) \\cdot \\text{det} \\begin{bmatrix}\nu_{y1} & u_{z1} \\\\\nu_{y2} & u_{z2}\n\\end{bmatrix}\n\nGroup Terms Involving x, y, z, d:\nCollect coefficients for each variable. For example for x, the coefficient C_x is:\n\nC_x = u_{x1} \\cdot \\text{det} \\begin{bmatrix}\nu_{y2} & u_{z2} \\\\\nu_{y3} & u_{z3}\n\\end{bmatrix}\n- u_{x2} \\cdot \\text{det} \\begin{bmatrix}\nu_{y1} & u_{z1} \\\\\nu_{y3} & u_{z3}\n\\end{bmatrix}\n+ u_{x3} \\cdot \\text{det} \\begin{bmatrix}\nu_{y1} & u_{z1} \\\\\nu_{y2} & u_{z2}\n\\end{bmatrix}\n\nOn expansion, you get an equation of the form:\n\nC_x x + C_y y + C_z z + C_d d + T = 0\n\nwhere C_x, C_y, C_z, C_d, T are combinations of determinants and known constants derived from the satellite data.\nIsolate x:\nTo solve for x, rearrange the equation:\n\nx = -\\frac{C_y y + C_z z + C_d d + T}{C_x}\n\nHowever, at this stage, x is still expressed in terms of y, z, d. To get x purely in terms of d, you must similarly isolate y and z in terms of d.\nRepeat for y and z:\nBy constructing similar determinant equations and performing analogous expansions, you isolate y and z as linear functions of d:\n\ny = f_y(d), \\quad z = f_z(d)\n\nOnce y(d) and z(d) are known, substitute them back into the expression for x:\n\nx = f_x(d)\n\nNow all three spatial variables are functions of d:\n\nx = f_x(d), \\quad y = f_y(d), \\quad z = f_z(d)\n\n\n\n\n\n\n\n\nAccompanying Code\n\n\n\nThe accompanying code uses SymPy’s linear_eq_to_matrix to extract the coefficient matrix and split it into parts:\n\nA_{xyz}: Coefficients for x, y, z,\nA_d: Coefficients for d.\n\nThe resulting system is:\n\nA\\_{xyz} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} = -A_d d - \\mathbf{w}.\n\nSolving this system for x, y, and z in terms of d is achieved using the LUsolve function. The symbolic solutions x(d), y(d), and z(d) are simplified and stored.\n\n\n\n\n2.3.3 Forming the Quadratic Equation in d\nWith x(d), y(d), z(d) established, we return to an original nonlinear equation. For example:\n\n\\sqrt{(x - A_1)^2 + (y - B_1)^2 + (z - C_1)^2} = c(t_1 - d)\n\n\nSubstitute x(d), y(d), z(d) into the left-hand side.\nSquare both sides to remove the square root.\n\nAfter simplification, you obtain a quadratic equation in d:\n\na d^2 + b d + c = 0\n\nSolving this quadratic equation using the quadratic formula:\n\nd = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n\nChoose the physically meaningful solution for d (the one placing the receiver near Earth’s surface), then substitute d back into x(d), y(d), z(d) to find the final coordinates (x, y, z).\n\n\n\n\n\n\nAccompanying Code\n\n\n\n\nOnce x(d), y(d), and z(d) are known, they are substituted back into one of the original nonlinear equations (e.g., the first equation). This substitution is performed symbolically in the code using subs().\nSimplifying the resulting equation produces a quadratic equation in d:\n\n\na d^2 + b d + c = 0.\n\n\nThe coefficients of the quadratic equation are extracted using SymPy’s Poly and all_coeffs functions. The quadratic formula is then applied to solve for d.\nAmong the solutions for d, the physically meaningful (real and close to zero) solution is selected. This step is automated in the code by evaluating the solutions and filtering for real roots.\nThe final values of x, y, and z are computed by substituting the selected d back into x(d), y(d), and z(d).\n\nResults\nThe determinant-based analytical approach produces the following results:\n\nReceiver Position (in km): \n(x, y, z) = (-41.77271, -16.78919, 6370.0596)\n\nClock Correction (in seconds): \nd = -3.201566 \\times 10^{-3}.\n\n\nThese results are consistent with the numerical solutions obtained in numerical root-finding approach, confirming the correctness of the analytical method."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04.html#conclusion-1",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04.html#conclusion-1",
    "title": "REALITY CHECK 04",
    "section": "2.4 Conclusion",
    "text": "2.4 Conclusion\nThis determinant-based analytical approach transforms the original nonlinear GPS equations into a linearized problem, uses determinants to isolate x, y, z in terms of d, and ultimately reduces the entire system to solving a single quadratic equation in d.\nKey Advantages:\n\nNo iterative guesswork required, avoiding convergence issues.\nMore stable and robust, especially when satellites are poorly distributed.\nProvides a closed-form, analytical solution, offering deeper insight and efficiency.\n\nBy following these steps—reducing to a linear system, isolating variables in terms of d using determinants, and forming a single quadratic in d—we obtain a direct and reliable solution to the GPS positioning problem."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04.html#objective-2",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04.html#objective-2",
    "title": "REALITY CHECK 04",
    "section": "3.1 Objective",
    "text": "3.1 Objective\nThe objective of this section is to evaluate how the conditioning of the GPS system changes when the satellite positions are either loosely distributed or tightly grouped. Sensitivity to small errors in signal travel times t_i is analyzed using the Error Magnification Factor (EMF). By comparing EMF values for both configurations, we identify how satellite geometry impacts the robustness and stability of the GPS positioning system."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04.html#background-1",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04.html#background-1",
    "title": "REALITY CHECK 04",
    "section": "3.2 Background",
    "text": "3.2 Background\n\n3.2.1 GPS Sensitivity and Conditioning\nIn GPS systems, small timing errors \\Delta t_i in the satellite signals can result in significant positional errors. These errors arise from the ill-conditioning of the system of equations, which occurs when satellite positions are geometrically clustered.\nThe system is analyzed by:\n\nIntroducing Perturbations: A small timing error \\Delta t_i \\approx 10^{-8} \\, \\text{s} (equivalent to 3 \\, \\text{meters}) is applied to each signal.\nForward Error: The resulting change in the computed position is measured as: \n\\|\\Delta x, \\Delta y, \\Delta z\\|_2 = \\sqrt{(\\Delta x)^2 + (\\Delta y)^2 + (\\Delta z)^2}\n\nError Magnification Factor (EMF): The EMF quantifies the sensitivity of the system to perturbations: \n\\text{EMF} = \\frac{\\|\\Delta x, \\Delta y, \\Delta z\\|_2}{c \\, \\|\\Delta t_i\\|_2}\n A higher EMF indicates a poorly conditioned system.\n\n\n\n3.2.2 Satellite Geometry\n\nLoosely Distributed Satellites: Satellites are widely spaced across the sky, providing better geometric diversity. This reduces ill-conditioning and improves robustness.\nTightly Grouped Satellites: Satellites are close together (within 5% of one another in spherical coordinates), leading to geometric correlation and increased sensitivity to timing errors."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04.html#methodology-2",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04.html#methodology-2",
    "title": "REALITY CHECK 04",
    "section": "3.3 Methodology",
    "text": "3.3 Methodology\n\n3.3.1 Steps to Solve\nThe analysis was conducted as follows:\n\nSatellite Position Generation:\n\nLoosely distributed satellites were generated using diverse spherical coordinates (\\phi and \\theta).\nTightly grouped satellites were generated by restricting both \\phi_i and \\theta_i within 5% of one another.\n\nNominal and Perturbed Signal Times:\n\nFor both configurations, nominal signal travel times t_i were calculated based on the range: \nR_i = \\sqrt{A_i^2 + B_i^2 + (C_i - 6370)^2}, \\quad t_i = d + \\frac{R_i}{c}\n\nEach t_i was perturbed by 10^{-8} \\, \\text{s}, while other times were held constant.\n\nSolve for Position:\n\nThe GPS equations were solved numerically using fsolve for both nominal and perturbed travel times.\nThe positional error \\|\\Delta x, \\Delta y, \\Delta z\\|_2 was computed for each perturbation.\n\nCalculate EMF:\nThe EMF was calculated using the formula:\n\n\\text{EMF} = \\frac{\\|\\Delta x, \\Delta y, \\Delta z\\|_2}{c \\, \\|\\Delta t_i\\|_2}\n\nCompare Results:\nEMF values were compared for both the loosely distributed and tightly grouped satellite configurations.\n\n\n\n\n\n\n\nAccompanying Code\n\n\n\nThe accompanying code performs the following steps:\n\nGenerate Satellite Positions:\n\nLoosely spaced satellites use diverse \\phi and \\theta values.\nTightly grouped satellites have \\phi and \\theta within 5% of each other.\n\nCalculate Ranges and Times:\nNominal ranges R_i and travel times t_i are computed.\nIntroduce Perturbations:\nEach signal time t_i is perturbed by 10^{-8} \\, \\text{s}, and the GPS equations are solved numerically using fsolve.\nCompute Forward Error and EMF:\n\nThe positional error is computed as the Euclidean distance between the nominal and perturbed positions.\nEMF values are calculated for each perturbation.\n\nComparison of Configurations:\nEMF values for loosely and tightly grouped satellites are compared, and the maximum EMF is identified."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04.html#results-1",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04.html#results-1",
    "title": "REALITY CHECK 04",
    "section": "3.4 Results",
    "text": "3.4 Results\nThe results of the EMF analysis for both satellite configurations are as follows:\n\n3.4.1 Loose Satellites\n\n\n\nPerturbation in t_i\nEMF Value\n\n\n\n\nt_1\n2.207538\n\n\nt_2\n2.828432\n\n\nt_3\n2.856126\n\n\nt_4\n2.203932\n\n\n\nMaximum EMF:\n\n\\text{EMF}_{\\text{max}} = 2.856126\n\n\n\n3.4.2 Tightly Grouped Satellites\n\n\n\nPerturbation in t_i\nEMF Value\n\n\n\n\nt_1\n1553.480087\n\n\nt_2\n2317.787515\n\n\nt_3\n3402.498373\n\n\nt_4\n466.068303\n\n\n\nMaximum EMF:\n\n\\text{EMF}_{\\text{max}} = 3402.498373\n\n\n\n3.4.3 Comparison\n\n\n\nConfiguration\nEMF Range\nMaximum EMF\nSensitivity\n\n\n\n\nLoose Satellites\n2.2 to 2.8\n2.856126\nLow\n\n\nTightly Grouped\n466 to 3402\n3402.498373\nVery High"
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04.html#interpretation",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04.html#interpretation",
    "title": "REALITY CHECK 04",
    "section": "3.5 Interpretation",
    "text": "3.5 Interpretation\nThe results demonstrate a clear relationship between satellite geometry and the conditioning of the GPS system:\n\nLoose Satellites:\n\nEMF values remain low (around 2–3), indicating that the system is well-conditioned.\nPerturbations in signal times result in small position errors due to the geometric diversity of the satellites.\n\nTightly Grouped Satellites:\n\nEMF values increase dramatically (up to 3402), showing that the system becomes ill-conditioned.\nSmall input errors are significantly amplified, leading to large position errors.\nThis sensitivity arises from the satellites’ correlated geometry, where their signals cannot provide sufficient independent information."
  },
  {
    "objectID": "mathematics/numerical-analysis/projects/rc04/rc04.html#conclusion-2",
    "href": "mathematics/numerical-analysis/projects/rc04/rc04.html#conclusion-2",
    "title": "REALITY CHECK 04",
    "section": "3.6 Conclusion",
    "text": "3.6 Conclusion\nThis analysis highlights the critical role of satellite geometry in the conditioning of the GPS system:\n\nLoosely distributed satellites provide better geometric diversity and robustness, resulting in lower EMF values and greater accuracy.\nTightly grouped satellites lead to poor conditioning, amplifying errors and making the system highly sensitive to small perturbations in signal travel times.\n\nTo ensure accurate and stable GPS positioning, it is essential to use satellites with diverse spatial distributions. This insight is particularly valuable for optimizing satellite selection algorithms in GPS receivers."
  },
  {
    "objectID": "mathematics/numerical-analysis/quadrature/index.html",
    "href": "mathematics/numerical-analysis/quadrature/index.html",
    "title": "QUADRATURE",
    "section": "",
    "text": "Trapezoidal Rule"
  },
  {
    "objectID": "mathematics/numerical-analysis/root-finding-methods/bisection-method.html",
    "href": "mathematics/numerical-analysis/root-finding-methods/bisection-method.html",
    "title": "Bisection Method",
    "section": "",
    "text": "Bisection Method is one of the simplest and most reliable numerical methods for finding a root of a continuous function f(x) = 0 over a closed interval [a, b]. The method works by repeatedly bisecting the interval and then selecting the subinterval in which the function changes sign, ensuring that a root lies within that subinterval."
  },
  {
    "objectID": "mathematics/numerical-analysis/root-finding-methods/bisection-method.html#the-bisection-method-formula",
    "href": "mathematics/numerical-analysis/root-finding-methods/bisection-method.html#the-bisection-method-formula",
    "title": "Bisection Method",
    "section": "The Bisection Method Formula",
    "text": "The Bisection Method Formula\nThe Bisection Method requires that the function f(x) be continuous over the interval [a, b], and that the function has opposite signs at the endpoints a and b, i.e., f(a) \\cdot f(b) &lt; 0. This guarantees that there is at least one root in the interval by the Intermediate Value Theorem.\nThe basic idea of the method is to repeatedly bisect the interval and check the sign of f(x) at the midpoint to determine the subinterval containing the root.\n\nAlgorithm\n\nInitial Guess: Choose an interval [a_0, b_0] such that f(a_0) \\cdot f(b_0) &lt; 0.\nMidpoint Calculation: Compute the midpoint c_k = \\frac{a_k + b_k}{2} of the interval [a_k, b_k].\nCheck the Sign: Evaluate f(c_k).\n\nIf f(c_k) = 0, then c_k is the root.\nIf f(a_k) \\cdot f(c_k) &lt; 0, set b_{k+1} = c_k, and the root lies in [a_k, c_k].\nIf f(c_k) \\cdot f(b_k) &lt; 0, set a_{k+1} = c_k, and the root lies in [c_k, b_k].\n\nRepeat: Continue bisecting the interval until the length of the interval is smaller than a specified tolerance \\epsilon, or until |f(c_k)| &lt; \\epsilon.\n\nThe approximate root will be:\n\nx^* = \\frac{a_k + b_k}{2}\n\n\n\nConvergence\nThe Bisection Method converges linearly. The length of the interval halves at each iteration, ensuring that the method always converges to a solution (if one exists) within the interval.\nThe number of iterations n required to achieve an accuracy of \\epsilon can be estimated by:\n\nn \\geq \\frac{\\log \\left( \\frac{b_0 - a_0}{\\epsilon} \\right)}{\\log 2}\n\n\n\nExample\nLet’s solve the equation f(x) = x^3 - x - 2 = 0 in the interval [1, 2].\n\nInitial Interval:\nf(1) = 1^3 - 1 - 2 = -2\nf(2) = 2^3 - 2 - 2 = 4\nSince f(1) \\cdot f(2) &lt; 0, there is a root in [1, 2].\nFirst Iteration:\nMidpoint: c_1 = \\frac{1 + 2}{2} = 1.5\nf(1.5) = 1.5^3 - 1.5 - 2 = -0.125\nSince f(1) \\cdot f(1.5) &lt; 0, the root lies in [1, 1.5].\nSecond Iteration:\nMidpoint: c_2 = \\frac{1 + 1.5}{2} = 1.25\nf(1.25) = 1.25^3 - 1.25 - 2 = -1.796875\nSince f(1) \\cdot f(1.25) &lt; 0, the root lies in [1, 1.25].\nFurther Iterations:\nRepeat the process until the interval width is smaller than the desired tolerance \\epsilon.\n\n\n\nGeneral Properties of the Bisection Method\n\nGuaranteed Convergence: The method is guaranteed to converge to a root if f(a) \\cdot f(b) &lt; 0 and f(x) is continuous on [a, b].\nRate of Convergence: The Bisection Method has linear convergence, meaning the error decreases by a constant factor with each iteration. This makes the method slower than other methods like Newton’s Method, but much more reliable.\nRobustness: The method is very robust as it does not require the derivative of the function and is insensitive to the initial guesses, provided the condition f(a) \\cdot f(b) &lt; 0 holds.\n\n\n\nApplications of the Bisection Method\n\nRoot Finding: The Bisection Method is used in various fields, including physics, engineering, and mathematics, to find roots of non-linear equations.\nModeling and Simulation: It is used when precise solutions are needed and the function is known to be continuous over the interval.\nInitial Root Estimates: The Bisection Method is often used to find a good initial approximation for more efficient methods like Newton’s Method or the Secant Method.\n\n\n\nAdvantages of the Bisection Method\n\nGuaranteed Convergence: The method always converges if the initial interval contains a root.\nNo Derivatives Needed: Unlike Newton’s Method, the Bisection Method does not require the computation of derivatives.\nSimplicity: The method is easy to understand and implement.\n\n\n\nLimitations of the Bisection Method\n\nSlow Convergence: The method converges linearly, which makes it slower compared to methods like Newton’s Method, which has quadratic convergence.\nOnly One Root: The Bisection Method only finds one root in the interval. If multiple roots exist, it cannot find them all without running the method on different intervals.\nInitial Interval Requirement: The method requires an initial interval [a, b] where the function changes sign, which may not always be easy to determine.\n\n\n\nConclusion\nThe Bisection Method is a reliable and simple method for finding roots of continuous functions, especially when no derivative information is available. While slower than other root-finding algorithms, its guaranteed convergence and robustness make it a valuable tool in numerical analysis."
  },
  {
    "objectID": "mathematics/numerical-analysis/root-finding-methods/index.html",
    "href": "mathematics/numerical-analysis/root-finding-methods/index.html",
    "title": "ROOT FINDING METHODS",
    "section": "",
    "text": "Bisection Method\nFixed-Point Iteration\nNewtons’s Method\nSecant Method"
  },
  {
    "objectID": "mathematics/numerical-analysis/root-finding-methods/secant-method.html",
    "href": "mathematics/numerical-analysis/root-finding-methods/secant-method.html",
    "title": "Secant Method",
    "section": "",
    "text": "The Secant Method is a numerical method for finding roots of a nonlinear equation f(x) = 0. It is similar to Newton’s Method, but it does not require the computation of the derivative f'(x). Instead, the Secant Method approximates the derivative using a secant line through two points on the function."
  },
  {
    "objectID": "mathematics/numerical-analysis/root-finding-methods/secant-method.html#the-secant-method-formula",
    "href": "mathematics/numerical-analysis/root-finding-methods/secant-method.html#the-secant-method-formula",
    "title": "Secant Method",
    "section": "The Secant Method Formula",
    "text": "The Secant Method Formula\nGiven two initial approximations x_{k-1} and x_k, the next approximation x_{k+1} is computed using the secant line through these points. The formula is:\n\nx_{k+1} = x_k - \\frac{f(x_k)(x_k - x_{k-1})}{f(x_k) - f(x_{k-1})}\n\nThis equation is derived by approximating the derivative f'(x_k) using the difference quotient:\n\nf'(x_k) \\approx \\frac{f(x_k) - f(x_{k-1})}{x_k - x_{k-1}}\n\nThe main advantage of the Secant Method is that it avoids the need to compute the derivative f'(x), making it useful for functions where the derivative is difficult to compute or does not exist.\n\nAlgorithm\n\nInitial Guesses: Start with two initial approximations x_0 and x_1.\nIteration Formula: Compute successive approximations using the formula:\n\n\nx_{k+1} = x_k - \\frac{f(x_k)(x_k - x_{k-1})}{f(x_k) - f(x_{k-1})}\n\n\nRepeat: Continue iterating until the difference between successive approximations is less than a specified tolerance \\epsilon, or until |f(x_k)| &lt; \\epsilon.\n\n\n\nConvergence\nThe Secant Method typically converges faster than the Bisection Method but slower than Newton’s Method. It has a convergence rate of approximately 1.618, known as superlinear convergence. This is faster than the linear convergence of the Bisection Method, but slower than the quadratic convergence of Newton’s Method.\n\n\nExample\nLet’s solve the equation f(x) = x^2 - 4 = 0 using the Secant Method, which has roots at x = \\pm 2.\n\nInitial Guesses: Let x_0 = 3 and x_1 = 2.5.\nFirst Iteration:\n\nx_2 = x_1 - \\frac{f(x_1)(x_1 - x_0)}{f(x_1) - f(x_0)} = 2.5 - \\frac{(2.5^2 - 4)(2.5 - 3)}{(2.5^2 - 4) - (3^2 - 4)} = 2.05\n\nSecond Iteration:\n\nx_3 = x_2 - \\frac{f(x_2)(x_2 - x_1)}{f(x_2) - f(x_1)} = 2.05 - \\frac{(2.05^2 - 4)(2.05 - 2.5)}{(2.05^2 - 4) - (2.5^2 - 4)} \\approx 2.0006\n\nFurther Iterations: Continue until the difference between successive approximations is less than a specified tolerance (e.g., \\epsilon = 10^{-5}).\n\nIn this case, after just two iterations, we are already very close to the root x = 2.\n\n\nGeneral Properties of the Secant Method\n\nNo Derivatives Needed: Unlike Newton’s Method, the Secant Method does not require the computation of the derivative f'(x), making it useful for functions that are not differentiable or where computing the derivative is expensive.\nSuperlinear Convergence: The Secant Method converges faster than the Bisection Method but slower than Newton’s Method. Its convergence rate is superlinear with a rate of approximately 1.618.\nRequires Two Initial Guesses: The method requires two initial approximations, x_0 and x_1, unlike Newton’s Method, which only needs one initial guess.\n\n\n\nApplications of the Secant Method\n\nRoot Finding: The Secant Method is widely used to find roots of non-linear equations, especially in cases where the derivative is not available or is costly to compute.\nOptimization: It can be used in optimization problems where the objective is to minimize or maximize a function without requiring the calculation of the derivative.\n\n\n\nAdvantages of the Secant Method\n\nNo Derivatives: The method does not require the calculation of f'(x), making it easier to apply in situations where the derivative is not known.\nFaster than Bisection: The Secant Method generally converges more quickly than the Bisection Method, especially when the initial guesses are close to the root.\n\n\n\nLimitations of the Secant Method\n\nSlower than Newton’s Method: While it converges faster than the Bisection Method, the Secant Method typically converges more slowly than Newton’s Method, which has quadratic convergence.\nConvergence is Not Guaranteed: The Secant Method does not always converge, especially if the initial guesses are not close to the actual root. If f(x_k) = f(x_{k-1}), the method will fail due to division by zero.\nRequires Good Initial Guesses: Poor choices for the initial approximations x_0 and x_1 can result in slow convergence or failure to converge.\n\n\n\nConclusion\nThe Secant Method provides a good balance between speed and ease of use, especially when derivatives are difficult or costly to compute. It is faster than the Bisection Method but not as fast as Newton’s Method when derivatives are available. Careful selection of initial guesses is important for ensuring successful convergence."
  }
]